"X","Id","Titre","Auteurs","Description","Date"
1,124,"Hybrid Automata: An Algorithmic Approach to the Specification and Verification of Hybrid Systems","Costas Courcoubetis; Pei-hsin Ho; Rajeev Alur; Thomas A. Henzinger;","We introduce the framework of hybrid automata as a model and specification language for hybrid systems. Hybrid automata can be viewed as a generalization of timed automata, in which the behavior of variables is governed in each state by a set of differential equations. We show that many of the examples considered in the workshop can be defined by hybrid automata. While the reachability problem is undecidable even for very restricted classes of hybrid automata, we present two semidecision procedures for verifying safety properties of piecewise-linear hybrid automata, in which all variables change at constant rates. The two procedures are based, respectively, on minimizing and computing fix points on generally infinite state spaces. We show that if the procedures terminate, then they give correct answers. We then demonstrate that for many of the typical workshop examples, the procedures do terminate and thus provide an automatic way for verifying their properties. 1 Introduction More and...","1996-02-21"
2,496,"Representing Action: Indeterminacy and Ramifications","Enrico Giunchiglia; G. Neelakantan Kartha; Vladimir Lifschitz;","We define and study a high-level language for describing actions, more expressive than the action language A introduced by Gelfond and Lifschitz. The new language, AR, allows us to describe actions with indirect effects (ramifications), nondeterministic actions, and actions that may be impossible to execute. It has symbols for non propositional fluents and for the fluents that are exempt from the commonsense law of inertia. Temporal projection problems specified using the language AR can be represented as nested abnormality theories based on the situation calculus. 1 Introduction Mary jumped into the lake, then got out of the water and put her hat on. Common sense allows us to answer some questions about the outcome of this series of events. ffl Is Mary in the lake? Of course not. She just got out of it. ffl Does she have the hat on? Of course she does. She just put it on. ffl Is she wet? Of course she is. She just got out of the lake. These are examples of reasoning about actions...","1996-09-12"
3,712,"A Decision Procedure for a Temporal Belief Logic","Michael Fisher; Michael Wooldridge;",". This paper presents a temporal belief logic called L TB . In addition to the usual connectives of linear discrete temporal logic, L TB contains an indexed set of modal belief connectives, via which it is possible to represent the belief systems of resource-bounded reasoning agents. The applications of L TB in general, and its use for representing the dynamic properties of multi-agent AI systems in particular, are discussed in detail. A tableau-based decision procedure for L TB is then described, and some examples of its use are presented. The paper concludes with a discussion and future work proposals. 1 Introduction Temporal logics have been shown to have many applications, in a variety of disciplines. For example: in computer science, temporal logics are used in the specification and verification of reactive systems [16]; in artificial intelligence, they are used as knowledge representation formalisms, and have proved to be a valuable tool in tackling such problems as reasoning ab...","1998-07-23"
4,1186,"A Comparative Evaluation of Sequential Feature Selection Algorithms","David W. Aha; Richard L. Bankert;","Several recent machine learning publications demonstrate the utility of using feature selection algorithms in supervised learning tasks. Among these, sequential feature selection algorithms are receiving attention. The most frequently studied variants of these algorithms are forward and backward sequential selection. Many studies on supervised learning with sequential feature selection report applications of these algorithms, but do not consider variants of them that might be more appropriate for some performance tasks. This paper reports positive empirical results on such variants, and argues for their serious consideration in similar learning tasks. 19.1 Motivation Feature selection algorithms attempt to reduce the number of dimensions considered in a task so as to improve performance on some dependent measures. In this paper, we restrict our attention to supervised learning tasks, where our dependent variables are classification accuracy, size of feature subset, and computational ...","1995-12-29"
5,1347,"Extending Promela and Spin for Real Time","Costas Courcoubetis; Stavros Tripakis;","The efficient representation and manipulation of time information is key to any successful implementation of a verification tool. We extend the syntax and semantics of the higher level specification language Promela to include constructs and statements based on the model of timed Buchi automata [2]. We implement these extensions on top of the verification tool Spin. 1 Introduction Promela [8] is a language for the specification of interactive concurrent systems. Such systems consist of a finite number of separate components, which act independently one from another, and interact through the exchange of messages over message channels. A large part of these systems, including communication protocols, asynchronous circuits, traffic or flight controllers, and real--time operating systems can be characterized as real--time systems. This characterization comes from the following two observations : 1. the correct functioning of those systems depends on the timely coordination of their inter...","1998-11-10"
6,1948,"TTA Processor Synthesis","Henk Corporaal; Reinoud Lamberts;","Due to the decreasing feature size of VLSI technology, the amount of hardware which can be integrated into a single chip increases. As a result, future processor chips can execute tens of operations concurrently. Many applications can profit from these huge amounts of hardware parallelism by designing application specific processors. Two problems emerge however: 1) the design space gets large; it is difficult to chose a satisfactory solution, and 2) the design complexity increases and therefore design cycle gets too long. Solving these problems makes tools for automatic processor design necessary. In this paper we present such a tool. It generates processor layout for so called transport triggered processors. The processor configuration is largely parameterized. A working processor has already been produced using this generator. An important question concerns the quality of automatically generated designs: what do we pay in area and cycle time, when compared to a manual design? In orde...","1997-03-19"
7,2104,"Sample Method for Minimization of OBDDs","Christoph Meinel;","This paper contributes to the solution of the minimization problem of Ordered Binary Decision Diagrams by means of variable reordering. We suggest a new heuristic that is based on sampling. A small OBDD sample is chosen from the OBDDs that are considered for minimization. Solving the problem for this small sample, we obtain a variable order that is adapt for the entire OBDDs. We present first experimental results with the Sample Reordering targeted at combinatorial verification. The suggested heuristic is substantially faster than Sifting. 1 Introduction The problem of highest priority for all practical applications that use Ordered Binary Decision Diagrams (OBDDs, for definitions see [Bry86]) is their conciseness. Since the size of the OBDD representation for a function may vary exponentially for different orders of variables, the problem of finding an optimal variable ordering that realizes the minimal size is of special interest. This problem is known to be NP-complete [THY93, BW96...","1998-10-20"
8,2111,"Pushing the Envelope: Planning, Propositional Logic, and Stochastic Search","Bart Selman; Henry Kautz;","Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning. Introduction There is a widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. The origin of this belief can be traced to the early 1970's, when work on plan generation using first-order, resolution theorem-proving (Green 1969) failed to scale up to realistically-sized problems. The relative succe...","1998-03-17"
9,2490,"Decision Trees for Automated Identification of Cosmic Ray Hits in Hubble Space Telescope Images","Holland Ford; Richard L. White; Rupali Chandar; Sreerama K. Murthy; Steven Salzberg;","We have developed several algorithms for classifying objects in astronomical images. These algorithms have been used to label stars, galaxies, cosmic rays, plate defects, and other types of objects in sky surveys and other image databases. Our primary goal has been to develop techniques that classify with high accuracy, in order to ensure that celestial objects are not stored in the wrong catalogs. In addition, classification time must be fast due to the large number of classifications and to future needs for on-line classification systems. This paper reports on our results from using decision tree classifiers to identify cosmic ray hits in Hubble Space Telescope images. This method produces classifiers with over 95% accuracy using data from a single, unpaired image. Our experiments indicate that this accuracy will get even higher if methods for eliminating background noise improve. 1 Address for Salzberg and Murthy: Department of Computer Science, Johns Hopkins University, Baltimore,...","1994-11-04"
10,3156,"Irrelevant Features and the Subset Selection Problem","George H. John; Karl Pfleger; Ron Kohavi;","We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets. 1 INTRODUCTION In supervised learning, one is given a training set containing labelled instances. T...","1997-01-14"
11,3170,"An Approach to the Description and Analysis of Hybrid Systems","A. Olivero; J. Sifakis; S. Yovine; X. Nicollin;","Introduction The paper presents a model for hybrid systems, that is, systems that combine discrete and continuous components. Such systems are usually reactive real-time systems used to control an environment evolving over time. A main assumption is that a run of a hybrid system is a sequence of two-phase steps. The first phase of a step corresponds to a continuous state transformation usually described in terms of some parameter representing the time elapsed during this phase. In the second phase the state is submitted to a discrete change taking zero time. To illustrate this assumption, consider a temperature regulator commanding a heater so as to maintain the temperature ` of a room between two given bounds ` min and ` max . A run of such a system is a sequence of steps determined by the alternating state changes of the heater from ON to OFF<F26.","1999-03-15"
12,3175,"Parametric Real-time Reasoning","Moshe Y. Vardi; Rajeev Alur; Thomas A. Henzinger;",". Traditional approaches to the algorithmic verification of real-time systems are limited to checking program correctness with respect to concrete timing properties (e.g., ""message delivery within 10 milliseconds""). We address the more realistic and more ambitious problem of deriving symbolic constraints on the timing properties required of real-time systems (e.g., ""message delivery within the time it takes to execute two assignment statements""). To model this problem, we introduce parametric timed automata --- finite-state machines whose transitions are constrained with parametric timing requirements. The emptiness question for parametric timed automata is central to the verification problem. On the negative side, we show that in general this question is undecidable. On the positive side, we provide algorithms for checking the emptiness of restricted classes of parametric timed automata. The practical relevance of these classes is illustrated with several verification examples. There ...","1997-06-17"
13,3177,"A Heuristic Estimator for Means-Ends Analysis in Planning","Drew Mcdermott;","Means-ends analysis is a seemingly well understood search technique, which can be described, using planning terminology, as: keep adding actions that are feasible and achieve pieces of the goal. Unfortunately, it is often the case that no action is both feasible and relevant in this sense. The traditional answer is to make subgoals out of the preconditions of relevant but infeasible actions. These subgoals become part of the search state. An alternative, surprisingly good, idea is to recompute the entire subgoal hierarchy after every action. This hierarchy is represented by a greedy regression-match graph. The actions near the leaves of this graph are feasible and relevant to a sub. . . subgoals of the original goal. Furthermore, each subgoal is assigned an estimate of the number of actions required to achieve it. This number can be shown in practice to be a useful heuristic estimator for domains that are otherwise intractable. Keywords: planning, search, means-ends analysis Reinven...","1999-03-09"
14,3651,"Data Model and Query Evaluation in Global Information Systems","Alon Y. Levy; Thomas Kirk;",". Global information systems involve a large number of information sources distributed over computer networks. The variety of information sources and disparity of interfaces makes the task of easily locating and efficiently accessing information over the network very cumbersome. We describe an architecture for global information systems that is especially tailored to address the challenges raised in such an environment, and distinguish our architecture from architectures of multidatabase and distributed database systems. Our architecture is based on presenting a conceptually unified view of the information space to a user, specifying rich descriptions of the contents of the information sources, and using these descriptions for optimizing queries posed in the unified view. The contributions of this paper include: (1) we identify aspects of site descriptions that are useful in query optimization; (2) we describe query optimization techniques that minimize the number of information source...","1996-12-10"
15,3776,"Generalization in Reinforcement Learning: Safely Approximating the Value Function","Andrew W. Moore; Justin A. Boyan;","To appear in: G. Tesauro, D. S. Touretzky and T. K. Leen, eds., Advances in Neural Information Processing Systems 7, MIT Press, Cambridge MA, 1995. A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization. 1 INTRODUCTION Reinforcement learning---the problem of getting an agent to learn to act from sparse, delayed rewards---has been advanced by techniques based on dynamic programming (DP). These algorithms compute a value function ...","1995-02-07"
16,3950,"Efficient Scaling-Invariant Checking of Timed Bisimulation","Carsten Weise; Dirk Lenzkes; Lehrstuhl Fur Informatik I;",". Bisimulation is an important notion for the verification of distributed systems. Timed bisimulation is its natural extension to real time systems. Timed bisimulation is known to be decidable for timed automata using the so-called region technique. We present a new, top down approach to timed bisimulation which applies the zone technique from the theory of hybrid systems. In contrast to the original decision algorithm, our method has a better space complexity and is scaling invariant: altering the time scale does not effect the space complexity. 1 Introduction Strong and weak bisimulation ([Mil89]) are useful notions of equivalence for the verification and analysis of distributed systems. Timed bisimulation is the suitable notion of bisimulation for real-time systems. We use timed graphs (or timed automata) ([NSY93,HNSY92,AD94,AC+95]) as the specification formalism for real time systems. Timed graph use the positive reals as time domain. While trace-based equivalences for timed autom...","1997-10-29"
17,4526,"Random Early Detection Gateways for Congestion Avoidance","Sally Floyd; Van Jacobson;","This paper presents Random Early Detection (RED) gateways for congestion avoidance in packet switched networks. The gateway detects incipient congestion by computing the average queue size. The gateway could notify connections of congestion either by dropping packets arriving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold, the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a function of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection's share of the bandwidth through the gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP. The RED gateway has no bias against bursty traffic and avoids the global ...","1997-12-03"
18,4897,"Implicit and Incremental Computation of Primes and Essential Primes of Boolean functions","J. C. Madre; O. Coudert;","Recently introduced implicit set manipulation techniques have made it possible to formally verify finite state machines with state graphs too large to be built. This paper shows that these techniques can also be used with success to compute and manipulate implicitly extremely large sets of prime and of essential prime implicants of incompletely specified Boolean functions. These sets are denoted by meta-products that are represented with binary decision diagrams. This paper describes two procedures. One is based on the standard BDD operators, and the other, more efficient, takes advantage of the structural properties of BDDs and of meta-products to handle a larger class of functions than the former one. 1 Introduction We have recently introduced a technique [ 4 ] for verifying finite state machines that can deal with machines with state graphs too large to be built. The key concepts that make this verification possible are to denote subsets of f0; 1g @ with their characteristic fu...","1995-08-05"
19,4913,"Resource Spackling: A Framework for Integrating Register Allocation in Local and Global Schedulers","David A. Berson; Mary Lou Soffa; Rajiv Gupta;",": We present Resource Spackling, a framework for integrating register allocation and instruction scheduling that is based on a Measure and Reduce paradigm. The technique measures the resource requirements of a program and uses the measurements to distribute code for better resource allocation. The technique is applicable to the allocation of different types of resources. A program's resource requirements for both register and functional unit resources are first measured using a unified representation. These measurements are used to find areas where resources are either under or over utilized, called resource holes and excessive sets, respectively. Conditions are determined for increasing resource utilization in the resource holes. These conditions are applicable to both local and global code motion. 1 Introduction A variety of local and global scheduling techniques have been developed for exploiting instruction level parallelism. The degree of parallelism in the schedule is affected...","1996-09-16"
20,5101,"A System for Induction of Oblique Decision Trees","Simon Kasif; Sreerama K. Murthy; Steven Salzberg;","This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees. 1. Introduction Current data collection technology provides a unique challenge and opportunity for automated machine learning techniques. The advent of major scientific projects such as the Human Genome Project, the Hubble Space Telescope, and the human brain mappi...","1994-11-04"
21,5266,"Minimizing ROBDD Sizes of Incompletely Specified Boolean Functions by Exploiting Strong Symmetries","Christoph Scholl; Gunter Hotz; Paul Molitor; Stephan Melchior;","We present a method computing a minimum sized partition of the variables of an incompletely specified Boolean function into symmetric groups. The method can be used during minimization of robdds of incompletely specified Boolean functions. We apply it as a preprocessing step of symmetric sifting presented by Panda [24] and Moller [20] and of techniques for robdd minimization of incompletely specified Boolean functions presented by Chang [6] and Shiple [28]. The technique is shown to be very effective: it improves robdd sizes of symmetric sifting by a factor of 51% and by a factor of 70% in combination with a slightly modified version of the technique of Chang and Shiple. 1 Introduction Binary Decision Diagrams (BDDs) as a data structure for representation of Boolean functions were first introduced by Lee [17] and further popularized by Akers [1] and Moret [21]. In the restricted form of reduced ordered BDDs (robdds) they gained widespread application because robdds are a canonical r...","1997-04-18"
22,5476,"Quantitative Analysis and Model Checking","Marta Kwiatkowska; Michael Huth;","Many notions of models in computer science provide quantitative information, or uncertainties, which necessitate a quantitative model checking paradigm. We present such a framework for reactive and generative systems based on a non-standard interpretation of the modal mu-calculus, where ¯x:OE/x:OE are interpreted as least/greatest fixed points over the infinite lattice of maps from states to the unit interval. By letting formulas denote lower bounds of probabilistic evidence of properties, the values computed by our quantitative model checker can serve as satisfactory correctness guarantees in cases where conventional qualitative model checking fails. Since fixed point iteration in this infinite domain is computationally unfeasible, we establish that the computation of fixed points may be restated as a conventional, and on average efficient, optimization problem in linear programming; this holds for a fragment of the modal mu-calculus which subsumes CTL. Our semantics induces a state e...","1997-03-21"
23,6008,"Receiver-driven Layered Multicast","Martin Vetterli; Steven Mccanne; Van Jacobson;","State of the art, real-time, rate-adaptive, multimedia applications adjust their transmission rate to match the available network capacity. Unfortunately, this source-based rate-adaptation performs poorly in a heterogeneous multicast environment because there is no single target rate --- the conflicting bandwidth requirements of all receivers cannot be simultaneously satisfied with one transmission rate. If the burden of rate-adaption is moved from the source to the receivers, heterogeneity is accommodated. One approach to receiver-driven adaptation is to combine a layered source coding algorithm with a layered transmission system. By selectively forwarding subsets of layers at constrained network links, each user receives the best quality signal that the network can deliver. We and others have proposed that selective-forwarding be carried out using multiple IP-Multicast groups where each receiver specifies its level of subscription by joining a subset of the groups. In this paper, we ...","1998-10-26"
24,6497,"Efficient Verification of Real-Time Systems: Compact Data Structures and State-Space Reduction","Kim G. Larsen; Wang Yi;",". During the past few years, a number of verification tools have been developed for real-time systems in the framework of timed automata. One of the major problems in applying these tools to industrial-sized systems is the huge memory-usage for the exploration of the state space of a network (or product) of timed automata, as the model-checkers must keep information about not only the control structure of the automata but also the clock values specified by clock constraints. In this paper, we present a compact data structure for representing clock constraints. The data structure is based on an O(n 3 ) algorithm which, given a constraint system over real-valued variables consisting of bounds on differences, constructs an equivalent system with a minimal number of constraints. In addition, we have developed an on-the-fly reduction technique to minimize the space-usage. Based on static analysis of the control structure of a network of timed automata, we are able to compute a set of symbo...","1999-01-09"
25,6885,"PCCTS Reference Manual - Version 1.00","H. G. Dietz; T. J. Parr; W. E. Cohen;","Syntax-Trees. Default is not to generate trees. -gc Do not generate output parser code or lexical description (implies -gx). This option can be used to check a grammar for ambiguities. Default is to generate parsers. -ge Generate an error class for each non-terminal of the form #errclass Rule { rule } Default is not to generate the classes. -gs When a token in the current look-ahead needs to be compared against two or more tokens, code for set membership is generated rather than a set of integer comparisons. This renders the parsers more efficient, but much more difficult to read for a human. When debugging an ANTLR parser with a source level debugger, this command-line option should be used to create more readable parsers. The default is to generate sets for any token expression list with two or more members. -gd Generate debugging code; trace rule invocation. This command-line option forces ANTLR to generate code that calls zzTRACE() with the rule name in which it appears as an ...","1994-09-13"
26,7099,"Two examples of verification of multirate timed automata with Kronos","C. Daws; S. Yovine;","Multirate timed automata [2] are an extension of timed automata [3] where each clock has its own speed varying between a lower and an upper bound that may change from one control location to another. This formalism is well-suited for specifying hybrid systems where the dynamics of the continuous variables are defined or can be approximated by giving the minimal and maximal rate of change. To avoid the difficulties inherent in the verification of multirate timed automata, we follow the approach suggested in [8]. This approach consists of first transforming the multirate timed automata into timed automata and then applying the symbolic techniques implemented in Kronos. We show the practical interest of this approach analyzing two examples recently proposed in the literature and considered to be realistic case studies: the manufacturing plant of [10] and the Philips audio control protocol [4, 7]. 1 Introduction Multirate timed automata [2] are an extension of timed automata [3] where clo...","1996-09-09"
27,7624,"Modularity for Timed and Hybrid Systems","Rajeev Alur; Thomas A. Henzinger;","In a trace-based world, the modular specification, verification, and control of live systems require each module to be receptive; that is, each module must be able to meet its liveness assumptions no matter how the other modules behave. In a real-time world, liveness is automatically present in the form of diverging time. The receptiveness condition, then, translates to the requirement that a module must be able to let time diverge no matter how the environment behaves. We study the receptiveness condition for real-time systems by extending the model of reactive modules to timed and hybrid modules. We define the receptiveness of such a module as the existence of a winning strategy in a game of the module against its environment. By solving the game on region graphs, we present an (optimal) EXPTIME algorithm for checking the receptiveness of propositional timed modules. By giving a fixpoint characterization of the game, we present a symbolic procedure for checking the receptiveness of linear hybrid modules. Finanly, we present an assume-guarantee principle for reasoning about timed and hybrid modules, and a method for synthesizing receptive controllers of timed and hybrid modules.","1997-06-17"
28,7844,"An Embedding of Timed Transition Systems in HOL","John Herbert; Rachel Cardell-oliver; Roger Hale;","The theory of Timed Transition Systems (TTSs) developed by Henzinger, Manna and Pnueli provides a formal framework for specifying and reasoning about realtime systems. In this theory a system is described by a set of state transitions with associated time constraints. We report on work in progress to mechanize the published theory of timed transition systems using the HOL theorem prover. Different specification languages may be defined in terms of the TTS model. In particular, a real-time temporal logic (RTTL) has been used for specifying requirements and a graphical notation for specifying system designs. A semantics for each of these languages can be given in terms of TTSs, which can in turn be represented in the HOL logic, and these common semantic interpretations enable formal proofs that one specification satisfies another. 1 Introduction The theory of Timed Transition Systems developed by Henzinger, Manna and Pnueli provides a formal framework for specifying and reasoning about ...","1997-03-27"
29,7960,"Partial-Order Methods for the Verification of Concurrent Systems - An Approach to the State-Explosion Problem","Patrice Godefroid;","State-space exploration techniques are increasingly being used for debugging and proving correct finite-state concurrent reactive systems. The reason for this success is mainly the simplicity of these techniques. Indeed, they are easy to understand, easy to implement and, last but not least, easy to use: they are fully automatic. Moreover, the range of properties that they can verify has been substantially broadened thanks to the development of model-checking methods for various temporal logics. The main limit of state-space exploration verification techniques is the often excessive size of the state space due, among other causes, to the modeling of concurrency by interleaving. However, exploring all interleavings of concurrent events is not a priori necessary for verification: interleavings corresponding to the same concurrent execution contain related information. One can thus hope to be able to verify properties of a concurrent system without exploring all interleavings of its concu...","1995-01-10"
30,8069,"Composite Model Checking with Type Specific Symbolic Encodings","Richard Gerber; Tevfik Bultan;","We present a new symbolic model checking technique, which analyzes temporal properties in multityped transition systems. Specifically, the method uses multiple type-specific data encodings to represent system states, and it carries out fixpoint computations via the corresponding type-specific symbolic operations. In essence, different symbolic encodings are unified into one composite model checker. Any type-specific language can be included in this framework -- provided that the language is closed under Boolean connectives, propositions can be checked for satisfiability, and relational images can be computed. Our technique relies on conjunctive partitioning of transition relations of atomic events based on variable types involved, which allows independent computation of one-step pre- and post-conditions for each variable type. In this paper we demonstrate the effectiveness of our method on a nontrivial data-transfer protocol, which contains a mixture of integer and Boolean-valued varia...","1998-03-13"
31,8239,"Estimating the Accuracy of Learned Concepts","Charles Elkan; Timothy L. Bailey;","This paper investigates alternative estimators of the accuracy of concepts learned from examples. In particular, the cross-validation and 632 bootstrap estimators are studied, using synthetic training data and the foil learning algorithm. Our experimental results contradict previous papers in statistics, which advocate the 632 bootstrap method as superior to cross validation. Nevertheless, our results also suggest that conclusions based on cross-validation in previous machine learning papers are unreliable. Specifically, our observations are that (i) the true error of the concept learned by foil from independently drawn sets of examples of the same concept varies widely, (ii) the estimate of true error provided by cross-validation has high variability but is approximately unbiased, and (iii) the 632 bootstrap estimator has lower variability than cross-validation, but is systematically biased. 1 Introduction The problem of concept induction (also known as the classification problem [ K...","1995-02-12"
32,8373,"Acting Optimally in Partially Observable Stochastic Domains","Anthony R. Cassandra; Kaelbling Michael; L. Littman; Leslie Pack;","In this paper, we describe the partially observable Markov decision process (pomdp) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The pomdp approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community. We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient. We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the pomdp approach. Introduction Agents that act in real environments, whether physical or virtual, rarely have complete information about the state of the environment in which they are working. It is necessary for them to choose their actions in partial ignorance and o...","1995-10-20"
33,8434,"Timed and Hybrid Statecharts and their Textual Representation","A. Pnueli; Y. Kesten;",". A structured operational semantics is presented for Timed and Hybrid Statecharts, which are generalizations of the visual specification language of Statecharts intended to model real-time and hybrid systems. In order to study some of the basic features of Statecharts and the extensions necessary to treat real-time and continuous behaviors without being distracted by the graphical representation, we introduce a concurrent real-time language that can be viewed as a textual representation of Statecharts. The language contains statements for delays, preemption, and timeouts. A structured operational semantics of the language and an illustrative example of its use for specification are presented. Extensions to the specification of hybrid systems are obtained by allowing a differential equation as a statement of the extended language. Structured operational semantics is also given for the hybrid version. The same extensions are then applied to the visual Statechart language, and similar ...","1996-02-21"
34,8484,"Model Checking Large Software Specifications","David Notkin; Francesmary Modugno; Jon D. Reese; Paul Beame; Richard J. Anderson; Steve Burns; William Chan;","In this paper we present our results and experiences of using symbolic model checking to study the specification of an aircraft collision avoidance system. Symbolic model checking has been highly successful when applied to hardware systems. We are interested in the question of whether or not model checking techniques can be applied to large software specifications. To investigate this, we translated a portion of the finitestate requirements specification of TCAS II (Traffic Alert and Collision Avoidance System) into a form accepted by a model checker (SMV). We successfully used the model checker to investigate a number of dynamic properties of the system. We report on our experiences, describing our approach to translating the specification to the SMV language and our methods for achieving acceptable performance in model checking, and giving a summary of the properties that we were able to check. We consider the paper as a data point that provides reason for optimism about the potentia...","1997-11-27"
35,8668,"Higher-Order Logic Programming","Dale Miller; Gopalan Nadathur;","ly, if a tactic R holds of G1 and G2, i.e., if (R G1 G2) is solvable from a presentation of primitive tactics as a set of definite clauses, then satisfying the goal G2 in the object-language should suffice to satisfy goal G1. An illustration of these ideas can be provided by considering the task of implementing a proof procedure for propositional Horn clauses. For simplicity of presentation, we restrict the proposi51 type p g. type q g. type r g. type s g. type cl1tac g -? g -? o. type cl2tac g -? g -? o. type cl3tac g -? g -? o. type cl4tac g -? g -? o. (cl1tac p (andgoal r s)). (cl2tac q r). (cl3tac s (andgoal r q)). (cl4tac r truegoal). Figure 5: A tactic-style encoding of some propositional definite clauses tional goal formulas that will be considered to be conjunctions of propositions. The objective will, of course, be to prove such formulas. Each primitive object-level goal therefore corresponds to showing some atomic proposition to be true, and such a goal might be encoded by a...","1995-02-21"
36,8750,"The Nonapproximability of OBDD Minimization","Detlef Sieling;","The size of Ordered Binary Decision Diagrams (OBDDs) is determined by the chosen variable ordering. A poor choice may cause an OBDD to be too large to fit into the available memory. The decision variant of the variable ordering problem is known to be NP -complete. We strengthen this result by showing that for each constant c ? 1 there is no polynomial time approximation algorithm with the performance ratio c for the variable ordering problem unless P = NP . This result justifies, also from a theoretical point of view, to use heuristics for the variable ordering problem. 1 Introduction Ordered Binary Decision Diagrams (OBDDs) are the state-of-the-art data structure for Boolean functions in programs for problems like logic synthesis, model checking or circuit verification. The reason is that many functions occurring in such applications can be represented by OBDDs of reasonable size and that for operations on Boolean functions like equivalence test or synthesis with binary operators eff...","1998-01-19"
37,9181,"Temporal Deductive Databases","Marianne Baudinet;","We survey a number of approaches to the problem of finite representation of infinite temporal extensions. Two of them, Datalog 1S and Templog, are syntactical extensions of Datalog ; the third is based on repetition and arithmetic constraints. We provide precise characterizations of the expressiveness and the computational complexity of these languages. We also describe query evaluation methods. 1 Introduction In a historical database, the information that is stored includes temporal attributes. These attributes indicate when (at what time) the information is valid. Within this context, many options are available. The temporal information can be attached to small or large chunks of data, the underlying time model can be discrete or continuous, and the time model can be based on time points or on time intervals. However, whichever of these options are selected, the choice of a representation for the extension of the temporal attributes remains a fundamental issue. To appear in: A. T...","1992-10-14"
38,9581,"Residuation and Guarded Rules for Constraint Logic Programming","Gert Smolka;","A major difficulty with logic programming is combinatorial explosion: since goals are solved with possibly indeterminate (i.e., branching) reductions, the resulting search trees may grow wildly. Constraint logic programming systems try to avoid combinatorial explosion by building in strong determinate (i.e., non-branching) reduction in the form of constraint simplification. In this paper, we present two concepts, residuation and guarded rules, for further strengthening determinate reduction. Both concepts apply to constraint logic programming in general and yield an operational semantics that coincides with the declarative semantics. Residuation is a control strategy giving priority to determinate reductions. Guarded rules are logical consequences of programs adding otherwise unavailable determinate reductions. R esum e Une difficulte majeure rencontree en programmation logique est l'explosion combinatoire : puisque les buts sont resolus selon des reductions potentiellement indetermine...","1995-04-20"
39,9721,"Formalizing Space Shuttle Software Requirements","Ben L. Di Vito; Judith Crow;","This paper describes two case studies in which requirements for new flight-software subsystems on NASA's Space Shuttle were analyzed, one using standard formal specification techniques, the other using state exploration. These applications serve to illustrate three main theses: (1) formal methods can complement conventional requirements analysis processes effectively, (2) formal methods confer benefits regardless of how extensively they are adopted and applied, and (3) formal methods are most effective when they are judiciously tailored to the application. 1 Introduction Although Space Shuttle flight software is generally considered exemplary among NASA software development projects, requirements analysis and quality assurance in early lifecycle phases still use products and tools dating from the late 1970s and early 1980s. As a result, these analysis and assurance activities remain largely manual exercises lacking well-defined methods or techniques. At the same time, Shuttle flight s...","1970-01-01"
40,9947,"Space- and Time-Efficient BDD Construction via Working Set Control","Bwolen Yang; David R. O'hallaron; Randal E. Bryant; Yirng-an Chen;","Binary decision diagrams (BDDs) have been shown to be a powerful tool in formal verification. Efficient BDD construction techniques become more important as the complexity of protocol and circuit designs increases. This paper addresses this issue by introducing three techniques based on working set control. First, we introduce a novel BDD construction algorithm based on partial breadth-first expansion. This approach has the good memory locality of the breadth-first BDD construction while maintaining the low memory overhead of the depth-first approach. Second, we describe how memory management on a per-variable basis can improve spatial locality of BDD construction at all levels, including expansion, reduction, and rehashing. Finally, we introduce a memory compacting garbage collection algorithm to remove unreachable BDD nodes and minimize memory fragmentation. Experimental results show that when the applications fit in physical memory, our approach has speedups of up to 1.6 in comparis...","1997-11-21"
41,9993,"Binary Decision Diagrams and Beyond: Enabling Technologies for Formal Verification","Randal E. Bryant;","Ordered Binary Decision Diagrams (OBDDs) have found widespread use in CAD applications such as formal verification, logic synthesis, and test generation. OBDDs represent Boolean functions in a form that is both canonical and compact for many practical cases. They can be generated and manipulated by efficient graph algorithms. Researchers have found that many tasks can be expressed as series of operations on Boolean functions, making them candidates for OBDD-based methods. The success of OBDDs has inspired efforts to improve their efficiency and to expand their range of applicability. Techniques have been discovered to make the representation more compact and to represent other classes of functions. This has led to improved performance on existing OBDD applications, as well as enabled new classes of problems to be solved. This paper provides an overview of the state of the art in graph-based function representations. We focus on several recent advances of particular importance for forma...","1997-01-22"
42,10151,"Behavior of Database Production Rules: Termination, Confluence, and Observable Determinism","Alexander Aiken; Jennifer Widom; Joseph M. Hellerstein;",". Static analysis methods are given for determining whether arbitrary sets of database production rules are (1) guaranteed to terminate","1996-06-26"
43,10302,"Compositional and Symbolic Model-Checking of Real-Time Systems","Kim G. Larsen; Paul Pettersson; Wang Yi;","Efficient automatic model-checking algorithms for real-time systems have been obtained in recent years based on the state-region graph technique of Alur, Courcoubetis and Dill. However, these algorithms are faced with two potential types of explosion arising from parallel composition: explosion in the space of control nodes, and explosion in the region space over clockvariables. In this paper we attack these explosion problems by developing and combining compositional and symbolic model-checking techniques. The presented techniques provide the foundation for a new automatic verification tool Uppaal. Experimental results indicate that Uppaal performs time- and space-wise favorably compared with other real-time verification tools. 1 Introduction Within the last decade model-checking has turned out to be a useful technique for verifying temporal properties of finite-state systems. Efficient model-checking algorithms for finite-state systems have been obtained with respect to a number o...","1995-09-12"
44,10394,"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting","Robert E. Schapire; Yoav Freund;","In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight update rule of Littlestone and Warmuth [15] can be adapted to this model yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in R n . In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of...","1997-04-28"
45,10563,"Verifying Safety Properties of a Class of Infinite-State Distributed Algorithms","Bengt Jonsson; Lars Kempe;",". We consider the problem of verifying correctness properties of a class of programs with states that are sets of ground atoms. Such programs can model specifications of telephone services, in which we are particularly interested. For this class of systems, we consider the problem of checking reachability properties. A large class of safety properties can also be reduced to the problem of checking reachability in a transformed system. The emphasis of our approach is on automated verification of such properties. Although the reachability problem is in general undecidable, we present a method for analyzing reachability properties, and show that it can be successfully applied to practical examples. The main idea of our method is the following. In order to check whether a certain set of ""error"" states can be reached from an initial state of the system, we first compute the set of ""unsafe states"" (i.e., states from which it is possible to reach an error state) as a fixpoint, and finally we ...","1997-09-29"
46,10900,"Locating Protein Coding Regions in Human DNA using a Decision Tree Algorithm","Steven Salzberg;","Genes in eukaryotic DNA cover hundreds or thousands of base pairs, while the regions of those genes that code for proteins may occupy only a small percentage of the sequence. Identifying the coding regions is of vital importance in understanding these genes. Many recent research efforts have studied computational methods for distinguishing between coding and noncoding regions, and several promising results have been reported. We describe here a new approach, using a machine learning system that builds decision trees from the data. This approach combines several coding measures to produce classifiers with consistently higher accuracies than previous methods, on DNA sequences ranging from 54 base pairs to 162 base pairs in length. The algorithm is very efficient, and it can easily be adapted to different sequence lengths. Our conclusion is that decision trees are a highly effective tool for identifying protein coding regions. Keywords: coding regions, decision trees, machine learning, ex...","1996-02-21"
47,11038,"Symbolic Exploration of Transition Hierarchies","Rajeev Alur; Sriram K. Rajamani; Thomas A. Henzinger;",". In formal design verification, successful model checking is typically preceded by a laborious manual process of constructing design abstractions. We present a methodology for partially---and in some cases, fully---bypassing the abstraction process. For this purpose, we provide to the designer abstraction operators which, if used judiciously in the description of a design, structure the corresponding state space hierarchically. This structure can then be exploited by verification tools, and makes possible the automatic and exhaustive exploration of state spaces that would otherwise be out of scope for existing model checkers. Specifically, we present the following contributions: -- A temporal abstraction operator that aggregates transitions and hides intermediate steps. Mathematically, our abstraction operator is a function that maps a flat transition system into a two-level hierarchy where each atomic upper-level transition expands into an entire lower-level transition system. For e...","1998-01-13"
48,11039,"Service Disciplines For Guaranteed Performance Service in Packet-Switching Networks","Hui Zhang;","While today's computer networks supports only best-effort service, Future packet-switching integratedservices networks will have to support real-time communication services that allow clients to transport information with performance guarantees expressed in terms of delay, delay jitter, throughput and loss rate. An important issue in providing guaranteed performance service is the choice of the packet service discipline at switching nodes. In this paper, we survey several service disciplines that are proposed in the literature to provide per-connection end-to-end performance guarantees in packet-switching networks. We describe their mechanisms, their similarities and differences, and the performance guarantees they can provide. Various issues and tradeoffs in designing service disciplines for guaranteed performance service are discussed, and a general framework for studying and comparing these disciplines are presented. I. Introduction Communication systems have been revolutionized by...","1999-02-14"
49,11980,"A Compact Petri Net Representation for Concurrent Programs","Lori A. Clarke; Matthew B. Dwyer;","This paper presents a compact Petri net representation for concurrent programs. These Petri nets are based on task interaction graphs and, thus, are called TIG-based Petri nets (TPN)s. They form a compact representation by summarizing the effects of large sequential regions of a program and making useful information about those regions available for program analysis. TPNs and their associated analyses represent a tradeoff between encoding information about program behavior in the program representation or in the analysis algorithms. To evaluate the cost-effectiveness of this tradeoff, we have developed a flexible framework for checking a variety of properties of concurrent programs using the reachability graph generated from a TPN. We present empirical results that demonstrate the benefit of TPNs over alternate Petri net representations and discuss techniques to further reduce the cost of TPN-based analysis. 1 Introduction An important goal of software engineering research is to prov...","1996-03-15"
50,12199,"Greedy Attribute Selection","Dayne Freitag; Rich Caruana;","Many real-world domains bless us with a wealth of attributes to use for learning. This blessing is often a curse: most inductive methods generalize worse given too many attributes than if given a good subset of those attributes. We examine this problem for two learning tasks taken from a calendar scheduling domain. We show that ID3/C4.5 generalizes poorly on these tasks if allowed to use all available attributes. We examine five greedy hillclimbing procedures that search for attribute sets that generalize well with ID3/C4.5. Experiments suggest hillclimbing in attribute space can yield substantial improvements in generalization performance. We present a caching scheme that makes attribute hillclimbing more practical computationally. We also compare the results of hillclimbing in attribute space with FOCUS and RELIEF on the two tasks. 1 INTRODUCTION As machine learning is applied to real-world tasks, difficulties arise that do not occur in simpler textbook experiments. One such diffic...","1997-10-31"
51,12335,"Building a large annotated corpus of English: the Penn Treebank","Beatrice Santorini; Mary Ann Marcinkiewicz; Mitchell P. Marcus; Penn Treebank;","this paper, we review our experience with constructing one such large annotated corpus---the Penn Treebank, a corpus","1996-02-19"
52,12398,"Applying Machine Learning to Agricultural Data","Craig Nevill-manning Witten; Robert Mcqueen; Stephen Garner;","Many techniques have been developed for learning rules and relationships automatically from diverse data sets, to simplify the often tedious and error-prone process of acquiring knowledge from empirical data. While these techniques are plausible, theoretically wellfounded, and perform well on more or less artificial test data sets, they depend on their ability to make sense of real-world data. This paper describes a project that is applying a range of machine learning strategies to problems in agriculture and horticulture. We briefly survey some of the techniques emerging from machine learning research, describe a software workbench for experimenting with a variety of techniques on real-world data sets, and describe a case study of dairy herd management in which culling rules were inferred from a medium-sized database of herd information. 2 Introduction Machine learning is an emerging technology that can aid in the discovery of rules and patterns in sets of data. It has frequently b...","1997-04-29"
53,14452,"CLIN -- An Automated Reasoning System Using Clause Linking","A. Plaisted; Advisor David; Reader Donald; Reader Gopalan Nadathur; Shie-jue Lee; W. Lovel;","ion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 137 10.2.2 Specialized Decision Procedures : : : : : : : : : : : : : : : : : : : : : 138 10.2.3 Semantics : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 138 10.2.4 Selective Hyper-Linking : : : : : : : : : : : : : : : : : : : : : : : : : 139 10.2.5 Others : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 139 A Test Results 140 viii A.1 Test Problems : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 140 A.2 Tables : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 141 B Test Problems 156 C User Manual 184 C.1 Using CLIN : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 184 C.2 Running Modes : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 185 C.3 Comments : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 185 C.4 Names : : : : : : : : : : : : : : : : : : : : : : : : : ...","1995-05-12"
54,14566,"When Visual Programs are Harder to Read than Textual Programs","And M. Petre; T. R. G. Green;","Claims for the virtues of visual programming languages have generally been strong, simple-minded statements that visual programs are inherently better than textual ones. They have paid scant attention to previous empirical literature showing difficulties in comprehending visual programs. This paper reports comparisons between the comprehensibility of textual and visual programs, drawing on the methods developed by Green (1977) for comparing detailed comprehensibility of conditional structures. The visual language studied was LabView, a circuit-diagram-like language which can express conditionals either as `forwards' structures (condition implies action, with nesting) or as `backwards' structures (action is governed by conditions, with boolean operators in place of nesting). Green (1977) found that forwards structures gave relatively better access to `sequential' information, and Gilmore and Green (1984) showed `backwards' structures gave relatively better access to `circumstantial' inf...","1994-11-16"
55,14606,"Optimizing Symbolic Model Checking for Constraint-Rich Models","Al E. Bryant; Bwolen Yang; David R. O'hallaron; Reid Simmons;",". This paper presents optimizations for verifying systems with complex time-invariant constraints. These constraints arise naturally from modeling physical systems, e.g., in establishing the relationship between different components in a system. To verify constraint-rich systems, we propose two new optimizations. The first optimization is a simple, yet powerful, extension of the conjunctivepartitioning algorithm. The second is a collection of BDD-based macro-extraction and macro-expansion algorithms to remove state variables. We show that these two optimizations are essential in verifying constraint-rich problems; in particular, this work has enabled the verification of fault diagnosis models of the Nomad robot (an Antarctic meteorite explorer) and of the NASA Deep Space One spacecraft. 1 Introduction To Appear in CAV'99. This paper presents techniques for using symbolic model checking to automatically verify a class of real-world applications that have many time-invariant constraint...","1999-03-24"
56,15141,"Exploiting Context When Learning to Classify","Peter D. Turney;",". This paper addresses the problem of classifying observations when features are context-sensitive, specifically when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on two domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The problem is to recognize words spoken by a new speaker, not represented in the training set. For both domains, exploiting context results in substantially more accurate classification. 1 Introduction A large body of research in machine learning is concerned with algorithms for classifying observations, where...","1993-12-01"
57,15287,"On the Representation of Probabilities over Structured Domains","Marius Bozga; Oded Maler;",". In this paper we extend one of the main tools used in verification of discrete systems, namely Binary Decision Diagrams (BDD), to treat probabilistic transition systems. We show how probabilistic vectors and matrices can be represented canonically and succinctly using probabilistic trees and graphs, and how simulation of large-scale probabilistic systems can be performed. We consider this work as an important contribution of the verification community to numerous domains which need to manipulate very large matrices. 1 Introduction Many problems in discrete verification can be reduced to the the following one: given a non-deterministic finite-state automaton A = (Q; ffi) and a set P ` Q of states, find the set P of all the states reachable from P . One common way to do this calculation is to let P 0 = P and P i+1 = ffi(P i ) until P i is included in the union P 0 [ : : : [ P iGamma1 . Here P i is the set of states reachable from P after exactly i steps. This method...","1999-03-24"
58,15321,"A Register Allocation Framework Based on Hierarchical Cyclic Interval Graphs","Chandrika Mukerji; Erik R. Altmany; Guang R. Gao; Laurie J. Hendren;","In this paper, we propose the use of cyclic interval graphs as an alternative representation for register allocation. The ""thickness"" of the cyclic interval graph captures the notion of overlap between live ranges of variables relative to each particular point of time in the program execution. We demonstrate that cyclic interval graphs provide a feasible and effective representation that accurately captures the periodic nature of live ranges found in loops. A new heuristic algorithm for minimum register allocation, the fat cover algorithm, has been developed and implemented to exploit such program structure. In addition, a new spilling algorithm is proposed that makes use of the extra information available in the interval graph representation. These two algorithms work together to provide a two-phase register allocation process that does not require iteration of the spilling or coloring phases. We extend the notion of cyclic interval graphs to hierarchical cyclic interval graphs and we...","1998-08-24"
59,15740,"Boolean Expression Diagrams","Henrik Hulgaard; Henrik Reif Andersen;","This paper presents a new data structure called Boolean Expression Diagrams (BEDs) for representing and manipulating Boolean functions. BEDs are a generalization of Binary Decision Diagrams (BDDs) which can represent any Boolean circuit in linear space and still maintain many of the desirable properties of BDDs. Two algorithms are described for transforming a BED into a reduced ordered BDD. One is a generalized version of the BDD apply-operator while the other can exploit the structural information of the Boolean expression. This ability is demonstrated by verifying that two dierent circuit implementations of a 16-bit multiplier implement the same Boolean function. Using BEDs, this verication problem is solved in less than a second, while using standard BDD techniques this problem is infeasible. Generally, BEDs are useful in applications, for example tautology checking, where the end-result as a reduced ordered BDD is small. 1 Introduction Within the last decade Reduced Ordered Bin...","1998-06-24"
60,16117,"A Performance Study of BDD-Based Model Checking","Al E. Bryant; Armin Biere; Bwolen Yang; David R. O'hallaron; Fabio Somenzi; Geert Janssen; Olivier Coudert; Rajeev K. Ranjan;",". We present a study of the computational aspects of model checking based on binary decision diagrams (BDDs). By using a tracebased evaluation framework, we are able to generate realistic benchmarks and perform this evaluation collaboratively across several different BDD packages. This collaboration has resulted in significant performance improvements and in the discovery of several interesting characteristics of model checking computations. One of the main conclusions of this work is that the BDD computations in model checking and in building BDDs for the outputs of combinational circuits have fundamentally different performance characteristics. The systematic evaluation has also uncovered several open issues that suggest new research directions. We hope that the evaluation methodology used in this study will help lay the foundation for future evaluation of BDD-based algorithms. 1 Introduction Second International Conference on Formal Methods in Computer-Aided Design (FMCAD'98), pp. ...","1998-12-19"
61,16242,"The Information Manifold","Alon Y. Levy; Divesh Srivastava; Thomas Kirk; Yehoshua Sagiv;","We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to completely exploit knowledge about local closed world information [ Etzioni et al., 1994 ] . Introduction We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of inform...","1996-10-04"
62,16295,"An Algorithm for Probabilistic Planning","Daniel Weld; Nicholas Kushmerick; Steve Hanks;","We define the probabilistic planning problem in terms of a probability distribution over initial world states, a boolean combination of propositions representing the goal, a probability threshold, and actions whose effects depend on the execution-time state of the world and on random chance. Adopting a probabilistic model complicates the definition of plan success: instead of demanding a plan that provably achieves the goal, we seek plans whose probability of success exceeds the threshold. In this paper, we present buridan, an implemented least-commitment planner that solves problems of this form. We prove that the algorithm is both sound and complete. We then explore buridan's efficiency by contrasting four algorithms for plan evaluation, using a combination of analytic methods and empirical experiments. We also describe the interplay between generating plans and evaluating them, and discuss the role of search control in probabilistic planning. 3 We gratefully acknowledge the comment...","1997-07-01"
63,16621,"Proving with BDDs and Control of Information","Jean Goubault;",". We present a new automated proof method for first-order classical logic, aimed at limiting the combinatorial explosion of the search. It is non-clausal, based on BDDs (binary decision diagrams) and on new strategies that control the size and traversal of the search space by controlling the amount of information, in Shannon's sense, gained at each step of the proof. Our prover does not search blindly for a proof, but thinks a lot to decide of a course of action. Practical results show that this pays off. 1 Introduction We present a complete refutation method for first-order classical logic that aims at controlling the growth and at guiding the traversal of the search space intelligently. Our starting point is [10], which proves that finding whether a given proposition in this logic is obvious, for several different reasonable definitions of non-obviousness, is Sigma p 2 -complete. This not only means that proving is hard, but also that any complete proof method is built on, or hid...","1995-02-10"
64,17094,"A Really Temporal Logic","Rajeev Alur; Thomas A. Henzinger;",". We introduce a temporal logic for the specification of real-time systems. Our logic, TPTL, employs a novel quantifier construct for referencing time: the freeze quantifier binds a variable to the time of the local temporal context. TPTL is both a natural language for specification and a suitable formalism for verification. We present a tableau-based decision procedure and a model checking algorithm for TPTL. Several generalizations of TPTL are shown to be highly undecidable. 1 Introduction Linear temporal logic is a widely accepted language for specifying properties of reactive systems and their behavior over time [Pnu77, OL82, MP92]. The tableau-based satisfiability algorithm for its propositional version, PTL, forms the basis for the automatic verification and synthesis of finite-state systems [LP84, MW84]. PTL is interpreted over models that abstract away from the actual times at which events occur, retaining only temporal ordering information about the states of a system. The a...","1997-06-17"
65,17507,"The Algorithmic Analysis of Hybrid Systems","A. Olivero; C. Courcoubetis; J. Sifakis; N. Halbwachs; P. -h. Ho; R. Alur; S. Yovine; T. A. Henzinger; X. Nicollin;","We present a general framework for the formal specification and algorithmic analysis of hybrid systems. A hybrid system consists of a discrete program with an analog environment. We model hybrid systems as finite automata equipped with variables that evolve continuously with time according to dynamical laws. For verification purposes, we restrict ourselves to linear hybrid systems, where all variables follow piecewise-linear trajectories. We provide decidability and undecidability results for classes of linear hybrid systems, and we show that standard programanalysis techniques can be adapted to linear hybrid systems. In particular, we consider symbolic model-checking and minimization procedures that are based on the reachability analysis of an infinite state space. The procedures iteratively compute state sets that are definable as unions of convex polyhedra in multidimensional real space. We also present approximation techniques for dealing with systems for which the iterative proced...","1996-06-25"
66,17574,"Markovian Analysis of Large Finite State Machines","Abelardo Pardo; Enrico Macii; Fabio Somenzi; Gary D. Hachtel;","Regarding finite state machines as Markov chains facilitates the application of probabilistic methods to very large logic synthesis and formal verification problems. In this paper we present symbolic algorithms to compute the steady-state probabilities for very large finite state machines (up to 10 27 states). These algorithms, based on Algebraic Decision Diagrams (ADDs) --- an extension of BDDs that allows arbitrary values to be associated with the terminal nodes of the diagrams --- determine the steady-state probabilities by regarding finite state machines as homogeneous, discrete-parameter Markov chains with finite state spaces, and by solving the corresponding Chapman-Kolmogorov equations. We first consider finite state machines with state graphs composed of a single terminal strongly connected component; for this type of systems we have implemented two solution techniques: One is based on the Gauss-Jacobi iteration, the other one is based on simple matrix multiplication. Then we...","1997-08-22"
67,17769,"Compositional Reachability Analysis Using Process Algebra","Michal Young; Wei Jen Yeh;","State explosion is the primary obstacle to practical application of reachability analysis techniques for concurrent systems. State explosion can be substantially controlled by using process algebra to achieve compositional (divide--and--conquer) analysis. A prototype tool incorporating process algebra is described. The promise and problems of the approach are illustrated by applying the tool to an example that incorporates the alternating bit protocol as a module. 1 Introduction Among techniques for analyzing the synchronization structure of concurrent systems, enumeration of reachable states in a finite-state model (reachability analysis) is attractive because it is simple and relatively straightforward to automate, and can be used in conjunction with model-checking procedures (e.g., [CES86]) to check for application-specific as well as general properties. Reachability analysis has been used successfully in limited domains like simple communication protocols [Sun81]. Application to r...","1992-04-28"
68,17824,"The MAXQ Method for Hierarchical Reinforcement Learning","Thomas G. Dietterich;","This paper presents a new approach to hierarchical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedural semantics---as a subroutine hierarchy---and a declarative semantics---as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchical Q learning algorithm, proves its convergence, and shows experimentally that it can learn much faster than ordinary ""flat"" Q learning. Finally, the paper discusses some interesting issues that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non-hierarchical execution of the MAXQ hierarchy. 1 Introduction Hierarchical approaches to reinforcement learning (RL) problems promise ma...","1998-07-02"
69,18083,"Supervisory Control of Finite State Machines","A. Aziz; A. Saldanha; F. Balarin; M. D. Dibenedetto; R. K. Brayton;",". We address a problem of finding a finite state machine (FSM), which composed with a given FSM, satisfies a given specification. The composition we use is the standard synchronous automata composition restricted to cases which correctly model hardware interconnection. For the satisfaction relation, we use language containment. We present a procedure that will generate a solution (if one exists) which is maximal, i.e. contains behaviors of all other solutions. 1 Introduction We consider the problem of finding a controller Q for a given plant P such that put together, P and Q conform to some specification L. The plant and the controller are (possibly incompletely specified) finite state machines. The specification L is given by a deterministic finite state automaton. ""Putting together"" basically corresponds to the standard synchronous composition of automata, and the notion of conformance is language containment. Our main result is the procedure that either returns the empty set (if t...","1995-07-13"
70,18140,"International Workshop on Description Logics","Alex Borgida; Bernhard Nebel;","A formal language for representing and reasoning about time, actions and plans in a uniform way is presented. We employ an action representation in the style of Allen, where an action is represented by describing what is true while the action is occurring. In this sense, an action is defined by means of temporal constraints on the world states, which pertain to the action itself, and on other possible qualifications for the action occurring over time, while a plan is seen just as a complex action. Therefore, there is no difference between actions and plans in this framework. A distinction between action types and individual actions is supported by the formalism. In this paper, we show how to extend the language with a decomposition operator in a way of distinguishing the different actions composing a plan and not just the different temporal qualifications for the plan itself. Thus, a plan is a hierarchical structure made of its distinct component subparts. The formal representation lan...","1999-03-17"
71,18222,"Multitask Learning: A Knowledge-Based Source of Inductive Bias","Richard A. Caruana;","This paper suggests that it may be easier to learn several hard tasks at one time than to learn these same tasks separately. In effect, the information provided by the training signal for each task serves as a domain-specific inductive bias for the other tasks. Frequentlythe world gives us clusters of related tasks to learn. When it does not, it is often straightforward to create additional tasks. For many domains, acquiring inductive bias by collecting additional teaching signal may be more practical than the traditional approach of codifying domain-specific biases acquired from human expertise. We call this approach MultitaskLearning (MTL). Since much of the power of an inductive learner follows directly from its inductive bias, multitask learning may yield more powerful learning. An empirical example of multitask connectionist learning is presented where learning improves by training one network on several related tasks at the same time. Multitask decision tree induction is also outl...","1995-09-12"
72,18496,"Validity Checking for Combinations of Theories with Equality","Clark Barrett; David Dill; Jeremy Levitt;",". An essential component in many verification methods is a fast decision procedure for validating logical expressions. This paper presents the algorithm used in the Stanford Validity Checker (SVC) which has been used to aid several realistic hardware verification efforts. The logic for this decision procedure includes Boolean and uninterpreted functions and linear arithmetic. We have also successfully incorporated other interpreted functions, such as array operations and linear inequalities. The primary techniques which allow a complete and efficient implementation are expression sharing, heuristic rewriting, and congruence closure with interpreted functions. We discuss these techniques and present the results of initial experiments in which SVC is used as a decision procedure in PVS, resulting in dramatic speed-ups. 1 Introduction Decision procedures are emerging as a central component of formal verification systems. Such a procedure can be included as a component of a general-purpos...","1996-08-10"
73,18642,"The Formal Verification of an Algorithm for Interactive Consistency under a Hybrid Fault Model","John Rushby; Patrick Lincoln;","Modern verification systems such as PVS are now reaching the stage of development where the formal verification of critical algorithms is feasible with reasonable effort. This paper describes one such verification in the field of fault tolerance. The distribution of single-source data to replicated computing channels (Interactive Consistency or Byzantine Agreement) is a central problem in this field. The classic Oral Messages (OM) algorithm solves this problem under the assumption that all channels are either nonfaulty or arbitrarily (Byzantine) faulty. Thambidurai and Park have introduced a ""hybrid"" fault model that distinguishes additional fault modes, along with a modified version of OM. They gave an informal proof that their algorithm withstands the same number of arbitrary faults, but more ""nonmalicious"" faults than OM. We detected a flaw in this algorithm while undertaking its formal verification using PVS. The discipline of mechanically-checked formal verification helped us to d...","1994-10-22"
74,18831,"Powerful Techniques for the Automatic Generation of Invariants","Hassen Saidi; Saddek Bensalem; Yassine Lakhnech;",". When proving invariance properties of programs one is faced with two problems. The first problem is related to the necessity of proving tautologies of the considered assertion language, whereas the second manifests in the need of finding sufficiently strong invariants. This paper focuses on the second problem and describes techniques for the automatic generation of invariants. The first set of these techniques is applicable on sequential transition systems and allows to derive so-called local invariants, i.e. predicates which are invariant at some control location. The second is applicable on networks of transition systems and allows to combine local invariants of the sequential components to obtain local invariants of the global systems. Furthermore, a refined strengthening technique is presented that allows to avoid the problem of size-increase of the considered predicates which is the main drawback of the usual strengthening technique. The proposed techniques are illustrated by ex...","1996-05-11"
75,18858,"PVM: A Framework for Parallel Distributed Computing","V. S. Sunderam;","The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...","1993-01-20"
76,19167,"Using PVS to Analyze Hierarchical State-Based Requirements for Completeness and Consistency","Barbara J. Czerny; Mats P. E. Heimdahl;","This paper has appeared in Proceedings of the IEEE High Assurance Systems Engineering Workshop (HASE'96), October, 1996. Copyright 1996 by The Institute of Electrical and Electronics Engineering, Inc. All rights reserved.","1996-09-12"
77,19249,"A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing","Ching-gung Liu; Lixia Zhang; Sally Floyd; Steven Mccanne; Van Jacobson;","This paper 1 describes SRM (Scalable Reliable Multicast), a reliable multicast framework for light-weight sessions and application level framing. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The SRM framework has been prototyped in wb, a distributed whiteboard application, which has been used on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided the SRM design, including the IP multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the co...","1998-11-17"
78,19422,"Symbolic Model Checking: 10 20 States and Beyond","D. L. Dill; E. M. Clarke; J. R. Burch; K. L. Mcmillan; L. J. Hwang;","Many different methods have been devised for automatically verifying finite state systems by examining state-graph models of system behavior. These methods all depend on decision procedures that explicitly represent the state space using a list or a table that grows in proportion to the number of states. We describe a general method that represents the state space symbolically instead of explicitly. The generality of our method comes from using a dialect of the Mu-Calculus as the primary specification language. We describe a model checking algorithm for Mu-Calculus formulas that uses Bryant's Binary Decision Diagrams (1986) to represent relations and formulas. We then show how our new Mu-Calculus model checking algorithm can be used to derive efficient decision procedures for CTL model checking, satisfiability of linear-time temporal logic formulas, strong and weak observational equivalence of finite transition systems, and language containment for finite !-automata. The fixed point co...","1996-03-15"
79,19508,"Constructive Analysis of Cyclic Circuits","Gerard Berry; Herve Touati; Thomas R. Shiple;","Traditionally, circuits with combinational loops are found only in asynchronous designs. However, combinational loops can also be useful for synchronous circuit design. Combinational loops can arise from high-level language behavioral compiling, and can be used to reduce circuit size. We provide a symbolic algorithm that detects if a sequential circuit with combinationalloops exhibits standard synchronous behavior, and if so, produces an equivalent circuit without combinational loops. We present applications to hardware and software synthesis from the Esterel synchronous programming language. 1 Introduction The interpretation of a loop-free combinational circuit 1 is clear. On the logical side, the circuit defines a boolean function f from inputs to outputs; the function f is computed by composing the intermediate functions according to the gate operators. On the electrical side, if the input voltages remain stable at logical values, then after some delay, the output voltages stabi...","1996-01-22"
80,20329,"Performance Analysis of the RIO Multimedia Storage System with Heterogeneous Disk Configurations","Jose Renato Santos; Richard Muntz;","RIO is a multimedia object server which manages a set of parallel disks and supports real-time data delivery with statistical delay guarantees. RIO uses random data allocation on disks combined with partial replication to achieve load balance and high performance. In this paper we analyze the performance of RIO when the set of disks used to store data blocks is not homogeneous, having both different bandwidths and different storage capacities. The basic problem to be addressed for heterogeneous configurations is that, on average, the fraction of the load directed to each disk is proportional to the amount of data stored on it, which may not be proportional to the disk bandwidth. This may cause some disks to be overloaded, with long queues and delays, even though bandwidth is available on other disks. This reduces the system throughput or increases the delay bound that can be guaranteed. This problem arises whenever the bandwidth to space ratio (BSR) is not uniform across all disks. In ...","1998-07-02"
81,20336,"Generalized Additive Models","Robert Tibshirani; Trevor Hastie;","This article describes flexible statistical methods that may be used to identify and characterize nonlinear regression effects. These methods are called ""generalized additive models"". For example, a commonly used statistical model in medical research is the logistic regression model for binary data. Here we relate the mean of the binary response ¯ = P (y = 1) to the predictors via a linear regression model and the logit link function: log","1997-01-26"
82,20430,"Model Checking Graphical User Interfaces Using Abstractions","Laura Hines; Matthew B. Dwyer; Vicki Carr;","ions * Matthew B. Dwyer, Vicki Carr, Laura Hines Kansas State University Abstract Symbolic model checking techniques have been widely and successfully applied to statically analyze dynamic properties of hardware systems. Efforts to apply this same technology to the analysis of software systems has met with a number of obstacles, such as the existence of non-finite state-spaces. This paper investigates abstractions that make it possible to cost-effectively model check specifications of software for graphical user interface (GUI) systems. We identify useful abstractions for this domain and demonstrate that they can be incorporated into the analysis of a variety of systems with similar structural characteristics. The resulting domain-specific model checking yields fast verification of naturally occurring specifications of intended GUI behavior. 1 Introduction The majority of modern software applications have a graphical user interface (GUI). These interfaces serve a number of functions...","1999-01-07"
83,20469,"Stochastic Process Algebras as a Tool for Performance and Dependability Modelling","Holger Hermanns; Ulrich Herzog; Vassilis Mertsiotakis;","The stochastic process-algebra modellingparadigm has been introduced recently as an extension of classical process algebras with timing information aiming mainly at the integration of functional design with quantitative analysis of computer systems. Time is represented by exponentially distributed random variables that are assigned to each activity in the model. Thus, the semantic model of a stochastic process-algebra model can easily be transformed into a continuous time Markov chain which is suitable for computing performance measures as well as dependability measures. The main problem that one encounters frequently in Markov based modelling is the problem of having to solve a huge and stiff Markov chain. In dependability modelling, largeness is caused by lots of detailed and sometimes surplus information stored in the high level model. Stiff Markov chains result when one uses performance related activities together with reliability events in the same model. Various methods to tackle...","1997-03-14"
84,20730,"Terminological Reasoning with Constraint Handling Rules","Thom Fruhwirth;","Terminological knowledge representation formalisms in the tradition of kl-oneenable one to define the relevant concepts of a problem domain and their interaction in a structured and well-formed way. Recently, M. Schmidt-Schauß and G. Smolka proposed a new methodology for constructing sound and complete inference algorithms for terminological logics. The consistency test of assertions is the basis for all terminological reasoning services. We propose constraint handling rules (CH rules) as an implementation language for terminological reasoning. CH rulesare a flexible means to implement `user-defined' constraints on top of existing host languages like Prolog and Lisp. The implementation results in a natural combination of three layers: (i) a constraint layer that reasons in well-understood domains such as rationals or finite domains, (ii) a terminological layer providing a tailored, validated vocabulary on which (iii) the application layer can rely. As an application example, a configur...","1996-12-11"
85,20858,"Scheduling Dynamic Dataflow Graphs With Bounded Memory Using The Token Flow Model","Joseph Tobin Buck;","SCHEDULING DYNAMIC DATAFLOW GRAPHS WITH BOUNDED MEMORY USING THE TOKEN FLOW MODEL by Joseph Tobin Buck Doctor of Philosophy in Electrical Engineering Prof. Edward A. Lee, chair iii 1 THE DATAFLOW PARADIGM 1 1.1 OPERATIONAL VS DEFINITIONAL 3 1.1.1 The Operational Paradigm 3 1.1.2 Definitional and Pseudo-definitional Models 5 1.2 GRAPHICAL MODELS OF COMPUTATION 7 1.2.1 Petri Nets 7 1.2.2 Analysis of Petri Nets 9 1.2.3 The Computation Graphs of Karp and Miller 13 1.2.4 Marked Graphs 14 1.2.5 Homogeneous Dataflow Graphs 15 1.2.6 General Dataflow Graphs 16 1.2.7 Kahn's Model for Parallel Computation 19 1.3 DATAFLOW COMPUTING 20 1.3.1 Static Dataflow Machines 21 1.3.2 Tagged-Token Dataflow Machines 22 1.3.3 Dataflow/von Neumann Hybrid Machine Models 24 1.4 DATAFLOW AND STREAM LANGUAGES 26 1.4.1 Lucid 27 1.4.2 SISAL 28 1.4.3 SIGNAL and LUSTRE 30 1.5 SUMMARY AND PREVIEW OF FUTURE CHAPTERS 32 2 STATIC SCHEDULING OF DATAFLOW PROGRAMS FOR DSP 33 2.1 COMPILE-TIME VERSUS RUN-TIME SCHE...","1994-11-15"
86,20970,"Specification and Refinement of Finite Dataflow Networks - a Relational Approach","Manfred Broy;",". We specify the black box behavior of dataflow components by characterizing the relation between their input and their output histories. We distinguish between three main classes of such specifications, namely time independent specifications, weakly time dependent specifications and strongly time dependent specifications. Dataflow components are semantically modeled by sets of timed stream processing functions. Specifications describe such sets by logical formulas. We emphasize the treatment of the well-known fair merge problem and the Brock/Ackermann anomaly. We give refinement rules which allow specifications to be decomposed modulo a feedback operator. 1 Introduction Dataflow components can be specified by formulas with a free variable ranging over domains of so-called stream processing functions [7], [5]. Both time independent and time dependent components can be described this way. In the latter case, the functions are timed in the sense that the input/output streams may have oc...","1997-09-29"
87,21290,"BDD Based Procedures for a Theory of Equality with Uninterpreted Functions","Adnan Aziz; Anuj Goel; Hai Zhou; Khurram Sajid; Vigyan Singhal;",". The logic of equality with uninterpreted functions has been proposed for verifying abstract hardware designs. The ability to perform fast satisfiability checking over this logic is imperative for this verification paradigm to be successful. We present symbolic methods for satisfiability checking for this logic. The first procedure is based on restricting analysis to finite instantiations of the design. The second procedure directly reasons about equality by introducing Boolean-valued indicator variables for equality. Theoretical and experimental evidence shows the superiority of the second approach. 1 Verifying High-level Designs Using the Theory of Equality A common problem with automatic formal verification is that the computational resources required for verification increase rapidly with the size of the design. Stateof -the art tools for verification of gate-level designs are not capable of routinely verifying designs possessing more than a hundred to two hundred binary-valued l...","1998-06-21"
88,21583,"Compositional Proof Systems for Model Checking Infinite State Processes","Mads Dam;",". We present the first compositional proof system for checking processes against formulas in the modal ¯-calculus which is capable of handling general infinite-state processes. The proof system is obtained in a systematic way from the operational semantics of the underlying process algebra. A non-trivial proof example is given, and the proof system is shown to be sound in general, and complete for finite-state processes. 1 Introduction In this paper we address the problem of verifying modal ¯-calculus properties of general infinite-state processes, and we present what we believe to be the first genuinely compositional solution to this problem. The value of compositionality in program logics is well established. Compositionality allows better structuring and decomposition of the verification task, it allows proof reuse, and it allows reasoning about partially instantiated programs, thus supporting program synthesis. Even more fundamentally it allows, at least in principle, verification...","1996-03-25"
89,21755,"Code Density Optimization for Embedded DSP Processors Using Data Compression Techniques","Kurt Keutzer; Srinivas Devadas; Stan Y. Liao;","We address the problem of code size minimization in VLSI systems with embedded DSP processors. Reducing code size reduces the production cost of embedded systems. We use data compression methods to develop code size minimization strategies. We present a framework for code size minimization where the compressed data consists of a dictionary and a skeleton. The dictionary can be computed using popular text compression algorithms. We describe two methods to execute the compressed code that have varying performance characteristics and varying degrees of freedom in compressing the code. Experimental results obtained with a TMS320C25 code generator are presented. 1: Introduction An increasingly common micro-architecture for embedded systems is to integrate a microprocessor or microcontroller, a ROM and an ASIC all on a single integrated circuit (Figure 1). Such a micro-architecture can currently be found in such diverse embedded systems as FAX modems, laser printers and cellular telephones....","1996-01-22"
90,21854,"Efficient Algorithms for Identifying Relevant Features","Hussein Almuallim; Thomas G. Dietterich;","This paper describes efficient methods for exact and approximate implementation of the MINFEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This bias is useful for learning domains where many irrelevant features are present in the training data. We first introduce FOCUS-2, a new algorithm that exactly implements the MINFEATURES bias. This algorithm is empirically shown to be substantially faster than the FOCUS algorithm previously given in [ Almuallim and Dietterich, 1991 ] . We then introduce the Mutual-Information-Greedy, SimpleGreedy and Weighted-Greedy algorithms, which apply efficient heuristics for approximating the MIN-FEATURES bias. These algorithms employ greedy heuristics that trade optimality for computational efficiency. Experimental studies show that the learning performance of ID3 is greatly improved when these algorithms are used to preprocess the training data by eliminating the irrelevant features from ID3's consideration. I...","1994-09-13"
91,22178,"Compile-Time Scheduling of Dynamic Constructs in Dataflow Program Graphs","Edward A. Lee; Soonhoi Ha;","Scheduling dataflow graphs onto processors consists of assigning actors to processors, ordering their execution within the processors, and specifying their firing time. While all scheduling decisions can be made at runtime, the overhead is excessive for most real systems. To reduce this overhead, compile-time decisions can be made for assigning and/or ordering actors on processors. Compile-time decisions are based on known profiles available for each actor at compile time. The profile of an actor such as the execution time and the communication patterns. However, a dynamic construct within a macro actor, such as a conditional and a data-dependent iteration, makes the profile of the actor unpredictable at compile time. For those constructs, we propose to assume some profile at compile-time and define a cost to be minimized when deciding on the profile under the assumption that the runtime statistics are available at compile-time. Our decisions on the profiles of dynamic constructs are s...","1997-10-10"
92,22241,"Targeting Safety-Related Errors During Software Requirements Analysis","Robyn R. Lutz;","This paper provides a Safety Checklist for use during the analysis of software requirements for spacecraft and other safety-critical, embedded systems. The checklist specifically targets the two most common causes of safety-related software errors: (1) inadequate interface requirements and (2) discrepancies between the documented requirements and the requirements actually needed for correct functioning of the system. The analysis criteria represented in the checklist are evaluated by application to two spacecraft projects. Use of the checklist to enhance the software requirements analysis is shown to reduce the number of safety-related software errors. I. Introduction An earlier study of the causes of safety-related software errors found that those errors identified as potentially hazardous to a system tend to be produced by different error mechanisms than non-safety-related software errors [15]. Safety-related software errors found during the integration and system testing of two spac...","1996-07-26"
93,22312,"Hard and Easy Distributions of SAT Problems","Bart Selman; David Mitchell; Hector Levesque;","We report results from large-scale experiments in satisfiability testing. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures. Introduction Many computational tasks of interest to AI, to the extent that they can be precisely characterized at all, can be shown to be NP-hard in their most general form. However, there is fundamental disagreement, at least within the AI community, about the implications of this. It is claimed on the one hand that since the performance of algorithms designed to solve NP-hard tasks degrades rapidly with small increases in input size, something will need to be given up to obtain acceptable behavior....","1997-06-05"
94,22400,"A Study of Experimental Evaluations of Neural Network Learning Algorithms: Current Research Practice","Fakultat Fur Informatik; Lutz Prechelt;","113 articles about neural network learning algorithms published in 1993 and 1994 are examined for the amount of experimental evaluation they contain. Every third of them does employ not even a single realistic or real learning problem. Only 6% of all articles present results for more than one problem using real world data. Furthermore, one third of all articles does not present any quantitative comparison with a previously known algorithm. These results indicate that the quality of research in the area of neural network learning algorithms needs improvement. The publication standards should be raised and easily accessible collections of example problems be built. Contents 1 Introduction 2 2 Methodology 2 2.1 Approach : : : : : : : : : : : : : : 2 2.2 Method : : : : : : : : : : : : : : : 2 2.3 Limitations : : : : : : : : : : : : : 4 3 Results and Discussion 5 4 Conclusion 7 A Collected data 7 A.1 Neural Computation 1993 : : : : : 7 A.2 Neural Computation 1994, 1--4 : : 8 A.3 Neural...","1994-08-30"
95,22421,"On the Applications of Multiplicity Automata in Learning","Amos Beimel; Eyal Kushilevitz; Francesco Bergadano; Nader H. Bshouty; Stefano Varricchio;","Recently the learnability of multiplicity automata [8, 24] attracted a lot of attention, mainly because of its implications on the learnability of several classes of DNF formulae [7]. In this paper we further study the learnability of multiplicity automata. Our starting point is a known theorem from automata theory relating the number of states in a minimal multiplicity automaton for a function f to the rank of a certain matrix F . With this theorem in hand we obtain the following results: ffl A new simple algorithm for learning multiplicity automata in the spirit of [24] with a better query complexity. As a result, we improve the complexity for all classes that use the algorithms of [8, 24] and also obtain the best query complexity for several classes known to be learnable by other methods such as decision trees [13] and polynomials over GF(2) [26]. ffl We prove the learnability of some new classes that were not known to be learnable before. Most notably, the class of polynomials ov...","1996-07-16"
96,22491,"Finding Structure in Time","Jeffrey Elman;","Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.","1998-11-17"
97,22638,"Timed Transition Systems","Amir Pnueli; Thomas A. Henzinger; Zohar Manna;",". We incorporate time into an interleaving model of concurrency. In timed transition systems, the qualitative fairness requirements of traditional transition system are replaced (and superseded) by quantitative lower-bound and upperbound timing constraints on transitions. The purpose of this paper is to explore the scope of applicability for the abstract model of timed transition systems. We demonstrate that the model can represent a wide variety of phenomena that routinely occur in conjunction with the timed execution of concurrent processes. Our treatment covers both processes that are executed in parallel on separate processors and communicate either through shared variables or by message passing, and processes that time-share a limited number of processors under a given scheduling policy. Often it is this scheduling policy that determines if a system meets its real-time requirements. Thus we explicitly address such questions as time-outs, interrupts, static and dynamic priorities. ...","1996-02-21"
98,22811,"Experience with embedding hardware description languages in HOL","Andrew Gordon; John Harrison; John Herbert; John Van Tassel; Mike Gordon; Richard Boulton;","The semantics of hardware description languages can be represented in higher order logic. This provides a formal definition that is suitable for machine processing. Experiments are in progress at Cambridge to see whether this method can be the basis of practical tools based on the HOL theorem-proving assistant. Three languages are being investigated: ELLA, Silage and VHDL. The approaches taken for these languages are compared and current progress on building semantically-based theorem-proving tools is discussed. Keyword Codes: F.4.1; B.7.2; I.2.3 Keywords: Mathematical Logic; Integrated Circuits, Design Aids; Deduction and Theorem Proving. 1 Introduction Hardware can be directly specified in the notation of mathematical logic [11, 14, 18], but this form is unacceptable to many designers and is also unsuitable for input to CAD tools such as simulators and circuit synthesizers. The work described here aims to support the use of conventional hardware description languages (HDLs) within ...","1992-03-18"
99,22904,"From Coloured Petri Nets to Object Petri Nets","Charles Lakos;","ion Abstract: This paper seeks to establish within a formal framework how Coloured Petri Nets can be enhanced to produce Object Petri Nets. It does so by defining a number of intermediate Petri Net formalisms and identifying the features introduced at each step of the development. Object Petri Nets support a complete integration of object-oriented concepts into Petri Nets, including inheritance and the associated polymorphism and dynamic binding. In particular, Object Petri Nets have a single class hierarchy which includes both token types and subnet types. Interaction between subnets can be either synchronous or asynchronous depending on whether the subnet is defined as a super place or a super transition. The single class hierarchy readily supports multiple levels of activity in the net and the generation and removal of tokens has been defined so that all subcomponents are simultaneously generated or removed, thus simplifying memory management. Despite this descriptive power, Object...","1995-06-27"
100,22995,"Subsumption Algorithms for Concept Description Languages","Bernhard Hollunder; Manfred Schmidt-schau; Werner Nutt;","We investigate subsumption algorithms for logic-based knowledge representation languages of the kl-one family. We define an attributive concept description language that contains the logical connectives conjunction, disjunction, and negation, as well as role quantification, number restrictions and role intersection. We describe a rule based calculus to decide subsumption in this language, that closely resembles the tableaux calculus of first order predicate logic. Furthermore, we give polynomial space algorithms for certain sublanguages. 1 Introduction Concept description languages of the kl-one family are a means of expressing taxonomical knowledge by describing hierarchies of concepts [1, 2, 4, 6, 11]. Concepts are described as specializations of other concepts and by the values of attributes (so-called roles). In contrast to earlier knowledge representation formalisms like frames and semantic networks, kl-one languages have the advantage of a Tarski style declarative semantics th...","1995-04-25"
101,23056,"A New Method for Solving Hard Satisfiability Problems","Bart Selman; David Mitchell; Hector Levesque;","We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the approach are also discussed. GSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks.","1997-06-05"
102,23766,"Learning Decision Trees using the Fourier Spectrum","Eyal Kushilevitz; Yishay Mansour;","This work gives a polynomial time algorithm for learning decision trees with respect to the uniform distribution. (This algorithm uses membership queries.) The decision tree model that is considered is an extension of the traditional boolean decision tree model that allows linear operations in each node (i.e., summation of a subset of the input variables over GF (2)). This paper shows how to learn in polynomial time any function that can be approximated (in norm L 2 ) by a polynomially sparse function (i.e., a function with only polynomially many non-zero Fourier coefficients). The authors demonstrate that any function f whose L 1 -norm (i.e., the sum of absolute value of the Fourier coefficients) is polynomial can be approximated by a polynomially sparse function, and prove that boolean decision trees with linear operations are a subset of this class of functions. Moreover, it is shown that the functions with polynomial L 1 -norm can be learned deterministically. The algorithm can a...","1997-03-16"
103,23817,"Incremental Model Checking in the Modal Mu-Calculus","Oleg V. Sokolsky; Scott A. Smolka;",". We present an incremental algorithm for model checking in the alternation -free fragment of the modal mu-calculus, the first incremental algorithm for model checking of which we are aware. The basis for our algorithm, which we call MCI (for Model Checking Incrementally), is a linear-time algorithm due to Cleaveland and Steffen that performs global (non-incremental) computation of fixed points. MCI takes as input a set Delta of changes to the labeled transition system under investigation, where a change constitutes an inserted or deleted transition; with virtually no additional cost, inserted and deleted states can also be accommodated. Like the Cleaveland-Steffen algorithm, MCI requires time linear in the size of the LTS in the worst case, but only time linear in Delta in the best case. We give several examples to illustrate MCI in action, and discuss its implementation in the Concurrency Factory, an interactive design environment for concurrent systems. 1 Introduction The Concurr...","1999-01-29"
104,23976,"Higher-Order Logic Programming as Constraint Logic Programming","Frank Pfenning; Spiro Michaylov;","Higher-order logic programming (HOLP) languages are particularly useful for various kinds of metaprogramming and theorem proving tasks because of the logical support for variable binding via - abstraction. They have been used for a wide range of applications including theorem proving, programming language interpretation, type inference, compilation, and natural language parsing. Despite their utility, current language implementations have acquired a well-deserved reputation for being inefficient. In this paper we argue that HOLP languages can reasonably be viewed as Constraint Logic Programming (CLP) languages, and show how this can be expected to lead to more practical implementations by applying the known principles for the design and implementation of practical CLP systems. 1 Introduction Higher-order logic programming (HOLP) languages [17] typically use a typed -calculus as their domain of computation. In the case of Prolog [18] it is the simply-typed -calculus, while in the case...","1993-04-23"
105,23991,"Incremental Methods for Formal Verification and Logic Synthesis","Gitanjali Meher Swamy;","Incremental Methods for Formal Verification and Logic Synthesis by Gitanjali Meher Swamy Doctor of Philosophy in Engineering-Electrical Engineering and Computer Sciences University of California, Berkeley Professor Robert K. Brayton, Chair IC design is an iterative process; the initial specification of a design is rarely complete and correct. The designer begins with a preliminary and usually incorrect sketch (possibly from a previous generation design), and iteratively refines and corrects it. Usually, refinements are small, and there is much common information between successive design iterations. The current genre of CAD tools do not take into account this iterative nature of design. For each change made to the design, the design is re-verified and re-optimized without taking advantage of information from previous iterations. This leads to inefficient performance. In this thesis, we propose the paradigm of incremental algorithms for CAD. Incremental algorithms use information from a...","1997-06-11"
106,24458,"Query-Answering Algorithms for Information Agents","Alon Y. Levy; Anand Rajaraman; Joann J. Ordille;","We describe the architecture and queryanswering algorithms used in the Information Manifold, an implemented information gathering system that provides uniform access to structured information sources on the World-Wide Web. Our architecture provides an expressive language for describing information sources, which makes it easy to add new sources and to model the fine-grained distinctions between their contents. The queryanswering algorithm guarantees that the descriptions of the sources are exploited to access only sources that are relevant to a given query. Accessing only relevant sources is crucial to scale up such a system to large numbers of sources. In addition, our algorithm can exploit run-time information to further prune information sources and to reduce the cost of query planning. Introduction The number of structured information sources that is available online is growing rapidly. For example, there are many sources on the World-Wide Web (WWW) that provide a query interfac...","1996-07-17"
107,24875,"Efficient Boolean Manipulation with OBDD's Can be Extended to FBDD's","Christoph Meinel; Fachbereich Iv; Jordan Gergov; Mathematik Informatik; Trierer Forschungsberichte;","OBDD's are the state--of--the--art data structure for Boolean function manipulation since basic tasks of Boolean manipulation such as testing equivalence, satisfiability, or tautology, and performing single Boolean synthesis steps can be done efficiently. In the following we show that the efficient manipulation of OBDD's can be extended to a more general data structure, so-- called FBDD's. In detail, the advantages of using FBDD's instead of OBDD's are ffl FBDD's are generally more (sometimes even exponentially more) succinct than OBDD's, ffl FBDD's provide, similarly to OBDD's, canonical representations of Boolean functions, and ffl in terms of FBDD's basic tasks of Boolean manipulation can be performed similarly efficient as in terms of OBDD's. The power of the FBDD--concept is demonstrated by showing that the verification of the benchmark circuit design for the hidden weighted bit function HWB proposed by Bryant can be carried out efficiently in terms of FBDD's while, for princip...","1997-02-19"
108,25284,"A New Heuristic for Bad Cycle Detection Using BDDs","M. Y. Vardi; R. H. Hardin; R. P. Kurshan; S. K. Shukla;",". We describe a new heuristic for detecting bad cycles (reachable cycles that are not confined within one or another designated sets of model states), a fundamental operation for model-checking algorithms. It is a variation on a standard implementation of the Emerson-Lei algorithm, which our experimental data suggests can result in a significant speed-up for verification runs that pass. We conclude that this heuristic can be used to advantage on ""mature"" designs for which the anticipated result of the verification is pass. 1 Introduction It is well known that the model-checking problem for the linear-time temporal logic LTL, the branching time temporal logic CTL, and !-automaton specifications are all solvable in time that is linear in the size of the model [CES86, LP85, VW86]. In most existing approaches, the model-checking problem is reduced to the graph problem of finding a ""bad"" cycle, i.e., a reachable cycle that is not contained in one or more designated set of nodes [CES86, CVW...","1997-04-11"
109,25286,"Bagging Predictors","Leo Breiman;","Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. 1. Introduction A learning set of L consists of data f(y n ; x n ), n = 1; : : : ; Ng where the y's are either class labels or a numerical response. We have a procedure for using this learning set to form a predictor '(x; L) --- if the input is x we ...","1995-07-26"
110,25394,"Implementing Mathematics with The Nuprl Proof Development System","D. J. Howe; H. M. Bromley; J. F. Cremer; J. T. Sasaki; N. P. Mendler; P. Panangaden; R. L. Constable; R. W. Harper; S. F. Allen; S. F. Smith; T. B. Knoblock; W. R. Cleaveland;","Problem solving is a significant part of science and mathematics and is the most intellectually significant part of programming. Solving a problem involves understanding the problem, analyzing it, exploring possible solutions, writing notes about intermediate results, reading about relevant methods, checking results, and eventually assembling a solution. Nuprl is a computer system which provides assistance with this activity. It supports the interactive creation of proofs, formulas, and terms in a formal theory of mathematics","1995-12-22"
111,25678,"An Improved Algorithm for Incremental Induction of Decision Trees","Paul E. Utgoff;","This paper presents an algorithm for incremental induction of decision trees that is able to handle both numeric and symbolic variables. In order to handle numeric variables, a new tree revision operator called `slewing' is introduced. Finally, a non-incremental method is given for finding a decision tree based on a direct metric of a candidate tree. Contents 1 Introduction 1 2 Design Goals 1 3 An Improved Algorithm 2 3.1 Incorporating a Training Instance : : : : : : : : : : : : : : : : : : : : : : : : 2 3.2 Ensuring a Best Test at Each Decision Node : : : : : : : : : : : : : : : : : : 3 3.3 Information Kept at a Decision Node : : : : : : : : : : : : : : : : : : : : : : 3 3.4 Tree Transposition : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 3.5 Slewing a Cutpoint : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 3.6 How to Ensure a Best Test Everywhere : : : : : : : : : : : : : : : : : : : : : 5 4 Incremental Training Cost 5 5 Error-Correction Mo...","1994-11-23"
112,25887,"Mining Association Rules between Sets of Items in Large Databases","Rakesh Agrawal; Tomasz Imielinski;","We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.","1997-02-27"
113,25937,"A View of Local Search in Constraint Programming","And Michel Gendreau; Gilles Pesant;",". We propose in this paper a novel way of looking at local search algorithms for combinatorial optimization problems which better suits constraint programming by performing branch-and-bound search at their core. We concentrate on neighborhood exploration and show how the framework described yields a more efficient local search and opens the door to more elaborate neighborhoods. Numerical results are given in the context of the traveling salesman problem with time windows. This work on neighborhood exploration is part of ongoing research to develop constraint programming tabu search algorithms applied to routing problems. Introduction Local search methods in operations research (or) date back to over thirty years ago ([Lin65]). Applied to difficult combinatorial optimization problems, this heuristic approach yields high-quality solutions by iteratively considering small modifications (called local moves) of a good solution in the hope of finding a better one. Used within a strategy de...","1996-11-23"
114,26170,"The tool Kronos","A. Olivero; C. Daws; S. Tripakis; S. Yovine;","this paper considerably improve Kronos performance and functionalities","1998-11-10"
115,26529,"Efficient Formal Design Verification: Data Structure + Algorithms","Adnan Aziz; Bernard Plessier; Carl Pixley; Rajeev K. Ranjan; Robert K. Brayton;","We describe a data structure and a set of BDD based algorithms for efficient formal design verification. We argue that hardware designs should be translated into an intermediate hierarchical netlist of combinational tables and sequential elements, and internally represented by a flattened network of gates and latches, akin to that in SIS [32]. We establish that the core computation in BDD based formal design verification is forming the image and pre-image of sets of states under the transition relation characterizing the design. To make this step efficient, we address BDD variable ordering, use of partitioned transition relations, use of clustering, use of don't cares, and redundant latch removal. Many of these techniques have been studied in the past. We provide a complete integrated set of modified algorithms and give references and comparisons with previous work. We report experimental results on a series of seven industrial examples containing from 28 to 172 binary valued latches. ...","1995-12-08"
116,26913,"Model Checking Timed Automata","Sergio Yovine;","this paper we survey the different algorithms and data-structures that have been proposed in the seek for efficiency. For the sake of simplicity, we focus our attention on algorithms for solving the so-called reachability problem.","1997-09-10"
117,27306,"The Safety Guaranteeing System at Station Hoorn-Kersenboogerd","J. F. Groote; J. W. C. Koorn; S. F. M. Van Vlijmen;","At the Dutch station Hoorn--Kersenboogerd, computer equipment is used for the safe and in time movement of trains. The computer equipment can be divided in two layers. A top layer offering an interface and means to help a human operator in scheduling train movement. And a bottom layer which checks whether commands issued by the top layer can safely be executed by the rail hardware and which acts appropriately on detection of a hazardous situation. The bottom layer is implemented with a programmable piece of equipment namely a Vital Processor Interlocking 1 (VPI ). This paper introduces the most important features of the VPI at Hoorn--Kersenboogerd. This particular VPI is modelled in ¯CRL. Furthermore, the paper touches upon correctness criteria and tool support for VPIs , and suggests ways for verification of properties of VPIs. Experiments show that it is indeed possible to efficiently verify these correctness criteria. 1991 Mathematics subject classification: 68Q40, 68Q45. 1990 CR...","1994-10-11"
118,27361,"Speeding up Variable Reordering of OBDDs","Christoph Meinel;","In this paper, we suggest a block-restricted sifting strategy which is based on the restriction of Rudell's sifting to certain blocks of variables. The application of this strategy results in a considerable improvement of the time performance without causing a substantial increase of OBDD size. The blocks of variables were determined according to some structural properties reflecting the communication complexity within the considered OBDD without using any additional information about its functionality or dependency of variables. This assures the general usability of the heuristic and the possibility to use it dynamically. 1 Introduction Ordered Binary Decision Diagrams (OBDDs) are one of the few known representations for Boolean functions that are, due their nice algorithmical properties, of a great importance in many different applications. Successful application to complex problems, however, almost invariably entails resorting to various techniques to reduce OBDD sizes. The main re...","1997-09-09"
119,27670,"Model Checking Tools for Parallelizing Compilers","Eric Van Wyk; Teodor Rus;","In this paper we apply temporal logic and model checking to analyze the structure of a source program represented as a process graph. The nodes of this graph are sequential processes whose computations are specified as transition systems; the edges are dependence (flow and control) relations between the computations at the nodes. This process graph is used as an intermediate source program representation by a parallelizing compiler. By labeling the nodes and the edges of the process graph with descriptive atomic propositions and by specifying the conditions necessary for optimizations and parallelizations as temporal logic formulas, we can use a model checker to locate nodes and sub-graphs of the process graph where particular optimizations can be made. We illustrate this technique by showing how a parallelizing compiler can determine if the iterations of an enumerated loop can be executed concurrently. To add or modify optimizations in this parallelizing compiler, we need only specify...","1996-12-30"
120,27759,"Data Flow Analysis for Verifying Properties of Concurrent Programs","Lori A. Clarke; Matthew B. Dwyer;","In this paper we present an approach, based on data flow analysis, that can provide cost-effective analysis of concurrent programs with respect to explicitly stated correctness properties. Using this approach, a developer specifies a property of a concurrent program as a pattern of selected program events and asks the analysis to verify that all or no program executions satisfy the given property. We have developed a family of polynomialtime, conservative data flow analysis algorithms that support reasoning about these questions. To overcome the traditional inaccuracies of static analysis, we have also developed a range of techniques for improving the accuracy of the analysis results. One strength of our approach is the flexibility allowed in choosing and combining these techniques so as to increase accuracy without making analysis time impractical. We have implemented a prototype toolset that automates the analysis for programs with explicit tasking and rendezvous style communication....","1994-12-21"
121,27921,"An Empirical Study of Greedy Local Search for Satisfiability Testing","Bart Selman And; Henry A. Kautz;","GSAT is a randomized local search procedure for solving propositional satisfiability problems. GSAT can solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches, such as the Davis-Putnam procedure. This paper presents the results of numerous experiments we have performed with GSAT, in order to improve our understanding of its capabilities and limitations. We first characterize the space traversed by GSAT. We will see that for nearly all problem classes we have encountered, the space consists of a steep descent followed by broad flat plateaus. We then compare GSAT with simulated annealing, and show how GSAT can be viewed as an efficient method for executing the low-temperature tail of an annealing schedule. Finally, we report on extensions to the basic GSAT procedure. We discuss two general, domain-independent extensions that dramatically improve GSAT's performance on structured problems: the use of claus...","1996-08-30"
122,28289,"A Method for Obtaining Digital Signatures and Public-Key Cryptosystems","A. Shamir; L. Adleman; R. L. Rivest;","An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: 1. Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key. 2. A message can be ""signed"" using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in ""electronic mail"" and ""electronic funds transfer"" systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two lar...","1998-10-08"
123,28323,"A Stochastic Automata Model and its Algebraic Approach","Joost-pieter Katoen; Pedro R. D'argenio;","We discuss a new model for the analysis and simulation of stochastic systems which we call stochastic automata. Basically, they are a combination of the timed automata model and generalised semi-Markov processes (GSMPs for short). We discuss their behaviour and we compare them to the GSMPs model. In addition, we define a stochastic process algebra that supports general distribution (both continuous and discrete). Its semantics is given in terms of stochastic automata. We show that stochastic automata can be expressed in terms of the process algebra. We discuss a concrete example and we finish by discussing our current work on this topic and possible future directions. 1991 Mathematics Subject Classification: 68Q55, 68Q60, 68Q75, 93E03. 1991 CR Categories: C.4, D.3.1, F.3.1, F.3.2, I.6.2. Keywords: stochastic automata, process algebra, stochastic systems, real-time, timed automata, performance analysis. Note: This is a revised version of the article appeared in E. Brinksma and A. Nym...","1997-08-08"
124,28329,"Verification of an Audio Protocol with Bus Collision Using UPPAAL","Fredrik Larsson; Johan Bengtsson; Kare J. Kristoffersen; Kim G. Larsen; Paul Pettersson; W. O. David Griffioen; Wang Yi;",". In this paper we apply the tool Uppaal 1 to an automatic analysis of a version of the Philips Audio Control Protocol with two senders and bus collision handling. This case study is significantly larger than the real-time/hybrid systems previously analysed by automatic tools. During the case study the tool Uppaal was extended with a new feature, committed locations, allowing efficient modelling of broadcast communication. 1 Introduction During the last few years a number of tools for automatic verification of hybrid and real-time systems have emerged [DY95, HHWT95, BLL + 95, HRP94]. These tools have by now reached a state, where they are mature enough for application on realistic case--studies; a claim we hope to substantiate in this paper. We present an application of our tool Uppaal to an automatic analysis of a version of the Philips Audio Control Protocol with two senders and the consequently caused problem of bus collision. The case study is comprehensive compared with prev...","1996-07-16"
125,28887,"Fast Planning Through Planning Graph Analysis","Avrim L. Blum; Merrick L. Furst;","We introduce a new approach to planning in STRIPS-like domains based on constructing and analyzing a compact structure we call a Planning Graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortestpossible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy, and the partial-order planner, UCPOP, on a variety of interesting natural and artificial planning problems. We also give empirical evidence that the plans produced by Graphplan are quite sensible. Since searches made by this approach are fundamentally different from the searches of other common planning methods, they provide a new perspective on the planning problem. Keywords: General Purpose Planning, STRIPS Planning, Graph Algorithms, Planning Graph Analysis. 1 Introduction In this paper we introduce a new planner, Graphplan, which plans in STRIPS-like d...","1997-07-15"
126,29783,"Off-Training Set Error and A Priori Distinctions Between Learning Algorithms","David H. Wolpert;",": This paper uses off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. It is shown, loosely speaking, that for any two algorithms A and B, there are as many targets (or priors over targets) for which A has lower expected OTS error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is ""anti-cross-validation"" (choose the generalizer with largest cross-validation error). On the other hand, for loss functions other than zero-one (e.g., quadratic loss), there are a priori distinctions between algorithms. However even for such loss functions, any algorithm is equivalent on average to its ""randomized"" version, and in this still has no first principles justification in terms of average error. Nonetheless, it may be that (for example) cross-validation has better minimax properties than anti-cross-validation, even for zero-one loss. This paper also analyzes averages over hyp...","1995-01-26"
127,31018,"A Constraint-Based Approach to Diagnosing Software Problems in Computer Networks","Daniel Sabin; Eugene C. Freuder; Mihaela Sabin; Robert D. Russell;",". Distributed software problems can be particularly mystifying to diagnose, for both system users and system administrators. Modelbased diagnosis methods that have been more commonly applied to physical systems can be brought to bear on such software systems. A prototype system has been developed for diagnosing problems in software that controls computer networks. Our approach divides this software into its natural hierarchy of layers, subdividing each layer into three separately modeled components: the interface to the layer above on the same machine, the protocol to the same layer on a remote machine, and the configuration. For each component knowledge is naturally represented in the form of constraints. User interaction modeling is accomplished through the introduction of constraints representing user assumptions, the finitestate machine specification of a protocol is translated to a standard CSP representation and configuration tasks are modeled as dynamic CSPs. Diagnosis is viewed...","1995-10-16"
128,31084,"LogP: Towards a Realistic Model of Parallel Computation","Abhijit Sahay; David Culler; David Patterson; Eunice Santos; Klaus Erik Schauser; Ramesh Subramonian; Richard Karp; Thorsten Von Eicken;","A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. It is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling ...","1995-06-11"
129,31170,"Parallel Breadth-First BDD Construction","Bwolen Yang; David R. O'hallaron;","With the increasing complexity of protocol and circuit designs, formal verification has become an important research area and binary decision diagrams (BDDs) have been shown to be a powerful tool in formal verification. This paper presents a parallel algorithm for BDD construction targeted at shared memory multiprocessors and distributed shared memory systems. This algorithm focuses on improving memory access locality through specialized memory managers and partial breadth-first expansion, and on improving processor utilization through dynamic load balancing. The results on a shared memory system show speedups of over two on four processors and speedups of up to four on eight processors. The measured results clearly identify the main source of bottlenecks and point out some interesting directions for further improvements. 1 Introduction With the increasing complexity of protocol and circuit designs, formal verification has become an important research area. As an example, in 1994, In...","1997-04-07"
130,31494,"PROVERB - A System Explaining Machine-Found Proofs","Xiaorong Huang;","This paper outlines an implemented system called PROVERB that explains machine-found natural deduction proofs in natural language. Different from earlier works, we pursue a reconstructive approach. Based on the observation that natural deduction proofs are at a too low level of abstraction compared with proofs found in mathematical textbooks, we define first the concept of socalled assertion level inference rules. Derivations justified by these rules can intuitively be understood as the application of a definition or a theorem. Then an algorithm is introduced that abstracts machine-found ND proofs using the assertion level inference rules. Abstracted proofs are then verbalized into natural language by a presentation module. The most significant feature of the presentation module is that it combines standard hierarchical text planning and techniques that locally organize argumentative texts based on the derivation relation under the guidance of a focus mechanism. The behavior of the s...","1998-05-07"
131,32081,"Multivariate Decision Trees","Carla E. Brodley; Paul E. Utgoff;","Multivariate decision trees overcome a representational limitation of univariate decision trees: univariate decision trees are restricted to splits of the instance space that are orthogonal to the feature's axis. This paper discusses the following issues for constructing multivariate decision trees: representing a multivariate test, including symbolic and numeric features, learning the coefficients of a multivariate test, selecting the features to include in a test, and pruning of multivariate decision trees. We present some new and review some well-known methods for forming multivariate decision trees. The methods are compared across a variety of learning tasks to assess each method's ability to find concise, accurate decision trees. The results demonstrate that some multivariate methods are more effective than others. In addition, the experiments confirm that allowing multivariate tests improves the accuracy of the resulting decision tree over univariate trees. Contents 1 Introduc...","1993-09-08"
132,32269,"A Kleene Theorem for Timed Automata","Eugene Asarin; Oded Maler; Paul Caspi;","In this paper we define timed regular expressions, an extension of regular expressions for specifying sets of dense time discrete-valued signals. We show that this formalism is equivalent in expressive power to the timed automata of Alur and Dill by providing a translation procedure from expressions to automata and vice versa. The result is extended to !-regular expressions (B uchi's theorem). 1. Introduction Timed automata, i.e. automata equipped with clocks [AD94], have been studied extensively in recent years as they provide a rigorous model for reasoning about the quantitative temporal aspects of systems. Together with realtime logics and process algebras they constitute the underlying theoretical basis for the specification and verification of real-time systems. Kleene's theorem [K56], stating that the regular (or rational) subsets of Sigma are exactly the recognizable ones (those accepted by finite automata), is one of the cornerstones of automata theory. No such theorem has ...","1997-04-30"
133,32294,"Compositional Verification by Model Checking for Counter-Examples","Jeffrey Fischer; Richard Gerber; Tevfik Bultan;","Many concurrent systems are required to maintain certain safety and liveness properties. One emerging method of achieving confidence in such systems is to statically verify them using model checking. In this approach an abstract, finite-state model of the system is constructed; then an automatic check is made to ensure that the requirements are satisfied by the model. In practice, however, this method is limited by the state space explosion problem. We have developed a compositional method that directly addresses this problem in the context of multi-tasking programs. Our solution depends on three key space-saving ingredients: (1) checking for counter-examples, which leads to simpler search algorithms; (2) automatic extraction of interfaces, which allows a refinement of the finite model -- even before its communicating partners have been compiled; and (3) using propositional ""strengthening assertions"" for the sole purpose of reducing state space. In this paper we present our compositio...","1998-09-05"
134,32610,"A Tool for Symbolic Program Verification and Abstraction","Claire Loiseaux; Susanne Graf;","We give the description of a verification tool taking boolean programs of guarded commands as input; internal representation of programs are sets of Binary Decision Diagrams (BDD) (one for each guarded command). It allows to construct an abstract program of the same form obtained using an abstraction relation given by a boolean expression on ""concrete"" and ""abstract"" variables. The tool allows the verification of CTL formulas on programs. We illustrate its possibilities on an example. 1 Introduction In the domain of program verification an obvious idea is to verify some abstract program instead of the complete specification (called concrete program) depending on the properties to be verified. The motivation is to make the representation of the program model smaller and this for two reasons: one is to make the verification faster; the other is that in most practical cases the model of the concrete program is too large to be verified, whereas an abstraction of it may be sufficiently sma...","1998-09-24"
135,32808,"Wide-Area Traffic: The Failure of Poisson Modeling","Sally Floyd; Vern Paxson;","Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remote-login and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into ""c...","1995-07-19"
136,33208,"Pyramid Broadcasting for Video on DEMAND SERVICE","S. Viswanathan; T. Imielinski;","The proposed scheme of pyramid broadcasting is a new way of rendering Video On Demand service at metropolitan scale. In pyramid broadcasting, the most frequently requested movies are multiplexed on the broadcast network, resulting in radical improvement of access time and efficient bandwidth utilization. We provide analytical and experimental evaluations of pyramid broadcasting based on its implementation on ethernet LAN, illustrating its advantages. 1 Introduction Consider a future Video On Demand (VOD) service where movies are provided to subscribers over a high speed fiber-optic network. Advances in networking technologies will contribute to the realization of the VOD service over the Metropolitan Area Network [11] and [13]. Video objects are very large even in compressed form. One motion picture with NTSC quality video which is 100 minutes long, in uncompressed form occupies 40 GB (Giga Bytes) of storage and 1 GB when compressed according to MPEG standard [5]. These video objects ...","1994-10-11"
137,33212,"Schedulability Analysis for Automated Implementations of Real-Time Object-Oriented Models","A. Ptak; M. Saksena; P. Freedman; P. Rodziewicz;","The increasing complexity of real-time software has led to a recent trend in the use of high-level modeling languages for development of real-time software. One representative example is the modeling language ROOM (real-time object-oriented modeling), which provides features such as object-orientation, state machine description of behaviors, formal semantics for executability of models, and possibility of automated code generation. However, these modeling languages largely ignore the timeliness aspect of real-time systems, and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior. In this paper we consider schedulability analysis for automated implementations of ROOM models, based on the ObjecTime toolset. This work builds on results presented in [8], where we developed some guidelines for the design and implementation of real-time object-oriented models. Using the guidelines, we have modified the run-time system library provided by the ObjecTime...","1999-04-12"
138,33803,"Modeling answer constraints in Constraint Logic Programs","Giorgio Levi; Maurizio Gabbrielli;","The constraint logic programming paradigm CLP(X) (CLP for short) has been proposed by Jaffar and Lassez in order to integrate a generic computational mechanism based on constraints with the logic programming framework. This paradigm retains the semantic properties of logic languages, namely the existence of equivalent operational, model theoretic and fixpoint semantics. Moreover, since computation is performed over the particular domain of computation X , CLP(X) programs have an equivalent ""algebraic"" semantics, i.e. a semantics which is defined directly on the algebraic structure of the domain X . In this paper we propose an extension of such a semantics, for the success set case, in order to fully characterize the operational behaviour of programs. We introduce a framework for defining various notions of models, each corresponding to a specific operationally observable property. The construction is based on a new notion of interpretation (set of constrained atoms), on a natural exten...","1994-09-27"
139,33917,"Searching with Pattern Databases","Jonathan Schaeffer; Joseph C. Culberson;",". The efficiency of A* searching depends on the quality of the lower bound estimates of the solution cost. Pattern databases enumerate all possible subgoals required by any solution, subject to constraints on the subgoal size. Each subgoal in the database provides a tight lower bound on the cost of achieving it. For a given state in the search space, all possible subgoals are looked up, with the maximum cost over all lookups being the lower bound. For sliding tile puzzles, the database enumerates all possible patterns containing N tiles and, for each one, contains a lower bound on the distance to correctly move all N tiles into their correct final location. For the 15-Puzzle, iterative-deepening A* with pattern databases (N=8) reduces the total number of nodes searched on a standard problem set of 100 positions by over 1000-fold. 1 Introduction The A* search algorithm for single-agent search is of fundamental importance in artificial intelligence. Improvements to the search efficiency...","1998-08-19"
140,34249,"Boosting a Weak Learning Algorithm By Majority","Yoav Freund;","We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire in his paper ""The strength of weak learnability"", and represents an improvement over his results. The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant's polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the conc...","1997-01-07"
141,34251,"Active Messages: a Mechanism for Integrated Communication and Computation","David E. Culler; Klaus Erik Schauser; Seth Copen Goldstein; Thorsten Von Eicken;","The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware suppo...","1995-03-29"
142,34309,"An Industrial Strength Theorem Prover for a Logic Based on Common Lisp","J Strother Moore; Matt Kaufmann;","ACL2 is a re-implemented extended version of Boyer and Moore's Nqthm and Kaufmann's Pc-Nqthm, intended for large scale verification projects. This paper deals primarily with how we scaled up Nqthm's logic to an ""industrial strength"" programming language --- namely, a large applicative subset of Common Lisp --- while preserving the use of total functions within the logic. This makes it possible to run formal models efficiently while keeping the logic simple. We enumerate many other important features of ACL2 and we briefly summarize two industrial applications: a model of the Motorola CAP digital signal processing chip and the proof of the correctness of the kernel of the floating point division algorithm on the AMD5K 86 microprocessor by Advanced Micro Devices, Inc. Index terms---formal verification, automatic theorem proving, computational logic, partial functions, total functions, type checking, microcode verification, floating point division, digital signal processing 1. Introducti...","1997-05-19"
143,34792,"Really Visual Temporal Reasoning","G Kutty; L E Moser; L K Dillon; P M Melliar-smith; Y S Ramakrishna;","Real-Time Future Interval Logic (RTFIL) is a visual logic with formulae that resemble timing diagrams. It is a dense real-time temporal logic that is based on two simple temporal primitives: interval modalities for the purely qualitative part and duration predicates for the quantitative part. In this paper we present the logic, and illustrate its use in specifying the railroad crossing example and proving some of its properties. The logic in its propositional form is decidable by reduction to the emptiness problem of Timed Buchi Automata. A theorem prover, based on this decision procedure, has been implemented as part of a graphical proof environment. The proofs of the railroad crossing example have been verified using this theorem prover. The combination of an automated theorem prover and a graphical specification language greatly facilitate the task of verifying real-time proofs. This convenience apart, RTFIL is invariant under real-time stuttering and does not admit instantaneous st...","1995-12-05"
144,35074,"Automatic Abstraction Techniques for Propositional µ-calculus Model Checking","Abelardo Pardo; Gary D. Hachtel;","ion Techniques for Propositional ¯-calculus Model Checking ? Abelardo Pardo and Gary D. Hachtel University of Colorado ECEN Campus Box 425, Boulder, CO, 80309, USA fabel,hachtelg@vlsi.colorado.edu Abstract. An abstraction/refinement paradigm for the full propositional ¯-calculus is presented. No distinction is made between universal or existential fragments. Necessary conditions for conservative verification are provided, along with a fully automatic symbolic model checking abstraction algorithm. The algorithm begins with conservative verification of an initial abstraction. If the conclusion is negative, it derives a ""goal set"" of states which require further resolution. It then successively refines, with respect to this goal set, the approximations made in the sub-formulas, until the given formula is verified or computational resources are exhausted. 1 Introduction The success of formal verification in detecting incorrect designs has been proven over the last decade. However, limi...","1997-03-24"
145,35117,"Automatic Frequency Assignment for Cellular Telephones Using Constraint Satisfaction Techniques","Mats Carlsson; Mats Grindal;","We study the problem of automatic frequency assignment for cellular telephone systems. The frequency assignment problem is viewed as the problem to minimize the unsatisfied soft constraints in a constraint satisfaction problem (CSP) over a finite domain of frequencies involving co-channel, adjacent channel, and co-site constraints. The soft constraints are automatically derived from signal strength prediction data. The CSP is solved using a generalized graph coloring algorithm. Graph-theoretical results play a crucial role in making the problem tractable. Performance results from a real-world frequency assignment problem are presented. We develop the generalized graph coloring algorithm by stepwise refinement, starting from DSATUR and augmenting it with local propagation, constraint lifting, intelligent backtracking, redundancy avoidance, and iterative deepening. Key Words: frequency assignment, constraints, graph coloring, intelligent backtracking, iterative deepening. 1 Introduction...","1993-04-26"
146,35128,"Model Checking Concurrent Systems with Unbounded Integer Variables: Symbolic Representations, Approximations and Experimental Results","Richard Gerber; Tevfik Bultan; William Pugh;","Model checking is a powerful technique for analyzing large, finite-state systems. In an infinite-state system, however, many basic properties are undecidable. In this paper, we present a new symbolic model checker which conservatively evaluates safety and liveness properties on infinite-state programs. We use Presburger formulas to symbolically encode a program's transition system, as well as its model-checking computations. All fixpoint calculations are executed symbolically, and their convergence is guaranteed by using approximation techniques. We demonstrate the promise of this technology on some well-known infinite-state concurrency problems. 1 Introduction In recent years, there has been a surge of progress in the area of automated analysis for finitestate systems. Several reasons for this success are: (1) the development of powerful techniques such as model-checking (e.g., [6, 8]), which can efficiently verify safety and liveness properties; (2) innovative new data structures th...","1998-02-04"
147,35129,"The Dynascope Directing Server: Design and Implementation","Rok Sosic;","As computer systems are becoming increasingly complex, directing tools are gaining in importance. Directing denotes two classes of activities, monitoring and controlling. Monitoring is used for collecting information about the program behavior. Controlling is used to modify the program state in order to change program's future behavior. Some characteristic directing tools are debuggers and performance monitors. Dynascope is a directing platform, which provides basic monitoring and controlling primitives. These primitives are used in building advanced directing applications for networked and heterogeneous environments. Dynascope is integrated with existing programming tools and uses only generic operating system and networking primitives. This paper describes the design and implementation of the directing server, the central component of Dynascope. Dynascope is being used in several applications, including relative debugging, steering agents, and simulator testing. 1 Introduction Incre...","1999-04-05"
148,35468,"Solution Reuse in Dynamic Constraint Satisfaction Problems","Thomas Schiex;","Many AI problems can be modeled as constraint satisfaction problems (CSP), but many of them are actually dynamic: the set of constraints to consider evolves because of the environment, the user or other agents in the framework of a distributed system. In this context, computing a new solution from scratch after each problem change is possible, but has two important drawbacks: inefficiency and instability of the successive solutions. In this paper, we propose a method for reusing any previous solution and producing a new one by local changes on the previous one. First we give the key idea and the corresponding algorithm. Then we establish its properties: termination, correctness and completeness. We show how it can be used to produce a solution, either from an empty assignment, or from any previous assignment and how it can be improved using filtering or learning methods, such as forward-checking or nogoodrecording. Experimental results related to efficiency and stability are given, wit...","1999-02-01"
149,36960,"Reducing BDD Size by Exploiting Functional Dependencies","Alan J. Hu; David L. Dill;","Many researchers have reported that the use of Boolean decision diagrams (BDDs) greatly increases the size of hardware designs that can be formally verified automatically. Our own experience with automatic verification of high-level aspects of hardware design, such as protocols for cache coherence and communications, contradicts previous results; in fact, BDDs have been substantially inferior to brute-force algorithms that store states explicitly in a table. We believe that new techniques will be needed to realize the potential advantages of BDD verification at the protocol level. Here, we identify functionally dependent variables as a common cause of BDD-size blowup, and describe new techniques to avoid the problem. Using the improved algorithm, we reduce an exponentially sized problem to a provably O(n log n)-sized one, achieving several orders of magnitude reduction in BDD size. I INTRODUCTION With the increasing cost and complexity of hardware designs and protocols, formal verificatio...","1997-03-20"
150,37046,"A Formal Approach to Parallelizing Compilers","Eric Van Wyk; Teodor Rus;","This paper describes parallelizing compilers which allow programmers to tune parallel program performance through an interactive dialog. Programmers specify language constructs that define sequential processes, such as assignment or for-loops, to be used as units of computation, while the compiler discovers the parallelism existent in the source program in terms of these units. Programmers may provide target machine architectural features used by compilers to coalesce sequential processes, controlling process granularity and ensuring process load balance. 1 Introduction This paper describes two techniques used by an algebraic parallelizing compiler for program optimization. These techniques allow the programmer to control the granularity of the sequential processes executing a parallelized program and to find opportunities for parallelization and optimization between these processes. The programmer may specify units of computation describing the kind of computations, such as assignme...","1997-04-15"
151,37710,"A Graph-Based Approach To Resolution In Temporal Logic","And Howard Barringer; Clare Dixon; Michael Fisher;",". In this paper, we present algorithms developed in order to implement a clausal resolution method for discrete, linear temporal logics, presented in [Fis91]. As part of this method, temporal formulae are rewritten into a normal form and both `non-temporal' and `temporal' inference rules are applied. Through the use of a graph-based representation for the normal form, ""efficient"" search algorithms can be applied to detect sets of formulae for which temporal resolution is applicable. Further, rather than constructing the full graph structure, our algorithms only explore and construct as little of the graph as possible. These algorithms have been implemented and have been combined with sub-programs performing translation to normal form and non-temporal resolution to produce an integrated resolution based temporal theorem-prover. 1 Introduction Although resolution has been widely used as a decision procedure in classical logics, decision procedures in temporal logic have usually been tab...","1994-06-29"
152,37958,"A Tool Set for deciding Behavioral Equivalences","Jean-claude Fernandez; Laurent Mounier;","This paper deals with verification methods based on equivalence relations between labeled transition systems. More precisely, we are concerned by two practical needs: how to efficiently minimize and compare labeled transition systems with respect to bisimulation or simulation-based equivalence relations. First, we recall the principle of the classical algorithms for the existing equivalence relations, which are based on successive partition refinements of the state space of the labeled transition systems under consideration. However, in spite of their theoretical efficiency, the main drawback of these algorithms is that they require to generate and to store in memory the whole labeled transition systems to be compared or minimized. Therefore, the size of the systems which can be handled in practice remains limited. We propose here another approach, allowing to combine the generation and the verification phases, which is based on two algorithms respectively devoted to the comparison (""o...","1995-11-17"
153,38085,"Software Synthesis for Real-Time Information Processing Systems","And Franco Curatelli; Filip Thoen; Gert Goossens; Marco Cornero; Peter Marwedel;","Software synthesis is a new approach which focusses on the support of embedded systems without the use of operating-systems. Compared to traditional design practices, a better utilisation of the available time and hardware resources can be achieved with software synthesis, because the static information provided by the system specification is fully exploited. In this paper a novel software synthesis approach for real-time information processing systems is presented. Specifically, a flexible execution model for multi-tasking with real-time constraints is proposed, together with an internal representation model which is well suited for the support of concurrency and timing constraints. 1 INTRODUCTION Embedded systems are digital systems for dedicated applications, embedded in a larger (usually non-electric) environment. When the correctness of these systems does not only depend on their functional behaviour, but also on their timing behaviour, they are classified as real-time embedded s...","1998-08-25"
154,38113,"The LyriC Language: Querying Constraint Objects","Alexander Brodsky; Yoram Kornatzky;","Proposed in this paper is a novel data model and its language for querying object-oriented databases where objects may hold spatial, temporal or constraint data, conceptually represented by linear equality and inequality constraints. The proposed LyriC language is designed to provide a uniform and flexible framework for diverse application realms such as (1) constraint-based design in two-, three-, or higher-dimensional space, (2) large-scale optimization and analysis, based mostly on linear programming techniques, and (3) spatial and geographic databases. LyriC extends flat constraint query languages, especially those for linear constraint databases, to structurally complex objects. The extension is based on the object-oriented paradigm, where constraints are treated as first-class objects that are organized in classes. The query language is an extension of the language XSQL, and is built around the idea of extended path expressions. Path expressions in a query traverse nested struct...","1998-09-01"
155,38122,"A Resolution Method for Temporal Logic","Michael Fisher;","In this paper, a resolution method for propositional temporal logic is presented. Temporal formulae, incorporating both past-time and future-time temporal operators, are converted to Separated Normal Form (SNF), then both non-temporal and temporal resolution rules are applied. The resolution method is based on classical resolution, but incorporates a temporal resolution rule that can be implemented efficiently using a graph-theoretic approach. 1 Introduction This report describes a resolution procedure for discrete, linear, propositional temporal logic. This logic incorporates both past-time and future-time temporal operators and its models consist of sequences of states, each sequence having finite past and infinite future. A naive application of the classical resolution rule to temporal logics fails as two complementary literals may not represent a contradictory formula, depending on their temporal context. Because of such problems with resolution, the majority of the decision meth...","1994-06-29"
156,38499,"Proving Temporal Consistency in a New Multimedia Synchronization Model","J. -p. Courtiat; R. C. De Oliveira;","This paper introduces a new synchronization model for the formal specification of multimedia documents. By this approach, an user is able to formalize his document by hierarchically composing a set of presentation and constraint objects which are then automatically translated into a complete RT-LOTOS formal specification (RT-LOTOS is a temporal extension of the standard LOTOS Formal Description Technique) . Verification of a multimedia document aiming to identify potential temporal inconsistencies is then performed using standard reachability analysis developed and implemented for RT-LOTOS. KEYWORDS Multimedia-Hypermedia, Temporal Synchronization, Formal Specification and Verification, LOTOS, RT-LOTOS INTRODUCTION Multimedia documents represent a powerful method of communication integrating a variety of media with different temporal characteristics, e.g. time dependent media such as video, audio or animation, and time independent media, such as text, graphics and images. A central i...","1997-10-16"
157,40009,"Using if-then-else DAGs for Multi-Level Logic Minimization","Kevin Karplus;","This article describes the use of if-then-else dags for multi-level logic minimization. A new canonical form for if-then-else dags, analogous to Bryant's canonical form for binary decision diagrams (bdds), is introduced. Two-cuts are defined for binary decision diagrams, and a relationship is exhibited between general if-then-else expressions and the two-cuts of a bdd for the same function. The canonical form is based on representing the lowest non-trivial two-cut in the corresponding bdd, instead of the highest two-cut, as in Bryant's canonical form. The definitions of prime and irredundant expressions are extended to if-then-else dags.","1992-07-09"
158,40014,"MDL-based Decision Tree Pruning","Jorma Rissanen; Manish Mehta; Rakesh Agrawal;","This paper explores the application of the Minimum Description Length principle for pruning decision trees. We present a new algorithm that intuitively captures the primary goal of reducing the misclassification error. An experimental comparison is presented with three other pruning algorithms. The results show that the MDL pruning algorithm achieves good accuracy, small trees, and fast execution times. Introduction Construction or ""induction"" of decision trees from examples has been the subject of extensive research in the past [Breiman et. al. 84, Quinlan 86]. It is typically performed in two steps. First, training data is used to grow a decision tree. Then in the second step, called pruning, the tree is reduced to prevent ""overfitting"". There are two broad classes of pruning algorithms. The first class includes algorithms like cost-complexity pruning [Breiman et. al., 84], that use a separate set of samples for pruning, distinct from the set used to grow the tree. In many cases, ...","1996-03-22"
159,40057,"Locally Weighted Learning for Control","Andrew W. Moore; Christopher G. Atkeson; Stefan Schaal;","Lazy learning methods provide useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of complex systems. This paper surveys ways in which locally weighted learning, a type of lazy learning, has been applied by us to control tasks. We explain various forms that control tasks can take, and how this affects the choice of learning paradigm. The discussion section explores the interesting impact that explicitly remembering all previous experiences has on the problem of learning to control. Keywords: locally weighted regression, LOESS, LWR, lazy learning, memory-based learning, least commitment learning, forward models, inverse models, linear quadratic regulation (LQR), shifting setpoint algorithm, dynamic programming. 1 Introduction The necessity for self improvement in control systems is becoming more apparent as fields such as robotics, factory automation, and autonomous vehicles become impeded by the complexity of inventing...","1999-02-08"
160,40391,"Verifying Systems with Integer Constraints and Boolean Predicates: A Composite Approach","Christopher League; Richard Gerber; Tevfik Bultan;","Symbolic model checking has proved highly successful for large finite-state systems, in which states can be compactly encoded using binary decision diagrams (BDDs) or their variants. The inherent limitation of this approach is that it cannot be applied to systems with an infinite number of states -- even those with a single unbounded integer. Alternatively, we recently proposed a model checker for integer-based systems that uses Presburger constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of Boolean and (unordered) enumerated types -- which possess no natural mapping to the Euclidean coordinate space. In this paper we describe a model checker which combines the strengths of both approaches. We use a composite model, in which a formula's valuations are encoded in a mixed BDD-Presburger form, depending on the variables used. We demonstrate our technique's effecti...","1998-01-13"
161,41101,"Simple On-the-fly Automatic Verification of Linear Temporal Logic","D. Peled; Den Dolech Eindhoven; M. Y. Vardi; P. Wolper; R. Gerth;","We present a tableau-based algorithm for obtaining an automaton from a temporal logic formula. The algorithm is geared towards being used in model checking in an ""on-the-fly"" fashion, that is the automaton can be constructed simultaneously with, and guided by, the generation of the model. In particular, it is possible to detect that a property does not hold by only constructing part of the model and of the automaton. The algorithm can also be used to check the validity of a temporal logic assertion. Although the general problem is PSPACE-complete, experiments show that our algorithm performs quite well on the temporal formulas typically encountered in verification. While basing linear-time temporal logic model-checking upon a transformation to automata is not new, the details of how to do this efficiently, and in ""on-the-fly"" fashion have never been given. Keywords Automatic Verification, Linear Temporal Logic, Buchi Automata, Concurrency, Specification. 1 Introduction Checking auto...","1997-06-26"
162,41366,"Stacked Generalization","David H. Wolpert;",": This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation 's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After...","1992-05-06"
163,41420,"Intelligence Without Reason","Rodney A. Brooks;","Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation. Copyright c fl Massachusetts Institute of Technology, 1991 This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Suppo...","1998-03-22"
164,41722,"A Toolbox for the Verification of LOTOS Programs","Anne Rasse; Carlos Rodriguez; Hubert Garavel; Joseph Sifakis; Laurent Mounier;","This paper presents the tools Ald' ebaran, Caesar, Caesar.adt and Cl' eop atre which constitute a toolbox for compiling and verifying Lotos programs. The principles of these tools are described, as well as their performances and limitations. Finally, the formal verification of the rel/REL atomic multicast protocol is given as an example to illustrate the practical use of the toolbox. Keywords: reliability, formal methods, Lotos, verification, validation, model-based methods, model checking, transition systems, bisimulations, temporal logics, diagnostics Introduction There is an increasing need for reliable software, which is especially critical in some areas such as communication protocols, distributed systems, real-time control systems, and hardware synthesis systems. It is now agreed that reliability can only be achieved through the use of rigorous design techniques. This has motivated a lot of research on specification formalisms and associated verification methods and tools. Ver...","1998-09-01"
165,41832,"GRASP - A New Search Algorithm for Satisfiability","João P. Marques Silva; Karem A. Sakallah;","This report introduces GRASP (Generic seaRch Algorithm for the Satisfiability Problem), an integrated algorithmic framework for SAT that unifies several previously proposed search-pruning techniques and facilitates identification of additional ones. GRASP is premised on the inevitability of conflicts during search and its most distinguishing feature is the augmentation of basic backtracking search with a powerful conflict analysis procedure. Analyzing conflicts to determine their causes enables GRASP to backtrack non-chronologically to earlier levels in the search tree, potentially pruning large portions of the search space. In addition, by ""recording"" the causes of conflicts, GRASP can recognize and preempt the occurrence of similar conflicts later on in the search. Finally, straightforward bookkeeping of the causality chains leading up to conflicts allows GRASP to identify assignments that are necessary for a solution to be found. Experimental results obtained from a large number of ...","1996-04-18"
166,41878,"On Overfitting Avoidance As Bias","David H. Wolpert;",": In supervised learning it is commonly believed that Occam's razor works, i.e., that penalizing complex functions helps one avoid ""overfitting"" functions to data, and therefore improves generalization. It is also commonly believed that cross-validation is an effective way to choose amongst algorithms for fitting functions to data. In a recent paper, Schaffer (1993) presents experimental evidence disputing these claims. The current paper consists of a formal analysis of these contentions of Schaffer's. It proves that his contentions are valid, although some of his experiments must be interpreted with caution. In doing so, it proves that there are ""as many"" scenarios in which a learning algorithm using cross-validation fails as in which it succeeds (and similarly for any other learning algorithm), as far as off-training set behavior is concerned. Interestingly, this proof also indicates that there are as many scenarios in which use of a test set fails to accurately predict behavior off ...","1995-09-18"
167,42422,"SIS: A System for Sequential Circuit Synthesis","Alberto Sangiovanni-vincentelli; Alexander Saldanha; Cho Moon; Ellen M. Sentovich; Hamid Savoj; Kanwar Jit Singh; Luciano Lavagno; Paul R. Stephan; Rajeev Murgai; Robert K. Brayton;","SIS is an interactive tool for synthesis and optimization of sequential circuits. Given a state transition table, a signal transition graph, or a logic-level description of a sequential circuit, it produces an optimized net-list in the target technology while preserving the sequential input-output behavior. Many different programs and algorithms have been integrated into SIS, allowing the user to choose among a variety of techniques at each stage of the process. It is built on top of MISII [5] and includes all (combinational) optimization techniques therein as well as many enhancements. SIS serves as both a framework within which various algorithms can be tested and compared, and as a tool for automatic synthesis and optimization of sequential circuits. This paper provides an overview of SIS. The first part contains descriptions of the input specification, STG (state transition graph) manipulation, new logic optimization and verification algorithms, ASTG (asynchronous signal transitio...","1995-01-26"
168,42445,"Specification and Verification of Real-time Embedded Systems using Time-constrained Reactive Automata","Azer Bestavros;","The vital role that real-time embedded systems are playing and will continue to play in our world, coupled with their increasingly complex and critical nature, demand a rigorous and systematic treatment that recognizes their unique requirements. The Time-constrained Reactive Automaton (TRA) is a formal model of computation that admits these requirements. Among its salient features is a fundamental notion of space and time that restricts the expressiveness of the model in a way that allows the specification of only reactive, spontaneous, and causal computations. Using the TRA formalism, there is no conceptual distinction between a system and a property; both are specified as formal objects. This reduces the verification process to that of establishing correspondences -- namely preservation and implementation relationships -- between such objects. In this paper, we present the TRA model and briefly overview our experience in using it in the specification and verification of real-time e...","1997-04-06"
169,42528,"Planning With Deadlines in Stochastic Domains","Ann Nicholson; Jak Kirman; Leslie Pack Kaelbling; Thomas Dean;","We provide a method, based on the theory of Markov decision problems, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world state; the planner must find a policy (mapping from states to actions) that maximizes future rewards. Standard goals of achievement, as well as goals of maintenance and prioritized combinations of goals, can be specified in this way. An optimal policy can be found using existing methods, but these methods are at best polynomial in the number of states in the domain, where the number of states is exponential in the number of propositions (or state variables) . By using information about the starting state, the reward function, and the transition probabilities of the domain, we can restrict the planner's attention to a set of world states that are likely to be encountered in satisfying the goal. Furthermore, the planner can generate more or less complete plans depending on the time it has avail...","1995-09-27"
170,42779,"A Unified Signal Transition Graph Model for Asynchronous Control Circuit Synthesis","Alberto Sangiovanni-vincentelli; Alexandre Yakovlev; Luciano Lavagno;","Characterization of the behavior of an asynchronous system depending on the delay of components and wires is a major task facing designers. Some of these delays are outside the designer's control, and in practice may have to be assumed unbounded. The existing literature offers a number of analysis and specification models, but lacks a unified framework to verify directly if the circuit specification admits a correct implementation under these hypotheses. Our aim is to fill exactly this gap, offering both low-level (analysis-oriented) and high-level (specification-oriented) models for asynchronous circuits and the environment where they operate, together with strong equivalence results between the properties at the two levels. One interesting side result is the precise characterization of classical static and dynamic hazards in terms of our model. Consequently the designer can check the specification and directly decide if the behavior of any implementation will depend, e.g., on the d...","1995-01-26"
171,42942,"On Variable Ordering and Decomposition Type Choice in OKFDDs","Andrea Jahnke; Bernd Becker; Rolf Drechsler;","We present methods for the construction of small Ordered Kronecker Functional Decision Diagrams (OKFDDs). OKFDDs are a generalization of Ordered Binary Decision Diagrams (OBDDs) and Ordered Functional Decision Diagrams (OFDDs) as well. Starting with an upper bound for the size of an OKFDD representing a tree-like circuit we develope different heuristics to find good variable orderings and decomposition types for OKFDDs representing twolevel and multi-level circuits, respectively. Experimental results are presented to show the efficiency of our approaches. I. Introduction Decision Diagrams (DDs) are often used in CAD systems for efficient representation and manipulation of Boolean functions. The most popular data structure are Ordered Binary Decision Diagrams (OBDDs) [5] that are used in many applications [17, 7]. Nevertheless, some relevant classes of Boolean functions cannot be represented efficiently by OBDDs [6, 18, 2]. As one alternative Ordered Functional Decision Diagrams (OFD...","1995-06-19"
172,43727,"A Construction and Analysis Tool Based on the Stochastic Process Algebra TIPP","Holger Hermanns; Michael Rettelbach; Vassilis Mertsiotakis;","1 There are many ways to incorporate a notion of time into process algebras in order to integrate functional design and performance analysis. One major research strand, stochastic process algebras, concentrates on the annotation of actions with exponentially distributed random variables. This paper presents a tool for the functional analysis and performance evaluation of complex systems based on the stochastic process algebra paradigm. The TIPP-tool provides facilities for model specification, reachability analysis, as well as several numerical algorithms for the solution of the underlying Markov chain and the computation of performance measures. 1 Introduction Stochastic process algebras (SPA) have been introduced as an extension of classical process algebras, like CCS or CSP, with timing information aiming mainly at the integration of functional design and quantitative analysis of computer systems. Time is represented by attaching random variables to every activity in the model, de...","1996-01-17"
173,43984,"A Hardware Implementation of Pure Esterel","G. Berry;","Esterel is a synchronous concurrent programming language dedicated to reactive systems (controllers, protocols, man-machine interfaces, etc.). Esterel has an efficient standard software implementation based on well-defined mathematical semantics. We present a new hardware implementation of the pure synchronization subset of the language. Each program generates a specific circuit that responds to any input in one clock cycle. When the source program satisfies some statically checkable dynamic properties, the circuit is shown to be semantically equivalent to the source program. The hardware translation has been effectively implemented on the programmable active memory Perle0 developed by J. Vuillemin and his group at Digital Equipment.","1994-10-10"
174,44569,"Control Strategies for a Stochastic Planner","Jonathan Tash; Stuart Russell;","We present new algorithms for local planning over Markov decision processes. The base-level algorithm possesses several interesting features for control of computation, based on selecting computations according to their expected benefit to decision quality. The algorithms are shown to expand the agent's knowledge where the world warrants it, with appropriate responsiveness to time pressure and randomness. We then develop an introspective algorithm, using an internal representation of what computational work has already been done. This strategy extends the agent's knowledge base where warranted by the agent's world model and the agent's knowledge of the work already put into various parts of this model. It also enables the agent to act so as to take advantage of the computational savings inherent in staying in known parts of the state space. The control flexibility provided by this strategy, by incorporating natural problem-solving methods, directs computational effort towards where it'...","1998-12-17"
175,45071,"Register Allocation with Instruction Scheduling: a New Approach","Shlomit S. Pinter;","We present a new framework in which considerations of both register allocation and instruction scheduling can be applied uniformly and simultaneously. In this framework an optimal coloring of a graph, called the parallelizable interference graph, provides an optimal register allocation and preserves the property that no false dependences are introduced, thus all the options for parallelism are kept for the scheduler to handle. For this framework we provide heuristics for trading off parallel scheduling with register spilling.","1998-03-03"
176,45228,"Implementing Set-Oriented Production Rules as an Extension to Starburst","Bruce G. Lindsay; Jennifer Widom; Roberta Jo Cochrane;",". This paper describes the implementation of a set-oriented database production rule language proposed in earlier papers. Our implementation uses the extensibility features of the Starburst database system, and rule execution is fully integrated into database query and transaction processing. 1 Introduction In database systems, a production rules facility allows definition of database operations that are executed automatically whenever certain events occur or conditions are met. Production rules in database systems can be used for enforcing integrity constraints, maintaining derived data, and building efficient knowledge-base and expert systems. In [WF89,WF90] we propose a syntax for specifying production rules in relational database systems and a semantics for rule execution. In keeping with the setoriented approach of relational data manipulation languages, our production rules are set-oriented---they are triggered by sets of changes to the database and may perform sets of changes....","1999-02-05"
177,45318,"An Automata Theoretic Approach to Temporal Logic","Gjalt G. De Jong;","A syntax directed mapping is presented from Propositional Temporal Logic (PTL) formulae to Muller type finite automata. This is a direct and much more elegant and easier to implement approach than previously described methods. Most of these methods are based on tableau methods for satisfiability checking, and after that a Buchi type of automaton is extracted. Buchi and Muller automata are equally expressive. However, Muller automata have nicer properties than Buchi automata, for instance deterministic Muller automata are expressive as non-deterministic ones, while this is not true for Buchi automata. Also deterministic Buchi automata are not closed under complement. This transformation is the first step in a decision procedure, since the resulting Muller automaton represents the models of the temporal logic formula, and on which further verification and analysis can be performed. 1. Introduction Temporal logic has proven to be a well suited formalism for program verification [1] as we...","1996-02-21"
178,45448,"A Unifying Theoretical Background for Some BDD-based Data Structures","Christoph Meinel; Fachbereich Iv; Mathematik Informatik; Trierer Forschungsberichte;","Abstract In the paper, we propose a general concept (in what follows denoted by TBDD) for Boolean functions manipulation that is based on cube transformations. The basic idea is to manipulate a Boolean function by converting it by means of a cube transformation into a function that can be efficiently represented and manipulated in terms of ordered binary decision diagrams (OBDD's). We show that the new concept unifies and simplifies known BDD--based data structures considerably, and allows to work in all cases with the simple--structured and well--comprehended data struture of OBDD's (what is especially important from the point of practical applications.) Further, to give an example how TBDD's open new ways in the search for efficient data structures for Boolean functions, we discuss the data structure of typed kFBDD's. Keywords. OBDD's, data structures for Boolean manipulation, cube transformation. 1 Introduction One of the fundamental problems in computer-aided circuit design and ...","1998-12-15"
179,45724,"Constraint Query Languages","Gabriel M. Kuper; Paris C. Kanellakis; Peter Z. Revesz;","We investigate the relationship between programming with constraints and database query languages. We show that efficient, declarative database programming can be combined with efficient constraint solving. The key intuition is that the generalization of a ground fact, or tuple, is a conjunction of constraints over a small number of variables. We describe the basic Constraint Query Language design principles and illustrate them with four classes of constraints: real polynomial inequalities, dense linear order inequalities, equalities over an infinite domain, and boolean equalities. For the analysis, we use quantifier elimination techniques from logic and the concept of data complexity from database theory. This framework is applicable to managing spatial data and can be combined with existing multidimensional searching algorithms and data structures. Keywords: database queries, spatial databases, data complexity, quantifier elimination, constraint logic programming, relational calculu...","1997-04-07"
180,45751,"How many Decomposition Types do we need?","Bernd Becker; Rolf Drechsler;","Decision Diagrams (DDs) are used in many applications in CAD. Various types of DDs, e.g. BDDs, FDDs, KFDDs, differ by their decomposition types. In this paper we investigate the different decomposition types and prove that there are only three that really help to reduce the size of DDs. 1 Introduction Decision Diagrams (DDs) are successfully applied in many fields of design automation, e.g. [17, 4, 1, 14, 7, 24, 11, 2, 9]. The most popular type of DD is the Ordered Binary Decision Diagram (OBDD) allowing efficient representation and manipulation of Boolean functions [5]. The more recent techniques have made it possible to handle (some) large functions without any basic variation of the OBDD concept itself. The dynamic variable ordering with sifting introduced by Rudell [21] allows to represent examples which could not be represented by any previous heuristic methods. Moreover, the variable ordering in [21] is handled by the package itself, alleviating the need for variable ordering ...","1995-06-19"
181,45799,"Compositional Model Checking Of Partially Ordered State Spaces","Scott Hazelhurst;","Symbolic trajectory evaluation (STE) --- a model checking technique based on partial order representations of state spaces --- has been shown to be an effective model checking technique for large circuit models. However, the temporal logic that it supports is restricted, and as with all verification techniques has significant performance limitations. The demand for verifying larger circuits, and the need for greater expressiveness requires that both these problems be examined. The thesis develops a suitable logical framework for model checking partially ordered state spaces: the temporal logic TL and its associated satisfaction relations, based on the quaternary logic Q. TL is appropriate for expressing the truth of propositions about partially ordered state spaces, and has suitable technical properties that allow STE to support a richer temporal logic. Using this framework, verification conditions called assertions are defined, a generalised version of STE is developed, and three STE-...","1996-01-27"
182,46600,"Algebraic Processing of Programming Languages","Teodor Rus;","Current methodology for compiler construction evolved from the need to release programmers form the burden of writing machine-language programs. This methodology does not assume a formal concept of a programming language and is not based on mathematical algorithms that model the behavior of a compiler. The side effect is that compiler implementation is a difficult task and the correctness of a compiler usually is not proven mathematically. Moreover, a compiler may be based on assumptions about its source and target languages that are not necessarily acceptable for another compiler that has the same source and target languages. The consequence is that programs are not portable between platforms of machines and between generations of languages. In addition, while a conventional compiler freezes the notation that programmers can use to develop their programs the problem domain evolves and requires extensions that are not supported by the compiler. These problems are addressed by two direc...","1995-11-20"
183,46663,"Integrated Support For Data Archaeology","Alan Lazar; Alex Borgida; Boris Altman; Deborah L. Mcguinness; Fern Halper; Loren G. Terveen; Lori Alperin Resnick; Peter G. Selfridge; Ronald J. Brachman; Thomas Kirk;","Corporate databases increasingly are being viewed as potentially rich sources of new and valuable knowledge. Various approaches to""discovering"" or ""mining "" such knowledge have been proposed. Here we identify an important and previously ignored discovery task, which we call data archaeology. Data archaeology is a skilled human task, in which the knowledge sought depends on the goals of the analyst, cannot be specified in advance, and emerges only through an iterative process of data segmentation and analysis. We describe a system that supports the data archaeologist with a natural, objectoriented representation of an application domain, a powerful query language and database translation routines, and an easy-to-use and flexible user interface that supports interactive exploration. A formal knowledge representation system provides the core technology that facilitates database integration, querying, and the reuse of queries and query results. Keywords: data archaeology, knowledge discov...","1997-09-25"
184,46908,"Semantic Foundations of Concurrent Constraint Programming","Martin Rinard; Vijay A. Saraswat; Xerox Parc;","Concurrent constraint programming [Sar89,SR90] is a simple and powerful model of concurrent computation based on the notions of store-as constraint and process as information transducer. The store-as-valuation conception of von Neumann computing is replaced by the notion that the store is a constraint (a finite representation of a possibly infinite set of valuations) which provides partial information about the possible values that variables can take. Instead of ""reading"" and ""writing"" the values of variables, processes may now ask (check if a constraint is entailed by the store) and tell (augment the store with a new constraint). This is a very general paradigm which subsumes (among others) nondeterminate data-flow and the (concurrent)(constraint) logic programming languages. This paper develops the basic ideas involved in giving a coherent semantic account of these languages. Our first contribution is to give a simple and general formulation of the notion that a constraint system is.","1995-11-28"
185,46959,"An Empirical Analysis of the Benefit of Decision Tree Size Biases as a Function of Concept Distribution","Patrick M. Murphy;","The results reported here empirically show the benefit of decision tree size biases as a function of concept distribution. First, it is shown how concept distribution complexity (the number of internal nodes in the smallest decision tree consistent with the example space) affects the benefit of minimum size and maximum size decision tree biases. Second, a policy is described that defines what a learner should do given knowledge of the complexity of the distribution of concepts. Third, explanations for why the distribution of concepts seen in practice is amenable to the minimum size decision tree bias are given and evaluated empirically. Keywords: Induction, Decision Trees, Bias 1 Introduction Top down induction of decision trees has been significantly studied by a number of researchers, e.g. (Breiman, Friedman, Olshen, & Stone, 1984) and (Quinlan, 1986). The majority of the algorithms that construct decision trees from examples use splitting heuristics that aim to minimize the size ...","1995-08-08"
186,47148,"Bilattices and Modal Operators","Matthew L. Ginsberg;","A bilattice is a set equipped with two partial orders and a negation operation that inverts one of them while leaving the other unchanged; it has been suggested that the truth values used by inference systems should be chosen from such a structure instead of the two-point set ft; fg. Given such a choice, we redefine a modal operator to be a function on the bilattice selected. We show that this definition generalizes both Kripke's possible worlds approach and Moore's autoepistemic logic, and also use recent developments in the formalization of logic programs to construct a procedure for determining the truth value of a sentence when the underlying knowledge base involves modal operators of this form. 1 Introduction Modal operators are used in a variety of ways in AI, including reasoning about knowledge and belief, about time, and applications to nonmonotonic inference [11, 15, 13, and others]. The semantics assigned to a particular modal operator are usually determined using a scheme...","1993-10-22"
187,47387,"Application of Constraint Logic Programming for VLSI CAD Tools","Ingolf Markhof; Renate Beckmann; Ulrich Bieker;",": This paper describes the application of CLP (constraint logic programming) to several digital circuit design problems. It is shown that logic programming together with efficient constraint propagation techniques is an adequate programming environment for complex real world problems like high level synthesis, simulation, code generation, and memory synthesis. Different types of constraints - Boolean, integer, symbolic, structural, and type binding ones - are used to express relations between the components of a digital circuit and efficient propagation is achieved by the coroutining mechanism. To deal with the increasing complexity of digital circuits we use HDL's (hardware description languages) to represent structure and behaviour of circuits. I Issues This paper describes the successful use of logic programming extended by constraints to solve complex real world problems in the area of digital circuit design: high level synthesis, simulation, code generation, and memory synthesi...","1998-06-17"
188,47726,"Cross-Validation and the Bootstrap: Estimating the Error Rate of a Prediction Rule","Bradley Efron; Robert Tibshirani;","A training set of data has been used to construct a rule for predicting future responses. What is the error rate of this rule? The traditional answer to this question is given by cross-validation. The cross-validation estimate of prediction error is nearly unbiased, but can be highly variable. This article discusses bootstrap estimates of prediction error, which can be thought of as smoothed versions of cross-validation. A particular bootstrap method, the 632+ rule, is shown to substantially outperform cross-validation in a catalog of 24 simulation experiments. Besides providing point estimates, we also consider estimating the variability of an error rate estimate. All of the results here are nonparametric, and apply to any possible prediction rule: however we only study classification problems with 0-1 loss in detail. Our simulations include ""smooth"" prediction rules like Fisher's Linear Discriminant Function, and unsmooth ones like Nearest Neighbors. 1 Introduction This article conc...","1998-08-31"
189,47880,"An Automata-Theoretic Approach to Linear Temporal Logic","Moshe Y. Vardi;",". The automata-theoretic approach to linear temporal logic uses the theory of automata as a unifying paradigm for program specification, verification, and synthesis. Both programs and specifications are in essence descriptions of computations. These computations can be viewed as words over some alphabet. Thus,programs and specificationscan be viewed as descriptions of languagesover some alphabet. The automata-theoretic perspective considers the relationships betweenprograms and their specifications as relationships between languages. By translating programs and specifications to automata, questions about programs and their specifications canbe reduced to questions about automata. More specifically, questions such as satisfiability of specifications and correctness of programs with respect to their specifications can be reduced to questions such as nonemptiness and containment of automata. Unlike classical automata theory, which focused on automata on finite words, the applications to p...","1996-07-18"
190,48013,"Analyzing Partially-Implemented Real-Time Systems","George S. Avrunin; James C. Corbett; Laura K. Dillon;","|Most analysis methods for real-time systems assume that all the components of the system are at roughly the same stage of development and can be expressed in a single notation, such as a specication or programming language. There are, however, many situations in which developers would benet from tools that could analyze partially implemented systems, those for which some components are given only as high-level specications while others are fully implemented in a programming language. In this paper, we propose a method for analyzing such partially-implemented real-time systems. Here we consider real-time concurrent systems for which some components are implemented in Ada and some are partially specied using regular expressions and Graphical Interval Logic (GIL), a real-time temporal logic. We show how to construct models of the partially implemented systems that account for such properties as run-time overhead and scheduling of processes, yet support tractable analysis of nontrivial ...","1999-01-05"
191,48067,"Computing Least Common Subsumers in Description Logics","Alex Borgida; Haym Hirsh; William W. Cohen;","Description logics are a popular formalism for knowledge representation and reasoning. This paper introduces a new operation for description logics: computing the ""least common subsumer"" of a pair of descriptions. This operation computes the largest set of commonalities between two descriptions. After arguing for the usefulness of this operation, we analyze it by relating computation of the least common subsumer to the well-understood problem of testing subsumption; a close connection is shown in the restricted case of ""structural subsumption"". We also present a method for computing the least common subsumer of ""attribute chain equalities"", and analyze the tractability of computing the least common subsumer of a set of descriptions---an important operation in inductive learning. 1. Introduction and Motivation Description logics (DLs) or terminological logics are a family of knowledge representation and reasoning systems that have found applications in several diverse areas, ranging f...","1999-01-03"
192,48128,"Combining Multiple Models of Computation for Scheduling and Allocation","D. Ziegenbein; J. Teich; K. Richter; L. Thiele; R. Ernst; Tu Braunschweig;","Many applications include a variety of functions from different domains. Therefore, they are best modeled with a combination of different modeling languages. For a sound design process and improved design space utilization, these different input models should be mapped to a common representation. In this paper, we present a common internal representation that integrates the aspects of several models of computation and is targeted to scheduling and allocation. The representation is explained using an example combining a classical process model as used in real-time operating systems (RTOS) with the synchronous data flow model (SDF). 1 Introduction There are numerous system specification and modeling languages with fundamental differences in their underlying models of computation, such as event driven computation or data flow. Many complex designs use more than one modeling language to describe system functions of different characteristics. Since these functions are rarely completely ind...","1998-01-29"
193,48543,"Efficient Variable Ordering Heuristics for Shared ROBDD","Ibrahim N. Hajj; Janak H. Patel; Pi-yu Chung;","In this paper we describe several ordering heuristics for shared, Reduced and Ordered Binary Decision Diagrams (ROBDDs). These heuristics have been tested on ISCAS and MCNC benchmark circuits. In all examples, the ordering is accomplished in a few seconds and generates smaller shared ROBDDs than other previously proposed heuristics. The objective is to provide a fast way to generate shared ROBDDs of reasonable sizes, and the results could be used as a good initial solution to any semi-exhaustive ordering method [2, 3] to further reduce the sizes. 1 Introduction Reduced and Ordered Binary Decision Diagrams (ROBDD) proposed by Bryant [4] have been shown to be a practical way for representing boolean functions. ROBDDs have played an important role in logic synthesis, design verification and design correction [5]. In this paper, we are interested in generating shared ROBDDs for gate-level combinational circuits. Boolean functions at different primary outputs are represented in terms of pr...","1996-02-22"
194,48598,"Design of an Embedded Video Compression System - A Quantitative Approach","Raul Camposano;","A good system design should spend resources in a way that the system is well balanced and a high performance/cost ratio is achieved. In a quantitative approach the decisions upon the most suitable size for each component is based on measured analysis data. Adopting this approach for Embedded System (ES) design requires to measure analysis data for a specific application domain. The paper presents the analysis environment of the CASTLE system. The environments supports measurement and visualization of analysis data for complex applications. As an example the requirement analysis of an MPEG-1 video decoder is presented. The analysis data are used for specifying the structure of an embedded video compression system. 1.0 Introduction Embedded Systems(ESs) perform some dedicated function within a host system [12]. Host systems range from traditional computers to automobiles or planes. Hence, ESs are ubiqitous and perform functions as widespread as video compression or industrial controlli...","1994-11-04"
195,48747,"A Decision Procedure for the Subtype Relation on Intersection Types with Bounded Variables","Benjamin C. Pierce;","We introduce an extension of the intersection type discipline in which types may contain variables with upper and lower bounds, present an algorithm for deciding the subtype relation in the extended system, and prove that the algorithm is correct. This work was supported in part by the Office of Naval Research and the Defense Advanced Research Projects Agency (DOD) under contract number N00014-84-K-0415. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of DARPA or the U.S. Government. 1 INTRODUCTION 1 1 Introduction The intersection type discipline [?] is a simple, yet powerful language for describing the behavior of programs. Beyond its intrinsic theoretical interest [?, ?], it appears to be an attractive foundation for the type systems of practical programming languages [?, ?]. Although the usual formulations do not allow quantification over types---there ...","1996-08-18"
196,48796,"An Information-Maximization Approach to Blind Separation and Blind Deconvolution","Anthony J. Bell; Terrence J. Sejnowski;","We derive a new self-organising learning algorithm which maximises the information transferred in a network of non-linear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximisation has extra properties not found in the linear case (Linsker 1989). The non-linearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalisation of Principal Components Analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to ten speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and re...","1995-08-17"
197,49066,"Tcl and the Tk Toolkit","John K. Ousterhout;","this document, but I'm also interested in hearing about inaccuracies, typos, or any other constructive criticism you might have. 2 DRAFT (8/12/93): Distribution Restricted 1 DRAFT (8/12/93): Distribution Restricted","1995-12-20"
198,49370,"Stochastic Process Algebras - Constructive Specification Techniques Integrating Functional, Performance and Dependability Aspects","H. Hermanns; M. Rettelbach; N. Gotz; U. Herzog; V. Mertsiotakis;","This paper is organised as follows: in Section 2 we will give an overview on how everything began. We will introduce TIPP as an example of a Stochastic Process Algebra in Section 3, where we especially highlight the nice and important algebraic features of the approach. Section 4 illuminates the concept by means of a small example. Section 5 gives references to various case studies and Section 6 discusses tool support that is actually available.","1995-07-21"
199,49386,"A tutorial on Stålmarck's proof procedure for propositional logic","Gunnar Stalmarck; Mary Sheeran;",". We explain Stalmarck's proof procedure for classical propositional logic. The method is implemented in a commercial tool that has been used successfully in real industrial verification projects. Here, we present the proof system underlying the method, and motivate the various design decisions that have resulted in a system that copes well with the large formulas encountered in industrial-scale verification. We also discuss possible applications in Computer Aided Design of electronic circuits. c flSpringer To appear in the proceedings of FMCAD'98, published in Springer LNCS. A tutorial on Stalmarck's proof procedure for propositional logic Mary Sheeran and Gunnar Stalmarck Prover Technology AB and Chalmers University of Technology, Sweden Abstract. We explain Stalmarck's proof procedure for classical propositional logic. The method is implemented in a commercial tool that has been used successfully in real industrial verification projects. Here, we present the proof system underly...","1998-10-23"
200,49449,"Automated Protocol Validation in Argos , assertion proving and scatter searching","Gerard J. Holzmann;","Argos is a validation language for data communication protocols. To validate a protocol a model in Argos is constructed consisting of a control flow specification and a formal description of the correctness requirements. This model can be compiled into a minimized lower level description that is based on a formal model of communicating finite state machines. An automated protocol validator trace uses these minimized descriptions to perform a partial symbolic execution of the protocol to establish its correctness for the given requirements. IEEE Trans. on Software Engineering, Vol. 13, No. 6, June 1987, pp. 683-697. 1. Introduction In the last few years some experience has been gained with the capabilities and the restrictions of automated protocol validators [2,3,4,8,10,11,12,13]. The first validation methods required considerable effort from the user to translate an abstract protocol specification into the formal code used in the validation process. The analyses were often run in ba...","1999-04-12"
201,49474,"UPPAAL - a Tool Suite for Automatic Verification of Real-Time Systems","Fredrik Larsson; Johan Bengtsson; Kim Larsen; Paul Pettersson; Wang Yi;",". Uppaal is a tool suite for automatic verification of safety and bounded liveness properties of real-time systems modeled as networks of timed automata. It includes: a graphical interface that supports graphical and textual representations of networks of timed automata, and automatic transformation from graphical representations to textual format, a compiler that transforms a certain class of linear hybrid systems to networks of timed automata, and a model--checker which is implemented based on constraint--solving techniques. Uppaal also supports diagnostic model-checking providing diagnostic information in case verification of a particular real-time systems fails. The current version of Uppaal is available on the World Wide Web via the Uppaal home page http://www.docs.uu.se/docs/rtmv/uppaal. 1 Introduction Uppaal is a new tool suite for automatic verification of safety and bounded liveness properties of networks of timed automata [13, 8, 6]. The tool was developed during the spring ...","1995-12-01"
202,49996,"On the Self-Similar Nature of Ethernet Traffic","Bellcore Bellcore; Daniel V. Wilson; Murad S. Taqqu; Walter Willinger; Will E. Leland;","We demonstrate that Ethernet local area network (LAN) traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based (B-ISDN) networks, and that aggregating streams of such traffic typically intensifies the selfsimilarity (""burstiness"") instead of smoothing it. Intuitively, the critical characteristic of this self-similar traffic is that there is no natural length of a ""burst"": at every time scale ranging from a few milliseconds to minutes and hours, similar-looking traffic bursts are evident. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We al...","1998-01-03"
203,50000,"Partial Model Checking with ROBDDs","Henrik Reif; Staunstrup Niels Maretti;",". This paper introduces a technique for localizing model checking of concurrent state-based systems. The technique, called partial model checking, is fully automatic and performs model checking by gradually specializing the specification with respect to the concurrent components one by one, computing a ""concurrent weakest precondition."" Specifications are invariance properties and the concurrent components are sets of transitions. Both are expressed as predicates represented by Reduced Ordered Binary Decision Diagrams (ROBDDs). The self-reducing properties of ROBDDs are important for the success of the technique. We describe experimental results obtained on four different examples. 1 Introduction The major problem with automatic model checking is what has been known as the state-explosion problem: the combinatorial explosion of global states when combining loosely coupled concurrent components. Many techniques to overcome this problem have been proposed. Some techniques require manual...","1997-02-19"
204,50214,"A Method for Verifying Properties of Modechart Specifications","Douglas A. Stuart; Farnam Jahanian;","As software control of time-critical functions in embedded systems becomes more common, a means for the precise specification of their behavior becomes increasingly important. Modechart is a graphical specification language introduced to meet this need. This paper presents a method for verifying properties of systems specified in Modechart. The proposed approach makes use of a computation graph which takes advantage of the structuring inherent in a Modechart specification. Two classes of properties are presented for which decision procedures are developed. 1. Introduction Modechart is a graphical specification language developed to provide a compact and structured way to represent real-time systems [Jahanian & Mok 88]. Although similar in some ways to Harel's Statecharts [Harel 86], Modechart is specifically tailored to representing time-critical systems. The semantics of Modechart is given in Real Time Logic (RTL), a logic for the specification and analysis of such systems [Jahanian ...","1995-02-23"
205,50371,"Proof-Carrying Code","George C. Necula; Peter Lee;","This report describes Proof-Carrying Code, a software mechanism that allows a host system to determine with certainty that it is safe to execute a program supplied by an untrusted source. For this to be possible, the untrusted code supplier must provide with the code a safety proof that attests to the code's safety properties. The code consumer can easily and quickly validate the proof without using cryptography and without consulting any external agents. In order to gain preliminary experience with proof-carrying code, we have performed a series of case studies. In one case study, we write safe assembly-language network packet filters. These filters can be executed with no run-time overhead, beyond a one-time cost of 1 to 3 milliseconds for validating the attached proofs. The net result is that our packet filters are formally guaranteed to be safe and are faster than packet filters created using Berkeley Packet Filters, Software Fault Isolation, or safe languages such as Modula-3. In ...","1997-10-28"
206,50504,"Approximate Reachability with BDDs using Overlapping Projections","Alan J. Hu; David L. Dill; Mark A. Horowitz; Shankar G. Govindaraju;","Approximate reachability techniques trade off accuracy with the capacity to deal with bigger designs. Cho et al [3] proposed approximate FSM traversal algorithms over a partition of the set of state bits. In this paper we generalize it by allowing projections onto a collection of nondisjoint subsets of the state variables. We establish the advantage of having overlapping projections and present a new multiple constrain function for BDDs, to compute efficiently the approximate image during symbolic forward propagation using overlapping projections. We demonstrate the effectiveness of this new algorithm by applying it to several control modules from the I/O unit in the Stanford FLASH Multiprocessor. We also present our results on the larger ISCAS 89 benchmarks. 1 Introduction Binary Decision Diagrams (BDDs) [1] have enabled formal verification to tackle larger hardware designs than before. However for many large design examples, even the most sophisticated BDD-based verification metho...","1998-09-18"
207,50568,"High Performance BDD Package By Exploiting Memory Hierarchy","Alberto Sangiovanni-vincentelli; Jagesh V. Sanghavi; Rajeev K. Ranjan; Robert K. Brayton;","The success of binary decision diagram (BDD) based algorithms for verification depend on the availability of a high performance package to manipulate very large BDDs. State-of-the-art BDD packages, based on the conventional depth-first technique, limit the size of the BDDs due to a disorderly memory access patterns that results in unacceptably high elapsed time when the BDD size exceeds the main memory capacity. We present a high performance BDD package that enables manipulation of very large BDDs by using an iterative breadth-first technique directed towards localizing the memory accesses to exploit the memory system hierarchy. The new memory-oriented performance features of this package are 1) an architecture independent customized memory management scheme, 2) the ability to issue multiple independent BDD operations (superscalarity) , and 3) the ability to perform multiple BDD operations even when the operands of some BDD operations are the result of some other operations yet to be com...","1996-04-30"
208,50929,"Formally Specified Monitoring of Temporal Properties","Hanene Ben-abdallah; Insup Lee; Mahesh Viswanathan; Moonjoo Kim; Oleg Sokolsky; Sampath Kannan;","We describe the Monitoring and Checking (MaC) framework which provides assurance on the correctness of an execution of a real-time system at runtime. Monitoring is performed based on a formal specification of system requirements. MaC bridges the gap between formal specification, which analyzes designs rather than implementations, and testing, which validates implementations but lacks formality. An important aspect of the framework is a clear separation between implementation-dependent description of monitored objects and high-level requirements specification. Another salient feature is automatic instrumentation of executable code. The paper presents an overview of the framework, languages to express monitoring scripts and requirements, and a prototype implementation of MaC targeted at systems implemented in Java. 1 Introduction Real-time systems often arise in the area of embedded and safety-critical applications. Dependability of such systems is the utmost concern to their developers...","1999-04-07"
209,51180,"Verifying Out-of-Order Executions","A. Pnueli; W. Damm;","The paper presents an approach to the specification and verification of out-of-order execution in the design of micro-processors. Ultimately, the appropriate statement of correctness is that the out-of-order execution produces the same final state (and all relevant intermediate actions, such as writes to memory) as a purely sequential machine running the same program. 1 Introduction Modern processor architectures such as the PowerPC or the DEC Alpha employ aggressive implementation techniques to sustain peak-throughput of instructions. Multiple functional units inside the data-path allow for concurrent execution of multiple instructions and allow to hide latencies stemming from data-dependencies as well as varying pipeline delays. The design of controllers maintaining consistency to sequential program execution on the face of a mixture of out-of-order execution of instructions, speculative execution of instructions, interrupts, and load/store buffers is both challenging and error-pron...","1998-01-25"
210,51278,"Linear Machine Decision Trees","Carla E. Brodley; Paul E. Utgoff;","This article presents an algorithm for inducing multiclass decision trees with multivariate tests at internal decision nodes. Each test is constructed by training a linear machine and eliminating variables in a controlled manner. Empirical results demonstrate that the algorithm builds small accurate trees across a variety of tasks. 1 Introduction One of the fundamental research problems in machine learning is how to learn from examples. From a sequence or set of training examples, each labeled with its correct class name, a machine learns by forming or selecting a generalization of the training examples. This process, also known as supervised learning, is useful for real classification tasks, e.g. disease diagnosis, and for problem solving tasks in which control decisions depend on classification, e.g. rule applicability. The ability to generalize is fundamental to intelligence because it allows one to reason in accordance with predictions that are often correct. This article focuse...","1993-09-21"
211,51811,"Predicates and Predicate Transformers for Supervisory Control of Discrete Event Dynamical Systems","Ratnesh Kumar; Steven I. Marcus; Vijay Garg;","Most discrete event system models are based on defining the alphabet set or the set of events as a fundamental concept. In this paper, we take an alternative view of treating the state space as the fundamental concept. We approach the problem of controlling discrete event systems by using predicates and predicate transformers. Predicates have the advantage that they can concisely characterize an infinite state space. The notion of controllability of a predicate is defined, and the supervisory predicate control problem introduced in this paper is solved. A closed form expression for the weakest controllable predicate is obtained. The problem of controlling discrete event systems under incomplete state observation is also considered and observability of predicates is defined. Techniques for finding extremal solutions of boolean equations is used to derive minimally restrictive supervisors. 1 Introduction Many discrete event system models [24, 23, 25, 9, 11, 12] are based on defining t...","1998-07-06"
212,51827,"Timed Modal Specification - Theory and Tools","Jens Chr Godskesen; Kim G. Larsen;",". In this paper we present the theory of Timed Modal Specifications (TMS) together with its implementation, the tool Epsilon. TMS and Epsilon are timed extensions of respectively Modal Specifications [7, 9] and the Tav system [6, 4]. Also, the theory of TMS is an extension of real--timed process calculi with the specific aim of allowing loose or partial specifications. This allows us to define a notion of refinement, generalizing in a natural way the classical notion of bisimulation. 1 Introduction In this paper we present the theory of Timed Modal Specifications (TMS) together with its implementation, Epsilon. TMS and Epsilon are timed extensions of respectively Modal Specifications [7, 5, 9] and the Tav system [6, 4]. During the last few years various process calculi have been extended to include real--time in order to handle quantitative aspects of real--time systems. We mention the calculi defined in [14] and the ones defined in [12] and [3]. Common to these real--time calcul...","1996-01-09"
213,51999,"On Randomization in Sequential and Distributed Algorithms","Rajiv Gupta; Scott A. Smolka; Shaji Bhaskar;","Probabilistic, or randomized, algorithms are fast becoming as commonplace as conventional deterministic algorithms. This survey presents five techniques that have been widely used in the design of randomized algorithms. These techniques are illustrated using 12 randomized algorithms --- both sequential and distributed --- that span a wide range of applications, including: primality testing (a classical problem in number theory), universal hashing (choosing the hash function dynamically and at random) , interactive probabilistic proof systems (a new method of program testing), dining philosophers (a classical problem in distributed computing), and Byzantine agreement (reaching agreement in the presence of malicious processors). Included with each algorithm is a discussion of its correctness and its computational complexity. Several related topics of interest are also addressed, including the theory of probabilistic automata, probabilistic analysis of conventional algorithms, determinis...","1993-09-09"
214,52413,"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm","Peter D. Turney;","This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification --- EG2, CS-ID3, and IDX --- and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five realworld medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set look...","1995-07-27"
215,52476,"Dynascope: A Tool for Program Directing","Rok Sosic;","This paper introduces program directing, a new way of program interaction. Directing enables one program, the director, to monitor and to control another program, the executor. One important application of program directing is human interaction with complex computer simulations. The Dynascope programming environment is designed specifically to support directing in traditional, compiled languages. It provides a framework and building blocks for easy construction of sophisticated directors. Directors are regular programs that perform the directing of executors through Dynascope primitives. Dynascope is built around the concept of the execution stream which provides a complete description of the executor's computational behavior. The source code of executors requires no changes in order to be subjected to directing. This paper gives an overview of the Dynascope system. Sample applications are presented: debugging register allocation, animation of procedure calls, and a complex artificial ...","1996-10-02"
216,52498,"A Decision Algorithm for Full Propositional Temporal Logic","A. Pnueli; H. Mcguire; Y. Kesten; Z. Manna;","The paper presents an efficient algorithm for checking the satisfiability of a propositional linear time temporal logic formula, which may have past as well as future operators. This algorithm can be used to check validity of such formulas over all models as well as over computations of a finite-state program (model checking). Unlike previous theoretical presentations of a decision method for checking satisfiability or validity, whose first step is to construct the full set of all possible atoms of a tableau (satisfaction graph) and immediately pay the worst case exponential complexity price, the algorithm presented here builds the tableau incrementally. This means that the algorithm constructs only those atoms that are reachable from a possible initial atom, satisfying the formula to be checked. While incremental tableau construction for the future fragment of linear time temporal logic can be done in a single pass, the presence of past operators requires multiple passes that succes...","1999-02-05"
217,52543,"A Nitpick Analysis of Mobile IPv6","Daniel Jackson; Jeannette Wing; Yuchung Ng;","A lightweight formal method [JW96] enables partial specification and automatic analysis by sacrificing breadth of coverage and expressive power. By design, NP is a specification language that is a subset of Z and Nitpick is a tool that quickly and automatically checks properties of finite models of systems specified in NP. We used NP to state two critical acyclicity properties of Mobile IPv6, a new inter networking protocol that allows mobile hosts to communicate with each other. In our Nitpick analysis of Mobile IPv6 we discovered a flaw in a 1996 version of the design: one of the acyclicity properties does not hold. It takes only two hosts to exhibit this flaw. This paper gives self-contained overviews of Mobile IPv6 and of NP and Nitpick to understand the details of our specification and analysis. 1. Introduction A mobile internet working protocol (IP) is responsible for routing messages sent from one host to another where hosts may move around to different points in the network. On...","1998-08-25"
218,52561,"Analysis of Event Synchronization in A Parallel Programming Tool","David Callahan; Jaspal Subhlok; Ken Kennedy;","Understanding synchronization is important for a parallel programming tool that uses dependence analysis as the basis for advising programmers on the correctness of parallel constructs. This paper discusses static analysis methods that can be applied to parallel programs with event variable synchronization. The objective is to be able to predict potential data races in a parallel program. The focus is on how dependences and synchronization statements inside loops can be used to analyze complete programs with parallel loop and parallel case style parallelism. 1 Introduction Parallel programming is an intellectually demanding task. One of the most difficult challenges in the development of parallel programs for asynchronous shared memory systems is avoiding errors caused by inadvertent data sharing, often referred to as data races, which can lead to unpredictable results. These races correspond to data dependences [AK87, Wol82], where a data dependence links a pair of accesses to the sam...","1996-01-22"
219,52866,"Correlated Action Effects in Decision Theoretic Regression","Craig Boutilier;","Much recent research in decision theoretic planning has adopted Markov decision processes (MDPs) as the model of choice, and has attempted to make their solution more tractable by exploiting problem structure. One particular algorithm, structured policy construction achieves this by means of a decision theoretic analog of goal regression, using action descriptions based on Bayesian networks with tree-structured conditional probability tables. The algorithm as presented is not able to deal with actions with correlated effects. We describe a new decision theoretic regression operator that corrects this weakness. While conceptually straightforward, this extension requires a somewhat more complicated technical approach. 1 Introduction While Markov decision processes (MDPs) have proven to be useful as conceptual and computational models for decision theoretic planning (DTP), there has been considerable effort devoted within the AI community to enhancing the computational power of these m...","1997-05-30"
220,53287,"Tractable Flow Analysis for Anomaly Detection in Distributed Programs","J. Kramer; S. C. Cheung;",". Each process in a distributed program or design can be modelled as a process flow graph, where nodes represent program statements and directed edges represent control flows. This paper describes a flow analysis method to detect unreachable statements by examining the control flows and communication patterns in a collection of process flow graphs. The method can analyse programs with loops, non-deterministic structures and synchronous communication using an algorithm with a quadratic complexity in terms of program size. The method follows an approach described by Reif and Smolka [9] but delivers a more accurate result in assessing the reachability of statements. The higher accuracy is achieved using three techniques: statement dependency, history sets and statement re-reachability. The method is illustrated by a pump control application for a mining environment. A prototype has been implemented and its performance is presented. 1. Introduction There is a need for automatic analysis ...","1993-07-20"
221,53355,"A BDD-based Frontend for Retargetable Compilers","Peter Marwedel; Rainer Leupers;","this paper we present a unified frontend for retargetable compilers that performs analysis of the target processor model. Our approach bridges the gap between structural and behavioral processor models for retargetable compilation. This is achieved by means of instruction set extraction. The extraction technique is based on a BDD data structure which significantly improves control signal analysis in the target processor compared to previous approaches.","1998-06-17"
222,53428,"Solving Multiclass Learning Problems via Error-Correcting Output Codes","Ghulum Bakiri; Thomas G. Dietterich;","Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k ? 2 values (i.e., k ""classes""). The definition is acquired by studying collections of training examples of the form hx i ; f(x i )i. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to...","1995-07-27"
223,53595,"Reducing the Number of Clock Variables of Timed Automata","C. Daws; S. Yovine;","We propose a method for reducing the number of clocks of a timed automaton by combining two algorithms. The first one consists in detecting active clocks, that is, those clocks whose values are relevant for the evolution of the system. The second one detects sets of clocks that are always equal. We implemented the algorithms and applied them to several case studies. These experimental results show that an appropriate encoding of the state space, based on the output of the algorithms, leads to a considerable reduction of the memory space allowing a more efficient verification. 1 Introduction Timed automata [3, 13], are automata extended with a finite set of real-valued clocks that proceed at a uniform rate and constrain the times at which transitions occur. Since the time component makes the underlying transition system to be infinite, verification algorithms depend on the construction of a finite partition of the state space. As shown in [2, 3] the complexity of the verification probl...","1996-09-09"
224,53632,"A Theory of Timed Automata","David L. Dill; Rajeev Alur;",". We propose timed (finite) automata to model the behavior of realtime systems over time. Our definition provides a simple, and yet powerful, way to annotate state-transition graphs with timing constraints using finitely many realvalued clocks . A timed automaton accepts timed words --- infinite sequences in which a real-valued time of occurrence is associated with each symbol. We study timed automata from the perspective of formal language theory: we consider closure properties, decision problems, and subclasses. We consider both nondeterministic and deterministic transition structures, and both Buchi and Muller acceptance conditions. We show that nondeterministic timed automata are closed under union and intersection, but not under complementation, whereas deterministic timed Muller automata are closed under all Boolean operations. The main construction of the paper is an (PSPACE) algorithm for checking the emptiness of the language of a (nondeterministic) timed automaton. We also p...","1995-07-28"
225,53753,"A Simple Theorem Prover Based on Symbolic Trajectory Evaluation and OBDDs","Carl-johan H. Seger; Scott Hazelhurst;","Formal hardware verification based on symbolic trajectory evaluation shows considerable promise in verifying medium to large scale VLSI designs with a high degree of automation. However, in order to verify today's designs, a method for composing partial verification results is needed. One way of accomplishing this is to use a general purpose theorem prover to combine the verification results obtained by other tools. However, a specialised purpose theorem prover is more attractive since it can more easily exploit symbolic trajectory evaluation (and may be easier to use). Consequently we explore the possibility of developing a much simpler, but more tailor made, theorem prover designed specifically for combining verification results based on trajectory evaluation. In the paper we discuss the underlying inference rules of the prover as well as more practical issues regarding the user interface. We finally conclude with a couple of examples in which we are able to verify designs that could ...","1994-02-24"
226,53910,"A BDD-based Model Checker for the PEP Tool","Guido Wimmel;","PEP (Programming Environment based on Petri Nets) is a tool developed at the University of Hildesheim. It can be used for editing, simulating and verifying Petri nets, and for creating Petri nets from a program in an imperative programming language. For the verification task, model checkers are used to decide whether a given logical formula is true or false for a particular Petri net. A fairly new method in implementing model checkers, symbolic model checking, involves binary decision diagrams (BDDs), a data structure for representing considerably large state spaces. In the individual project described in this dissertation, a BDD-based model checker for the PEP tool was developed that can verify safe Petri nets. The model checker makes use of the SMV system, developed at the Carnegie Mellon University. In addition, a range of different modelling possibilities, model checking options and optimisation techniques is discussed and evaluated using examples like the dining philosophers probl...","1997-07-21"
227,54350,"OSCAR: Optimum Simultaneous Scheduling, Allocation and Resource Binding Based on Integer Programming","Birger L; Lehrstuhl Informatik Xii; Peter Marwedel; Rainer Domer;",": In this report we describe an IP-model based high-level synthesis system. In contrast to other approaches, the presented IP-model allows solving all three subtasks of high-level synthesis (scheduling, allocation and binding) simultaneously. As a result, designs which are optimal with respect to the cost function are generated. The model is able to exploit large component libraries with multi-functional units and complex components such as multiplier-accumulators. Additionally, the model is capable of handling mixed speeds and chaining in its general form. Applying algebraic transformations helps to exploit underlying component libraries more efficiently than other HLSsystems 1 . 1 This work has been partially supported by ESPRIT project BRA 6855 (LINK). Contents 1 Introduction 5 2 Related work 7 3 Design flow in the OSCAR system 9 4 IP-model based high-level synthesis 13 4.1 Integrated Scheduling, Allocation and Binding : : : : : : : : : : : : : : : : : : : : : : 13 4.1.1 Bind...","1997-04-15"
228,54455,"Retrieving And Integrating Data From Multiple Information Sources","Chin Y. Chee; Craig A. Knoblock; Yigal Arens;","With the current explosion of data, retrieving and integrating information from various sources is a critical problem. Work in multidatabase systems has begun to address this problem, but it has primarily focused on methods for communicating between databases and requires significant effort for each new database added to the system. This paper describes a more general approach that exploits a semantic model of a problem domain to integrate the information from various information sources. The information sources handled include both databases and knowledge bases, and other information sources (e.g., programs) could potentially be incorporated into the system. This paper describes how both the domain and the information sources are modeled, shows how a query at the domain level is mapped into a set of queries to individual information sources, and presents algorithms for automatically improving the efficiency of queries using knowledge about both the domain and the information sources. ...","1995-06-17"
229,54955,"Logic Programming in the LF Logical Framework","Frank Pfenning;","this paper we describe Elf, a meta-language intended for environments dealing with deductive systems represented in LF. While this paper is intended to include a full description of the Elf core language, we only state, but do not prove here the most important theorems regarding the basic building blocks of Elf. These proofs are left to a future paper. A preliminary account of Elf can be found in [26]. The range of applications of Elf includes theorem proving and proof transformation in various logics, definition and execution of structured operational and natural semantics for programming languages, type checking and type inference, etc. The basic idea behind Elf is to unify logic definition (in the style of LF) with logic programming (in the style of Prolog, see [22, 24]). It achieves this unification by giving types an operational interpretation, much the same way that Prolog gives certain formulas (Horn-clauses) an operational interpretation. An alternative approach to logic programming in LF has been developed independently by Pym [28]. Here are some of the salient characteristics of our unified approach to logic definition and metaprogramming. First of all, the Elf search process automatically constructs terms that can represent object-logic proofs, and thus a program need not construct them explicitly. This is in contrast to logic programming languages where executing a logic program corresponds to theorem proving in a meta-logic, but a meta-proof is never constructed or used and it is solely the programmer's responsibility to construct object-logic proofs where they are needed. Secondly, the partial correctness of many meta-programs with respect to a given logic can be expressed and proved by Elf itself (see the example in Section 5). This creates the possibilit...","1995-05-05"
230,54985,"Efficient Run-Time Monitoring of Timing Constraints","Aloysius K. Mok; Guangtian Liu;","A real-time system operates under timing constraints which it may be unable to meet under some circumstances. The criticality of a timing constraint determines how a system is to react when a timing failure happens. For critical timing constraints, a timing failure should be detected as soon as possible. However, early detection of timing failures requires more resource usage which may be deemed excessive. While work in real-time system monitoring has progressed in recent years, the issue of tradeoff between detection latency and resource overhead has not been adequately considered. This paper presents an approach for monitoring timing constraints in real-time systems which is based on a simple and expressive specification method for defining the timing constraints to be monitored. Efficient algorithms are developed to catch violations of timing constraints at the earliest possible time. These algorithms have been implemented in a tool called JRTM (Java Run-time Timing-constraint Monit...","1997-05-03"
231,55671,"Fast Algorithms for Mining Association Rules","Rakesh Agrawal; Ramakrishnan Srikant;","We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Experiments with synthetic as well as real-life data show that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database. 1 Introduction Database mining is motivated by the decision support problem faced by most large retail organizations [S 93]. Progress in bar-code technology has made it possible for retail organiz...","1996-02-21"
232,55781,"Verification of Real-Time Systems Using PVS","N. Shankar;","We present an approach to the verification of the real-time behavior of concurrent programs and describe its mechanization using the PVS proof checker. Our approach to real-time behavior extends previous verification techniques for concurrent programs by proposing a simple model for real-time computation and introducing a new operator for reasoning about absolute time. This model is formalized and mechanized within the higher-order logic of PVS. The interactive proof checker of PVS is used to develop the proofs of two illustrative examples: Fischer's real-time mutual exclusion protocol and a railroad crossing controller. This work was supported by National Aeronautics and Space Administration Langley Research Center and the US Naval Research Laboratory under contract NAS1-18969 and by the US Naval Research Laboratory contract N00015-92-C-2177. Connie Heitmeyer (NRL) suggested the railroad crossing example. Sam Owre (SRI) assisted with the use of PVS. The helpful comments of John Rush...","1994-11-05"
233,56673,"Symmetry and Model Checking","A. Prasad Sistla; E. Allen Emerson; Hermann Weyl;","We show how to exploit symmetry in model checking for concurrent systems containing many identical or isomorphic components. We focus in particular on those composed of many isomorphic processes. In many cases we are able to obtain significant, even exponential, savings in the complexity of model checking. 1 Introduction In this paper, we show how to exploit symmetry in model checking. We focus on systems composed of many identical (isomorphic) processes. The global state transition graph M of such a system exhibits a great deal of symmetry, characterized by the group of graph automorphisms of M. The basic idea underlying our method is to reduce model checking over the original structure M, to model checking over a smaller quotient structure M, where symmetric states are identified. In the following paragraphs, we give a more detailed but still informal account of a ""group-theoretic"" approach to exploiting symmetry. More precisely, the symmetry of M is reflected in the group, Aut M...","1997-02-20"
234,56745,"Verifying Continuous Time Markov Chains","Adnan Aziz; Kumud Sanwal; Robert Brayton; Vigyan Singhal;",". We present a logical formalism for expressing properties of continuous time Markov chains. The semantics for such properties arise as a natural extension of previous work on discrete time Markov chains to continuous time. The major result is that the verification problem is decidable; this is shown using results in algebraic and transcendental number theory. Introduction Recent work on formal verification has addressed systems with stochastic dynamics. Certain models for discrete time Markov chains have been investigated in [6, 3]. However, a large class of stochastic systems operate in continuous time. In a generalized decision and control framework, continuous time Markov chains form a useful extension [9]. In this paper we propose a logic for specifying properties of such systems, and describe a decision procedure for the model checking problem. Our result differs from past work in this area [2] in that quantitative bounds on the probability of events can be expressed in the logi...","1998-09-11"
235,56874,"Modeling and Verification of a Real Life Protocol Using Symbolic Model Checking","A. P. Sistla; Vivek G. Naik;","this paper, we show how symbolic model checking has been used to verify a real life protocol. Specifically, we have used SMV tool to model and verify IEEE 802.3 Etherenet CSMA/CD protocol with minimal abstraction. The Ethernet CSMA/CD protocol is a protocol that allows a set of computer systems connected over a local area network to communicate with each other. The major steps involved in using the SMV system for verification of the protocol were to correctly identify the processes within the protocol, to model them in the SMV toolkit, and to specify and verify the required properties of the protocol. Some design issues while modeling such a protocol are also dealt with in the research. We have verified the protocol under the asynchronous and synchronous models. The major problems encountered in using the SMV system were in modeling of the following aspects associated with the protocol: the channel, collision detection and carrier sensing, delay modeling (delay is used in successive attempts after a collision using the exponential backoff approach) and synchronization of transmitters and receivers. We first modeled the protocol at much detail and checked the properties. Under these two models, we used progressive abstraction to reduce the number of variables in each transmitter and receiver, and thus reduce the time taken for model checking. We have verified many properties for different stations, for various values for different values of maximum number attempts and frame sizes. This paper describes the appraoches employed in the verification purpose. The paper is organized as follows. Section 2 briefly describes the SMV system and","1997-02-20"
236,57841,"Constructing Nominal X-of-N Attributes","Zijian Zheng;","Most constructive induction researchers focus only on new boolean attributes. This paper reports a new constructive induction algorithm, called XofN, that constructs new nominal attributes in the form of X-of-N representations. An X-of-N is a set containing one or more attribute-value pairs. For a given instance, its value corresponds to the number of its attribute-value pairs that are true. The promising preliminary experimental results, on both artificial and real-world domains, show that constructing new nominal attributes in the form of X-of-N representations can significantly improve the performance of selective induction in terms of both higher prediction accuracy and lower theory complexity. 1 Introduction A well-known elementary limitation of selective induction algorithms is that when task-supplied attributes are not adequate for describing hypotheses, their performance in terms of prediction accuracy and/or theory complexity is poor. To overcome this limitation, constructiv...","1996-03-21"
237,58002,"A General Approach to Partial Order Reductions in Symbolic Verification (Extended Abstract)","null",") Parosh Aziz Abdulla Bengt Jonsson y Mats Kindahl z Doron Peled x Abstract The purpose of partial-order verification techniques is to avoid exploring several interleavings of independent transitions. The purpose of symbolic verification techniques is to perform basic manipulations on sets of states rather than on individual states. We present a general method for applying partial order reductions to improve symbolic verification. The method is equally applicable to the verification of finite-state and infinite-state systems. It considers methods that check safety properties, either by forward reachability analysis or by backward reachability analysis. We base the method on the concept of commutativity (in one direction) between predicate transformers. Since the commutativity relation is not necessarily symmetric, this generalizes those existing approaches to partial order verification which are based on a symmetric dependency relation. We show how our method can be applied to ...","1998-04-20"
238,58022,"Property Preserving Abstractions for the Verification of Concurrent Systems","C. Loiseaux; S. Graf;","ions for the Verification of Concurrent Systems * C. LOISEAUX loiseaux@imag.fr S. GRAF graf@imag.fr J. SIFAKIS sifakis@imag.fr A. BOUAJJANI bouajjan@imag.fr S. BENSALEM bensalem@imag.fr VERIMAG* , Rue Lavoisier, 38330 Monbonnot Received October 1, 1992; Revised February 1, 1994 Editor: David Probst Abstract. We study property preserving transformations for reactive systems. The main idea is the use of simulations parameterizedby Galois connections (ff; fl), relating the lattices of properties of two systems. We propose and study a notion of preservation of properties expressed by formulas of a logic, by a function ff mapping sets of states of a system S into sets of states of a system S'. We give results on the preservation of properties expressed in sublanguages of the branching time ¯-calculus when two systems S and S' are related via hff; fli-simulations. They can be used to verify a property for a system by verifying the same property on a simpler system which is an abstraction ...","1998-09-24"
239,58346,"Calculating Criticalities","A. Bundy; F. Giunchiglia; R. Sebastiani; T. Walsh;","We present a novel method for building Abstrips style abstraction hierarchies in planning. The aim of this method is to minimize search by limiting backtracking both between abstraction levels and within an abstraction level. Previous approaches for building Abstrips style abstractions have determined the criticality of operator preconditions by reasoning about plans directly. Here, we adopt a simpler and faster approach where we use numerical simulation of the planning process. We develop a simple but powerful theory to demonstrate the theoretical advantages of our approach. We use this theory to identify some simple properties lacking in previous approaches but possessed by our method. We demonstrate the empirical advantages of our approach by a set of four benchmark experiments using the AbTweak system. We compare the quality of the abstraction hierarchies generated with those built by the Alpine and Highpoint algorithms. Authors are listed in alphabetical order. The first author ...","1998-09-05"
240,58436,"Implementing Functional Languages with Fast Equality, Sets and Maps: an Exercise in Hash Consing","Jean Goubault;","We investigate hash consing, a memory allocation strategy for functional languages. Though the idea is not new, its systematic use as a foundation for the run-time system of a language is new. We call this systematic approach maximal sharing. This strategy is shown to be implementable in practice with small speed and space penalties, while offering great opportunities to save space and execution time in big projects. Besides, it paves the way towards more efficient implementations of very desirable data structures like sets and maps (set-theoretic functions of finite domain) [23], opening the door to a whole slew of set- and map-based functional languages like POPS-Lisp [18] and HimML [20], a variant of Standard ML written by the author. The average-case complexities of operations on sets and maps are investigated, and shown to be quite good indeed. Computation sharing and incremental computation are briefly considered in this framework. Garbage collection techniques are reviewed to d...","1997-04-17"
241,58664,"Handling preferences in Constraint Logic Programming with Relational Optimization","Julian Fowler;",". In many Constraint Logic Programming (CLP) applications one needs to express not only strict requirements but also preferences. Constraint hierarchies are one way of describing preferred criteria in the statement of a problem. In [18] CLP was extended to integrate constraint hierarchies resulting in Hierarchical Constraint Logic Programming (HCLP). We propose an alternative approach for describing preferred criteria in CLP as a problem of relational optimization (RO). In this approach the programmer defines a preference relation which indicates when a solution is better than another solution. We study several schemes based on pruning for optimizing an objective function, and we show how these schemes can be generalized to handle preference relations defined by CLP programs, while preserving a straightforward logical semantics. Further we show on some examples that the greater flexibility of the relational optimization scheme is not at the cost of efficiency. Keywords: preference, con...","1994-06-22"
242,58716,"Generating Data Flow Analysis Algorithms from Modal Specifications","Bernhard Steffen; Lehrstuhl Fur Informatik Ii;","The paper develops a framework that is based on the idea that modal logic provides an appropriate framework for the specification of data flow analysis (DFA) algorithms as soon as programs are represented as models of the logic. This can be exploited to construct a DFA-generator that generates efficient implementations of DFA-algorithms from modal specifications by partially evaluating a specific model checker with respect to the specifying modal formula. Moreover, the use of a modal logic as specification language for DFAalgorithms supports the compositional development of specifications and structured proofs of properties of DFA-algorithms. -- The framework is illustrated by means of a real life example: the problem of determining optimal computation points within flow graphs. Keywords: bit-vector algorithms, data flow analysis, data flow analysis generator, modal logic, model checking, mu-calculus, partial evaluation, transition systems. 1 Introduction Data flow analysis (DFA) is ...","1994-01-04"
243,59117,"Modeling a Dynamic and Uncertain World I: Symbolic and Probabilistic Reasoning about Change","Drew Mcdermott; Steve Hanks; Steve Hanks Drew Mcdermott;","Intelligent agency requires some ability to predict the future. An agent must ask itself what is presently its best course of action given what it now knows about what the world will be like when it intends to act. This paper presents a system that uses a probabilistic model to reason about the effects of an agent's proposed actions on a dynamic and uncertain world, computing the probability that relevant propositions will hold at a specified point in time. The model allows for incomplete information about the world, the occurrence of exogenous (unplanned) events, unreliable sensors, and the possibility of an imperfect causal theory. The system provides an application program with answers to questions of the form ""is the probability that ' will hold in the world at time t greater than ø ?"" It is unique among algorithms for probabilistic temporal reasoning in that it tries to limit its inference according to the proposition, time, and probability threshold provided by the application. T...","1993-07-12"
244,59128,"Quality of Service Routing for Supporting Multimedia Applications","Jon Crowcroft; Zheng Wang;","In recent years, several new architectures have been developed for supporting multimedia applications such as digital video and audio. However, quality of service routing is an important element that is still missing from these architectures. In this paper we consider a number of issues in QoS routing. We first examine the basic problem of QoS routing, namely, finding a path that satisfy multiple constraints, and its implications on routing metric selection, and then present three path computation algorithms for source routing and for hop-by-hop routing. 1. Introduction Multimedia applications such as digital video and audio often have stringent quality of service (QoS) requirements. For a network to deliver performance guarantees it has to make resource reservation and excise network control. In the past several years, there have been much discussion and research in the area of resource setup, admission control and packet scheduling, and many new architectures have been proposed [1-3...","1996-04-10"
245,59994,"Adaptation of Partitioning and High-Level Synthesis in Hardware/Software Co-Synthesis","J Org Henkel; Rolf Ernst; Thomas Benner; Ullrich Holtmann;","Previously, we had presented the system COSYMA for hardware/software co-synthesis of small embedded controllers [ErHeBe93]. Target system of COSYMA is a core processor with application specific co--processors. The system speedup for standard programs compared to a single 33MHz RISC processor solution with fast, single cycle access RAM was typically less than 2 due to restrictions in high-level co--processor synthesis, and incorrectly estimated back end tool performance, such as hardware synthesis, compiler optimization and communication optimization. Meanwhile, a high-level synthesis tool for highperformance co--processors in co-synthesis has been developed. This paper explains the requirements and the main features of the high-level synthesis system and its integration into COSYMA. The results show a speedup of 10 in most cases. Compared to the speedup, the co--processor size is very small. 1 Introduction -- the COSYMA system This paper presents a significant improvement of the hardw...","1997-06-18"
246,60275,"Static Detection Of Deadlocks In Polynomial Time","Graduate School---new Brunswick; P. Masticola;","OF THE DISSERTATION Static Detection of Deadlocks in Polynomial Time by Stephen P. Masticola, Ph.D. Dissertation Director: Dr. Barbara G. Ryder Parallel and distributed programming languages often include explicit synchronization primitives, such as rendezvous and semaphores. Such programs are subject to synchronization anomalies; the program behaves incorrectly because it has a faulty synchronization structure. A deadlock is an anomaly in which some subset of the active tasks of the program mutually wait on each other to advance; thus, the program cannot complete execution. In static anomaly detection, the source code of a program is automatically analyzed to determine if the program can ever exhibit a specific anomaly. Static anomaly detection has the unique advantage that it can certify programs to be free of the tested anomaly; dynamic testing cannot generally do this. Though exact static detection of deadlocks is NP-hard [Tay83a], many researchers have tried to detect deadlock by ...","1993-06-07"
247,60394,"Automatic Synthesis of Transport Triggered Processors","Henk Corporaal; Jan Hoogerbrugge;","A designer can chose from several options when mapping an application into a combination of hardware and software. Usage of an ASIP offers the advantage of a large design freedom, allowing optimal tuning of performance and costs. However there are two major problems related to the design of ASIPs: 1) the design trajectory is long, and 2) it is impossible to do a quantitative search of the whole design space. The alleviate these problems we propose a design trajectory based on a templated, transport triggered architecture. Using a restricted, but still very large, design space we are able to automate the design trajectory based on a quantitative analysis of many design points. This paper presents this design method and shows its results when the method is applied to two benchmarks. Keywords: CoSynthesis, Design space exploration, ASIP design, Transport triggering. 1 Introduction Designing ASICs based on templated application specific instruction set processors (ASIPs) is an attractive...","1995-11-06"
248,60654,"Structured Reachability Analysis for Markov Decision Processes","Christopher Geib; Craig Boutilier; Ronen I. Brafman;","Recent research in decision theoretic planning has focussedon making the solution of Markov decision processes (MDPs) more feasible. We develop a family of algorithms for structured reachability analysis of MDPs that are suitable when an initial state (or set of states) is known. Using compact, structured representations of MDPs (e.g., Bayesian networks), our methods, which vary in the tradeoff between complexity and accuracy, produce structured descriptions of (estimated) reachable states that can be used to eliminate variables or variable values from the problem description, reducing the size of the MDP and making it easier to solve. One contribution of our work is the extension of ideas from GRAPHPLAN to deal with the distributed nature of action representations typically embodied within Bayes nets and the problem of correlated action effects. We also demonstrate that our algorithm can be made more complete by using k-ary constraints instead of binary constraints. Another contributi...","1998-05-07"
249,61863,"Tableau-Based Model Checking in the Propositional Mu-Calculus","Rance Cleaveland;","This paper describes a procedure, based around the construction of tableau proofs, for determining whether finite-state systems enjoy properties formulated in the propositional mu-calculus. It presents a tableau-based proof system for the logic and proves it sound and complete, and it discusses techniques for the efficient construction of proofs that states enjoy properties expressed in the logic. The approach is the basis of an ongoing implementation of a model checker in the Concurrency Workbench, an automated tool for the analysis of concurrent systems. 1 Introduction One area of program verification that has proven amenable to automation involves the analysis of finite-state processes. While computer systems in general are not finite-state, many interesting ones, including a variety of communication protocols and hardware systems, are, and their finitary nature enables the development and implementation of decision procedures that test for various properties. Model checking has p...","1997-11-25"
250,61983,"Software Scheduling in the Co-Synthesis of Reactive Real-Time Systems","Gaetano Borriello; Pai Chou;","Existing software scheduling techniques limit the functions that can be implemented in software to those with a restricted class of timing constraints, in particular those with a coarse-grained, uniform, periodic behavior. In practice, however, many systems change their I/O behavior in response to the inputs from the environment. This paper considers one such class of systems, called reactive real-time systems, where timing requirements can include sequencing, rate, and response time constraints. We present a static, non-preemptive, fine-grained software scheduling algorithm to meet these constraints. This algorithm is suitable for control-dominated embedded systems with hard real-time constraints, and is part of the core of a hardware/software co-synthesis system. I Introduction An embedded system is a special purpose computer consisting of one or more controllers and peripheral devices. A reactive system is an embedded system that changes its I/O behavior in response to inputs from t...","1995-12-21"
251,62107,"A Stochastic Causality-Based Process Algebra","Diego Latella; Joost-pieter Katoen; Rom Langerak;","This paper discusses stochastic extensions of a simple process algebra in a causality based setting. Atomic actions are supposed to happen after a delay that is determined by a stochastic variable with a certain distribution. A simple stochastic type of event structures is discussed, restricting the distribution functions to be exponential. A corresponding operational semantics of this model is given and compared to existing (interleaved) approaches. Secondly, a stochastic variant of event structures is discussed where distributions are of a much more general nature, viz. of phase-type. This includes exponential, Erlang, Coxian and mixtures of exponential distributions. 1. INTRODUCTION","1995-08-31"
252,62259,"The Model Checker SPIN","Gerard J. Holzmann;","SPIN is an efficient verification system for models of distributed software systems. It has been used to detect design errors in applications ranging from high-level descriptions of distributed algorithms to detailed code for controlling telephone exchanges. This paper gives an overview of the design and structure of the verifier, reviews its theoretical foundation, and gives an overview of significant practical applications. Index Terms---Formal methods, program verification, design verification, model checking, distributed systems, concurrency. ------------------------------ F ------------------------------ 1 INTRODUCTION PIN is a generic verification system that supports the design and verification of asynchronous process systems [36], [38]. SPIN verification models are focused on proving the correctness of process interactions, and they attempt to abstract as much as possible from internal sequential computations. Process interactions can be specified in SPIN with rendezvous primi...","1999-04-12"
253,62412,"The Foundations of Esterel","Grard Berry;","Machine or CHAM [9] is a mathematical version of chemistry that now routinely serves as a basis for the semantics of interactive process calculi or languages [43]. Being unable to express timeliness, the chemical model is obviously inappropriate for reactive systems. In the Newtonian model, planets evolve in a deterministic and perfectly synchronous way. The Newtonian model will serve as a guideline for the deønition and semantics of our synchronous languages, where we shall similarly assume I J K Z X Y X = I and not J Y = J and K Z = X or Y Figure 1: A synchronous circuit that processes instantaneously exchange information in a deterministic way. For implementation, we shall use the more complex electrical vibration model, where information propagates with delay, where geometrical constraints may come in the picture, and where some (controllable) internal non-determinism may exist. In physics, there is a well-known tension between the accuracy and the adequacy of a mathematical mode...","1998-10-12"
254,62677,"Presenting Machine-Found Proofs","Armin Fiedler; Xiaorong Huang;",". This paper outlines an implemented system named PROVERB that transforms and abstracts machine-found proofs to natural deduction style proofs at an adequate level of abstraction and then verbalizes them in natural language. The abstracted proofs, originally employed only as an intermediate representation, also prove to be useful for proof planning and proving by analogy. 1 Introduction This paper outlines an implemented system named PROVERB that presents and verbalizes machine-found natural deduction proofs in natural language. Apart from its practical use, we hope PROVERB will also address some cognitive aspects of proof representation and proof presentation. Efforts have been made to transform proofs from machine-oriented formalisms into a more natural formalism [And80, Pfe87, Lin90]. As the target formalism, usually a variation of the natural deduction (ND) proof proposed by Gentzen [Gen35] is chosen. The resulting ND proofs are then used as inputs by natural language generators [...","1996-05-08"
255,62893,"(Inter-)Action Refinement: The Easy Way","Manfred Broy;","We outline and illustrate a formal concept for the specification and refinement of networks of interactive components. We describe systems by modular, functional specification techniques. We distinguish between black box and glass box views of interactive system components as well as refinements of their black box and glass box views. We identify and discuss several classes of refinements such as behaviour refinement, communication history refinement, interface interaction refinement, state space refinement, distribution refinement, and others. In particular, we demonstrate how these concepts of refinement and their verification are supported by functional specification techniques leading to a general formal refinement calculus. It can be used as the basis for a method for the development of distributed interactive systems. 1 This work was supported by the Sonderforschungsbereich 342 ""Werkzeuge und Methoden für die Nutzung paralleler Architekturen"" IAR-the easy way - 2 - 4/25/95 1....","1997-09-29"
256,63210,"Aspect-Oriented Programming","Chr Is; Cr Istina; Gregor Iczales; I Ntroduction; Ideira Lopes; Jeanmarc Loingtier; Lamp Ing;","this paper, we present an overview of our recent research on programming language expressivity. The goal of this work is to make it possible for programs to clearly capture all of the important aspects of a system's behavior, including not only its functionality, but also issues such as its failure handling strategy, its communication strategy, its coordination strategy, its memory reference locality, etc. Our current work is based on the belief that programming languages based on any SINGLE abstraction framework---procedures, constraints, whatever---are ultimately inadequate for many complex systems. The reason is that the different aspects of a system's behavior that must be programmed, each tend to have their own ""natural form"", so while one abstraction framework might do a good job of capturing one aspect, it will do a less good job capturing others. This conclusion has led us to develop a concept we call Aspect-Oriented Programming (AOP). In AOP, the different aspects of a system's behavior are each programmed in their most natural form, and then these separate programs are woven together to produce executable code. Our work on AOP is being carried out in the context of both general-purpose and domain specific languages, we believe that it has contributions to make to both areas. 1 . 1 CROSS - CUTTING The basic limitation of single abstraction framework languages is that one abstraction will not necessarily serve well for all the different issues that must be programmed in a specific system. A classic example is the notion of invariant relations among objects. While many standard object-oriented languages do a good job of clearly capturing the behavior of objects, they do a less good job of capturing structural and behavioral invariants, such as ""when this object ge...","1996-12-18"
257,63336,"Synthesis of the Hardware/Software Interface in Microcontroller-Based Systems","Gaetano Borriello; Pai Chou; Ross Ortega;","Microcontroller-based systems require the design of a hardware/software interface that enables software running on the microcontroller to control external devices. This interface consists of the sequential logic that physically connects the devices to the microcontroller and the software drivers that allow code to access the device functions. This paper presents a method for automatically synthesizing this hardware/software interface using a recursive algorithm. Practical examples are used to demonstrate the utility of the method and results indicate that the synthesized circuit and driver code are comparable to that generated by human designers. This new tool will be used by higher-level synthesis tools to evaluate partitionings of a system between hardware and software components. 1 Introduction Microcontrollers are microprocessors with integrated general-purpose interfacing logic to facilitate the control of peripheral devices. This logic is encapsulated in I/O ports (i.e., collec...","1997-11-12"
258,63435,"Systematic Nonlinear Planning","David Mcallester; David Rosenblitt;","This paper presents a simple, sound, complete, and systematic algorithm for domain independent STRIPS planning. Simplicity is achieved by starting with a ground procedure and then applying a general, and independently verifiable, lifting transformation. Previous planners have been designed directly as lifted procedures. Our ground procedure is a ground version of Tate's NONLIN procedure. In Tate's procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations. This allows Tate's procedure to avoid the use of Chapman's modal truth criterion. Systematicity is the property that the same plan, or partial plan, is never examined more than once. Systematicity is achieved through a simple modification of Tate's procedure. Introduction STRIPS planning was introduced in (Fikes and Nilsson 1971) as a model of the kind of planning problems that people appear to solve in common sense reasoning. STRIPS planning correspo...","1995-02-12"
259,63437,"Automatic Generation of Invariants and Intermediate Assertions","Anca Browne; Zohar Manna;","Verifying temporal specifications of reactive and concurrent systems commonly relies on generating auxiliary assertions and on strengthening given properties of the system. This can be achieved by two dual approaches: The bottom-up method performs an abstract forward propagation (computation) of the system, generating auxiliary assertions; the top-down method performs an abstract backward propagation to strengthen given properties. Exact application of these methods is complete but is usually infeasible for large-scale verification. Approximation techniques are often needed to complete the verification. We give an overview of known methods for generation of auxiliary invariants in the verification of invariance properties. We extend these methods, by formalizing and analyzing a general verification rule that uses assertion graphs to generate auxiliary assertions for the verification of general safety properties. 1 Introduction The deductive verification of temporal specifications of re...","1996-06-03"
260,63807,"A Lightweight Architecture for Program Execution Monitoring","Clinton Jeffery; Kevin Templer; Michael Brazell; Wenyi Zhou;","The Alamo monitor architecture reduces the difficulty of developing dynamic analysis tools, such as specialpurpose profilers, bug-detectors, and program visualizers. 1 Introduction Dynamic analysis tools are used in several phases of software development, including coding, testing, and maintenance [1]. Although conventional debuggers and profilers are well-suited for finding certain kinds of bugs and performance bottlenecks, they may be ineffective when problems arise for which they were not intended. Improvements in execution monitors have been slow to appear, primarily due to the high cost of developing such tools. This motivates the focus of our research: reducing the cost of writing monitors. A monitor framework for the Icon programming language presented one approach that reduces development costs for a broad class of execution monitors [2]. That framework provided monitor writers with solutions for several problems inherent in the execution monitoring realm, such as access to a...","1999-04-05"
261,63977,"Improved CLP Scheduling with Task Intervals","null","In this paper we present a new technique that can be used to improve performance of job scheduling with a constraint programming language. We show how, by focusing on some special sets of tasks, one can bring CLP in the same range of efficiency as traditional OR algorithms on a classical benchmark (MT10 [MT63]), thus making CLP both a flexible and an efficient technique for such combinatorial problems. We then present our programming methodology which we have successfully used on many problems, and draw conclusions on what features constraint programming languages should offer to allow its use. 1. Introduction Real-life scheduling problems are often the composition of various well-identified hard problems. In the previous years, we have worked on applications such as task-technician assignments [CK92] or staff timetable scheduling [CGL93] and developed a methodology for solving such problems with an extensible constraint logic programming language. In both cases we have applied the s...","1994-11-15"
262,64263,"Encoding Plans in Propositional Logic","Bart Selman; David Mcallester; Henry Kautz;","In recent work we showed that planning problems can be efficiently solved by general propositional satisfiability algorithms (Kautz and Selman 1996). A key issue in this approach is the development of practical reductions of planning to SAT. We introduce a series of different SAT encodings for STRIPS-style planning, which are sound and complete representations of the original STRIPS specification, and relate our encodings to the Graphplan system of Blum and Furst (1995). We analyze the size complexity of the various encodings, both in terms of number of variables and total length of the resulting formulas. This paper complements the empirical evaluation of several of the encodings reported in Kautz and Selman (1996). We also introduce a novel encoding based on the theory of causal planning, that exploits the notionof ""lifting"" from the theorem-proving community. This new encoding strictly dominates the others in terms of asymptotic complexity. Finally, we consider further reductions i...","1998-03-17"
263,64329,"Efficient Local Search with Conflict Minimization: A Case Study of the N-Queens Problem","Jun Gu; Rok Sosic; Senior Member; Student Member;","Backtracking search is frequently applied to solve a constraint-based search problem but it often suffers from exponential growth of computing time. We present an alternative to backtracking search: local search based on conflict minimization. We have applied this general search framework to study a benchmark constraint-based search problem, the n-queens problem. An efficient local search algorithm for the n-queens problem was implemented. This algorithm, running in linear time, does not backtrack at all. It is capable of finding a solution for extremely large size n-queens problems. For example, on a workstation computer, it can find a solution for 3,000,000 queens in less than 55 seconds. Keywords: conflict minimization, local search, n-queens problem, nonbacktracking search. 1 This research has been supported in part by the University of Utah research fellowships, in part by the Research Council of Slovenia, and in part by ACM/IEEE academic scholarship awards. 1 Introduction A ...","1996-10-02"
264,64392,"The NCSU Concurrency Workbench","Rance Cleaveland; Steve Sims;",". The NCSU Concurrency Workbench is a tool for verifying finite-state systems. A key feature is its flexibility; its modular design eases the task of adding new analyses and changing the language users employ for describing systems. This note gives an overview of the system 's features, including its capacity for generating diagnostic information for incorrect systems, and discusses some of its applications. 1 Introduction The NCSU Concurrency Workbench (NCSU-CWB) [1] supports the automatic verification of finite-state concurrent systems. The main goal of the system is to provide users with a tool that is flexible and easy to use and yet whose performance is competitive with that of existing special-purpose tools. In support of the former, and like its predecessor, the (Edinburgh) Concurrency Workbench [9, 15], the NCSU-CWB includes implementations of decision procedures for calculating a number of different behavioral equivalences and preorders between systems and for determining whe...","1996-05-15"
265,64835,"The Benefits of Relaxing Punctuality","Rajeev Alur; Thomas A. Henzinger;",". The most natural, compositional, way of modeling real-time systems uses a dense domain for time. The satisfiability of timing constraints that are capable of expressing punctuality in this model, however, is known to be undecidable. We introduce a temporal language that can constrain the time difference between events only with finite, yet arbitrary, precision and show the resulting logic to be EXPSPACE-complete. This result allows us to develop an algorithm for the verification of timing properties of real-time systems with a dense semantics. Keywords: temporal logic, verification, real time. 1 A preliminary version of this paper appeared in the Proceedings of the Tenth Annual ACM Symposium on Principles of Distributed Computing (PODC 1991), pp. 139--152, and an extended version appeared in the Journal of the ACM 43, 1996, pp. 116--146. 2 Supported in part by the Office of Naval Research Young Investigator award N00014-95-1-0520, by the National Science Foundation CAREER award C...","1997-06-17"
266,65301,"Using Causal Knowledge to Learn More Useful Decision Rules From Data","Louis Anthony Cox;","This paper presents results of an applied research effort focused on how to modify conventional recursive partitioning programs (e.g., CART, Knowledge Seeker, ID3) so that they will learn useful decision rules (prescriptions for action) from data. The desired output is no longer a probabilistic prediction of the value of a dependent variable, based on the observed values of independent variables. Instead, it is a prescription for what potentially costly actions to take (i.e., what values to assign to different controllable input variables), based on (perhaps costly) measurements of multiple independent variables, so as to bring several dependent variables simultaneously into a desired ""efficient"" (undominated) target set of joint values. The resulting methodology has been applied to several real problems in the telecommunications industry, including selecting actions to improve customer service (Cox and Bell, 1995) and identifying a strategy for improving employee performance and morale, discussed in the last part of this paper.","1996-01-06"
267,65602,"On Combining Formal and Informal Verification","Adnan Aziz; Jacob Abraham; Jian Shen; Jun Yuan;",". We propose algorithms which combine simulation with symbolic methods for the verification of invariants. The motivation is two-fold. First, there are designs which are too complex to be formally verified using symbolic methods; however by the use of symbolic techniques in conjunction with traditional simulation results in better ""coverage"" relative to the computational resources used. Additionally, even on designs which can be symbolically verified, the use of a hybrid methodology often detects the presence of bugs faster than either formal verification or simulation. 1 Introduction In this paper we will be concerned with the problem of design verification; specifically, the problem of invariant checking over gate-level designs. Traditionally, designs have been verified by extensive simulation. While offering the benefits of simplicity and scalability, simulation offers no guarantees of correctness; for large designs, the fraction of the design space which can be covered in this met...","1998-08-27"
268,65646,"Mediators in the Architecture of Future Information Systems","Gio Wiederhold;","The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information processing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users' workstations and data resources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software whic...","1994-10-11"
269,65841,"Fast OFDD based Minimization of Fixed Polarity Reed-Muller Expressions","Bernd Becker; Michael Theobald; Rolf Drechsler;","We present methods to minimize Fixed Polarity ReedMuller expressions (FPRMs), i.e. 2-level fixed polarity AND/EXOR canonical representations of Boolean functions, using Ordered Functional Decision Diagrams (OFDDs) . We investigate the close relation between both representations and use efficient algorithms on OFDDs for exact and heuristic minimization of FPRMs. In contrast to previously published methods our algorithm can also handle circuits with several outputs. Experimental results on large benchmarks are given to show the efficiency of our approach. 1 Introduction The high complexity of modern VLSI circuitry has shown an increasing demand for synthesis tools. In the last few years synthesis based on AND/EXOR realizations has gained more and more interest [12], because AND/EXOR realizations are very efficient for large classes of circuits, e.g. arithmetic circuits, error correcting circuits and circuits for tele-communication [16, 17]. For these classes EXOR-circuits, derived fro...","1995-04-03"
270,65852,"An Inductive Learning Approach to Prognostic Prediction","O. L. Mangasarian; W. H. Wolberg; W. Nick Street;","This paper introduces the Recurrence Surface Approximation, an inductive learning method based on linear programming that predicts recurrence times using censored training examples, that is, examples in which the available training output may be only a lower bound on the ""right answer."" This approach is augmented with a feature selection method that chooses an appropriate feature set within the context of the linear programming generalizer. Computational results in the field of breast cancer prognosis are shown. A straightforward translation of the prediction method to an artificial neural network model is also proposed. 1 INTRODUCTION Machine learning methods have been successfully applied to the analysis of many different complex problems in recent years, including many biomedical applications. One field which can benefit from this type of approach is the analysis of survival or lifetime data (Lee, 1992; Miller Jr., 1981), in which the objective can be broadly defined as predicting...","1998-08-04"
271,66285,"Timing Analysis of Asynchronous Circuits using Timed Automata","Amir Pnueli; Oded Maler;",". In this paper we present a method for modeling asynchronous digital circuits by timed automata. The constructed timed automata serve as ""mechanical"" and verifiable objects for asynchronous sequential machines in the same sense that (untimed) automata do for synchronous machines. These results, combined with recent results concerning the analysis and synthesis of timed automata provide for the systematic treatment of a large class of problems that could be treated by conventional simulation methods only in an ad-hoc fashion. The problems that can be solved due to the results presented in this paper include: the reachability analysis of a circuit with uncertainties in gate delays and input arrival times, inferring the necessary timing constraints on input signals that guarantee a proper functioning of a circuit and calculating the delay characteristics of the components required in order to meet some given behavioral specifications. Notwithstanding the existence of negative theoretica...","1995-08-22"
272,66289,"Conservation of Generalization: A Case Study","Cullen Schaffer;","We present results of a study of the applicability of information loss as an attribute selection criterion in decision tree induction. These results illustrate basic---though sometimes counter-intuitive---consequences of the conservation law for generalization performance and also suggest new avenues for research. 1 Introduction In many branches of mathematics, the construction and study of counterexamples is well accepted as a productive mode for research. The premise of the work reported here is that a similar tack may be useful in our study of inductive concept learning. An essential property of the problem of inductive generalization is that it admits no general solution. An algorithm that is good for learning certain sets of concepts must necessarily be bad for learning others. Moreover, no algorithm strictly dominates any other. If two learners differ in generalization performance, there must be problems for which each is superior to the other. As a consequence, every algorith...","1995-02-20"
273,66291,"On-the-Fly Symbolic Model Checking for Real-Time Systems","Ahmed Bouajjani; Sergio Yovine; Stavros Tripakis;","This paper presents an on-the-fly and symbolic algorithm for checking whether a timed automaton satisfies a formula of a timed temporal logic which is more expressive than TCTL. The algorithm is on-the-fly in the sense that the state-space is generated dynamically and only the minimal amount of information required by the verification procedure is stored in memory. The algorithm is symbolic in the sense that it manipulates sets of states, instead of states, which are represented as boolean combinations of linear inequalities of clocks. We show how a prototype implementation of our algorithm has improved the performances of the tool KRONOS for the verification of the FDDI protocol. 1 Introduction Formal methods provide a rigorous framework for modeling and analyzing the behavior of critical systems. Formal specifications of the system's behavior and its requirement, using suitable description languages, permit to formally prove that the former satisfies the latter. The development of e...","1998-11-10"
274,66776,"Probabilistic Linear-Time Model Checking: an Overview of The Automata-Theoretic Approach","Moshe Y. Vardi;",". We describe the automata-theoretic approach to the algorithmic verification of probabilistic finite-state systems with respect to linear-time properties. The basic idea underlying this approach is that for any linear temporal formula we can construct an automaton that accepts precisely the computations that satisfy the formula. This enables the reduction of probabilistic model checking to ergodic analysis of Markov chains. 1 Introduction Temporal logics, which are modal logics geared towards the description of the temporal ordering of events, have been adopted as a powerful tool for specifying and verifying concurrent systems [Pnu81]. One of the most significant developments in this area is the discovery of algorithmic methods for verifying temporal logic properties of finite-state systems [CE81,QS81,LP85,CES86]. This derives its significance both from the fact that many synchronization and communication protocols can be modeled as finite-state programs, as well as from the great e...","1999-03-10"
275,66991,"Geometric Embeddings for Faster and Better Multi-Way Netlist Partitioning","A. B. Kahng; C. J. Alpert;","We give new, effective algorithms for k-way circuit partitioning in the two regimes of k ø n and k = Theta(n), where n is the number of modules in the circuit. We show that partitioning an appropriately designed geometric embedding of the netlist, rather than a traditional graph representation, yields improved results as well as large speedups. We derive d- dimensional geometric embeddings of the netlist via (i) a new ""partitioning-specific"" net model for constructing the Laplacian of the netlist, and (ii) computation of d eigenvectors of the netlist Laplacian; we then apply (iii) fast top-down and bottom-up geometric clustering methods. 1 Preliminaries In top-down layout synthesis of complex VLSI systems, the goal of partitioning/clustering is to reveal the natural circuit structure, via a decomposition into k subcircuits which minimizes connectivity between subcircuits. A generic problem statement is as follows: k-Way Partitioning: Given a circuit netlist G = (V; E) with jV j...","1995-01-14"
276,67042,"Probabilistic Horn abduction and Bayesian networks","David Poole;","This paper presents a simple framework for Horn-clause abduction, with probabilities associated with hypotheses. The framework incorporates assumptions about the rule base and independence assumptions amongst hypotheses. It is shown how any probabilistic knowledge representable in a discrete Bayesian belief network can be represented in this framework. The main contribution is in finding a relationship between logical and probabilistic notions of evidential reasoning. This provides a useful representation language in its own right, providing a compromise between heuristic and epistemic adequacy. It also shows how Bayesian networks can be extended beyond a propositional language. This paper also shows how a language with only (unconditionally) independent hypotheses can represent any probabilistic knowledge, and argues that it is better to invent new hypotheses to explain dependence rather than having to worry about dependence in the language. Scholar, Canadian Institute for Advanced...","1998-08-27"
277,67138,"Design of Experiments in BDD Variable Ordering: Lessons Learned","Justin E. Harlow Iii;","Applying the Design of Experiments methodology to the evaluation of BDD variable ordering algorithms has yielded a number of conclusive results. The methodology relies on the recently introduced equivalence classes of functionally perturbed circuits that maintain logic invariance, or are within f1, 2, ...g-minterms of the original reference circuit function, also maintaining entropy-invariance. For some of the current variable ordering algorithms and tools, the negative results include: (1) statistically significant sensitivity to naming of variables, (2) confirmation that a number of variable ordering algorithms are statistically equivalent to a random variable order assignment, and (3) observation of a statistically anomalous variable ordering behavior of a wellknown benchmark circuit isomorphic class when analyzed under a single and multiple outputs. On the positive side, the methodology supports a statistically significant merit evaluation of any newly introduced variable ordering ...","1998-08-26"
278,67284,"Parallel Temporal Resolution","Clare Dixon; Michael Fisher; Rob Johnson;","Temporal reasoning is complex. Typically, the proof methods used for temporal logics are both slow and, due to the quantity of information required, consume a large amount of space. The introduction of parallelism provides the potential for significant speedups together with the increased memory size required to handle larger proofs. Thus, we see the effective utilisation of parallelism as being crucial in making temporal theorem-proving practical. In this paper we investigate and analyse opportunities for parallelism within a clausal resolution method for temporal logics. We show how parallelism might be introduced into the method in a variety of ways, providing a range of options for the parallel implementation of proof procedures for this important class of logics. 1 Introduction Temporal logics are non-classical logics that have been specifically developed for reasoning about properties that vary over time [10]. Varieties of temporal logic have been used in both computer science a...","1996-01-25"
279,67682,"A First-Order Branching Time Logic OF MULTI-AGENT SYSTEMS","Michael Fisher; Michael Wooldridge;","This paper presents a first-order branching time temporal logic that is suitable for describing and reasoning about a wide class of computational multi-agent systems. The logic is novel in that it supports reasoning about the beliefs, actions, goals, abilities and structure of groups of agents. A sound proof system for the logic is presented, and some short examples are given, showing how the logic might be used to specify desirable properties of multi-agent systems. 1 Introduction This paper presents a logic that is suitable for describing and reasoning about multi-agent systems (MAS). We take a MAS to be one composed of a number of computational entities built along the lines of classical AI research, which communicate through point-to-point message passing. Our strategy is to construct an abstract formal theory of MAS, and then build a logic corresponding to this theory [Wooldridge, 1992] . Our theory of MAS is in the spirit of [Konolige, 1986] , in that it is explicitly architectu...","1998-07-23"
280,67710,"Relaxation as a Platform of Cooperative Answering","Jack Minker; Parke Godfrey; Terry Gaasterland;","this paper, we provide a general notion and definition of query relaxation which is used to extend what is meant by ""an answer to a query."" We regard returning relaxed queries and their answers to the user to be a cooperative strategy. We present techniques for achieving relaxation, and we incorporate the relaxation techniques into a control strategy for query evaluation. 2. Background","1998-10-22"
281,67972,"The Complexity of Concept Languages","Werner Nutt;","A basic feature of Terminological Knowledge Representation Systems is to represent knowledge by means of taxonomies, here called terminologies, and to provide a specialized reasoning engine to do inferences on these structures. The taxonomy is built through a representation language called a concept language (or description logic), which is given a well-defined set-theoretic semantics. The efficiency of reasoning has often been advocated as a primary motivation for the use of such systems. The main contributions of the paper are: (1) a complexity analysis of concept satisfiability and subsumption for a wide class of concept languages; (2) the algorithms for these inferences that comply with the worst-case complexity of the reasoning task they perform. This is an extended and revised version of a paper presented at the 2nd Int. Conf. on Principles of Knowledge Representation and Reasoning, Cambridge, MA, 1991. 1 Introduction Among computer systems based on Artificial Intelligence ...","1999-03-05"
282,68452,"A Verified Typechecker","Robert Pollack;","this paper I describe partial attainment of the former goal: a verified type checking program for a class of PTS of practical interest, but one not yet efficient enough to actually execute in interesting cases. Techniques for type checking the Calculus of Constructions (CC) and its extensions were well known [Hue89], but there were some difficulties for other PTS. In [vBJMP94] we clarify the problem of type checking all PTS, giving satisfactory solutions for functional PTS and for semifull PTS, and some rather complicated theorems about the general case. While [vBJMP94] was written in informal language, many of its results were formalized and checked in LEGO. However, the ""satisfactory solution"" to type checking given in [vBJMP94] is expressed as a syntax-directed formal system (an inductively defined relation), not as an executable algorithm (a function), the idea being that the type checking function is obtained by just following the rules of the formal system, which, being syntax directed, leaves no choices to be made. A verified typechecker for CC is described in [DB93]. Working in the Boyer-Moore logic, the authors prove only the soundness of their typechecker, not its completeness (compare with our corollary 7), and even so, do not prove all the lemmas used in the soundness result. In contrast, our development is based on a completely formalized theory of PTS. [DB93] inspired me to formally verify a type checking function. Given the work in [vBJMP94], there are still two problems to be solved to have a verified type checking program for a class of PTS; (a) the algorithm for deciding PTS judgements from those of the syntax-directed system, and (b) termination of the process of applying the rules of the syntax-directed system. The current paper fills these gaps, alt...","1995-10-30"
283,68686,"Parametrisation of Coloured Petri Nets","Kjeld H. Mortensen;","In this paper we propose a conceptual framework for parametrisation of Coloured Petri Nets --- a first step towards the formulation and formalisation of Parametric Coloured Petri Nets. We identify and characterise three useful kinds of parametrisation, namely value, type, and net structure parameters. While the two former kinds are simple to design the latter kind is more complex, and in this context we describe how net structure parametrisation naturally induces concepts like modules and scope rules. The framework is applied to a non-trivial example from the domain of flexible manufacturing. Finally we discuss implementation issues. 1 Introduction When we wish to make a computer representation of a large family of entities or objects of interest from the world around us, we can either choose to represent all individual objects or try making more efficient representations. The perspective on a given problem has influence on the kind of efficiency needed. For instance, space efficiency...","1997-04-16"
284,68906,"BLACKBOX: A New Approach to the Application of Theorem Proving to Problem Solving","Bart Selman; Henry Kautz;","sentations (McCarthy and Hayes 1969) should go hand-in-hand with the study of practical reasoning algorithms, rather than being carried out as a separate activity.) ffl The use of powerful new general reasoning algorithms such as Walksat (Selman, Kautz, and Cohen 1994). Many researchers in different areas of computer science are creating faster SAT engines every year. Furthermore, these researchers have settled on common representations that allow algorithms and code to be freely shared and fine-tuned. As a result, at any point in time the best general SAT engines tend to be faster (in terms of raw inferences per second) than the best specialized planning engines. In principle, of course, these same improvements could be applied to the specialized engines; but by the time that is done, there will be a new crop of general systems. An approach that shares a number of features with with the SATPLAN strategy is the Graphlan system, developed independently by Blum and Furs","1998-05-22"
285,69148,"STeP: the Stanford Temporal Prover","Anca Browne; Anuchit Anuchitanukul; Edward Chang; Harish Devarajan; Henny Sipma; Luca De Alfaro; Zohar Manna;","We describe the Stanford Temporal Prover (STeP), a system being developed to support the computer-aided formal verification of concurrent and reactive systems based on temporal specifications. Unlike systems based on model-checking, STeP is not restricted to finite-state systems. It combines model checking and deductive methods to allow the verification of a broad class of systems, including programs with infinite data domains, N-process programs, and N-component circuit designs, for arbitrary N . In short, STeP has been designed with the objective of combining the expressiveness of deductive methods with the simplicity of model checking. The verification process is for the most part automatic. User interaction occurs mostly at the highest, most intuitive level, primarily through a graphical proof language of verification diagrams. Efficient simplification methods, decision procedures, and invariant generation techniques are then invoked automatically to prove resulting first-order ve...","1995-02-13"
286,70206,"A Methodology for Managing Hard Constraints in CLP Systems","Joxan Jaffar; Roland H. C. Yap; Spiro Michaylov;","In constraint logic programming (CLP) systems, the standard technique for dealing with hard constraints is to delay solving them until additional constraints reduce them to a simpler form. For example, the CLP(R) system delays the solving of nonlinear equations until they become linear, when certain variables become ground. In a naive implementation, the overhead of delaying and awakening constraints could render a CLP system impractical. In this paper, a framework is developed for the specification of wakeup degrees which indicate how far a hard constraint is from being awoken. This framework is then used to specify a runtime structure for the delaying and awakening of hard constraints. The primary implementation problem is the timely awakening of delayed constraints in the context of temporal backtracking, which requires changes to internal data structures be reversible. This problem is resolved efficiently in our structure. 1 Introduction The Constraint Logic Programming scheme [7...","1992-12-10"
287,70445,"Real-time Logics: Complexity and Expressiveness","Rajeev Alur; Thomas A. Henzinger;",". The theory of the natural numbers with linear order and monadic predicates underlies propositional linear temporal logic. To study temporal logics that are suitable for reasoning about real-time systems, we combine this classical theory of infinite state sequences with a theory of discrete time, via a monotonic function that maps every state to its time. The resulting theory of timed state sequences is shown to be decidable, albeit non elementary, and its expressive power is characterized by !-regular sets. Several more expressive variants are proved to be highly undecidable. This framework allows us to classify a wide variety of real-time logics according to their complexity and expressiveness. Indeed, it follows that most formalisms proposed in the literature cannot be decided. We are, however, able to identify two elementary real-time temporal logics as expressively complete fragments of the theory of timed state sequences, and we present tableau-based decision procedures for check...","1997-06-17"
288,70578,"Formal Requirements Analysis of an Avionics Control System","Bruno Dutertre; Victoria Stavridou;","We report on a formal requirements analysis experiment involving an avionics control system. We describe a method for specifying and verifying real-time systems with PVS. The experiment involves the formalization of the functional and safety requirements of the avionics system as well as its multilevel verification. First level verification demonstrates the consistency of the specifications whilst the second level shows that certain system safety properties are satisfied by the specification. We critically analyze methodological issues of large scale verification and propose some practical ways of structuring verification activities for optimising the benefits. Keywords---Formal specification, formal verification, safety critical systems, requirements analysis, avionics systems. I. Introduction T HIS paper reports on an experiment in the use of formal methods for producing and analyzing software requirements for a safety-related system. This work was conducted as part of the SafeFM ...","1997-05-30"
289,70752,"Flexible Metric Nearest Neighbor Classification","Jerome H. Friedman;","The K-nearest-neighbor decision rule assigns an object of unknown class to the plurality class among the K labeled ""training"" objects that are closest to it. Closeness is usually defined in terms of a metric distance on the Euclidean space with the input measurement variables as axes. The metric chosen to define this distance can strongly effect performance. An optimal choice depends on the problem at hand as characterized by the respective class distributions on the input measurement space, and within a given problem, on the location of the unknown object in that space. In this paper new types of K-nearest-neighbor procedures are described that estimate the local relevance of each input variable, or their linear combinations, for each individual point to be classified. This information is then used to separately customize the metric used to define distance from that object in finding its nearest neighbors. These procedures are a hybrid between regular K-nearest-neighbor methods and tree-structured recursive partitioning techniques popular in statistics and machine learning.","1994-11-25"
290,70908,"Analyzing Tabular and State-Transition Requirements Specifications in PVS","John Rushby; Natarajan Shankar; Sam Owre;","We describe PVS's capabilities for representing tabular specifications of the kind advocated by Parnas and others, and show how PVS's Type Correctness Conditions (TCCs) are used to ensure certain well-formedness properties. We then show how these and other capabilities of PVS can be used to represent the AND/OR tables of Leveson and the Decision Tables of Sherry, and we demonstrate how PVS's TCCs can expose and help isolate errors in the latter. We extend this approach to represent the mode transition tables of the Software Cost Reduction (SCR) method in an attractive manner. We show how PVS can check these tables for well-formedness, and how PVS's model checking capabilities can be used to verify invariants and reachability properties of SCR requirements specifications, and inclusion relations between the behaviors of different specifications. These examples demonstrate how several capabilities of the PVS language and verification system can be used in combination to provide customiz...","1996-05-26"
291,71090,"Retrofitting Decision Tree Classifiers Using Kernel Density Estimation","Er Gray; Padhraic Smyth; Usama M. Fayyad;","A novel method for combining decision trees and kernel density estimators is proposed. Standard classification trees, or class probability trees, provide piecewise constant estimates of class posterior probabilities. Kernel density estimators can provide smooth non-parametric estimates of class probabilities, but scale poorly as the dimensionality of the problem increases. This paper discusses a hybrid scheme which uses decision trees to find the relevant structure in high-dimensional classification problems and then uses local kernel density estimates to fit smooth probability estimates within this structure. Experimental results on simulated data indicate that the method provides substantial improvement over trees or density methods alone for certain classes of problems. The paper briefly discusses various extensions of the basic approach and the types of application for which the method is best suited. 1 INTRODUCTION There has been considerable interest in recent years in the use o...","1996-10-22"
292,72621,"Oracles for Checking Temporal Properties of Concurrent Systems","Laura K. Dillon; Qing Yu;","Verifying that test executions are correct is a crucial step in the testing process. Unfortunately, it can be a very arduous and error-prone step, especially when testing a concurrent system. System developers can therefore benefit from oracles automating the verification of test executions. This paper examines the use of Graphical Interval Logic (GIL) for specifying temporal properties of concurrent systems and describes a method for constructing oracles from GIL specifications. The visually intuitive representation of GIL specifications makes them easier to develop and to understand than specifications written in more traditional temporal logics. Additionally, when a test execution violates a GIL specification, the associated oracle provides information about a fault. This information can be displayed visually, together with the execution, to help the system developer see where in the execution a fault was detected and the nature of the fault. 1 Introduction Testing is a necessary a...","1996-03-26"
293,73134,"A Third Dimension to Rough Sets","Ron Kohavi;","Rough-sets relative reducts allow one to reduce the number of attributes in a supervised classification problem without sacrificing the consistency of the decision table. In many problems, the cardinality of the relative reducts is large, making the decision table unmanageably big. We propose reducing the size of reducts, and hence the decision table, by increasing the number of label values (values of the decision attribute). The reduction process is accomplished through a two-step process whereby an instance is mapped to an intermediate label using one subset of attributes, and then mapped to the final label using a disjoint subset of attributes. In some cases, the table representation size of the functions generated using this method will be much smaller than the representation of a single unified function. It is possible to repeat the splitting process, creating multiple rough-set approximations, each one containing one attribute less than the previous one, but at a possible increa...","1997-08-25"
294,73276,"Efficient Validity Checking for Processor Verification","David L. Dill; Jerry R. Burch; Robert B. Jones;","We describe an efficient validity checker for the quantifier-free logic of equality with uninterpreted functions. This logic is well suited for verifying microprocessor control circuitry since it allows the abstraction of data path values and operations. Our validity checker uses special data structures to speed up case splitting, and powerful heuristics to reduce the number of case splits needed. In addition, we present experimental results and show that this implementation has enabled the automatic verification of an actual high-level microprocessor description. 1 Introduction As microprocessor designs become more complex, the cost of validation becomes a larger fraction of the total design cost. Currently, validation consumes 25-30% of the design team and months of simulation time. Industry experts predict that there may soon be two or three validation engineers for every design engineer on major microprocessor design projects [Wil95]. Although today's theorem provers could be used,...","1996-07-09"
295,74362,"Algorithms for Synthesis of Hazard-Free Asynchronous Circuits","A. Sangiovanni-vincentelli; K. Keutzer; L. Lavagno;","A technique for the synthesis of asynchronous sequential circuits from a Signal Transition Graph (STG) specification is described. We give algorithms for synthesis and hazard removal, able to produce hazard-free circuits with the bounded wire-delay model, requiring the STG to be live, safe and to have the unique state coding property. A proof that, contrary to previous beliefs, STG persistency is not necessary for hazard-free implementation is given. 1 Introduction Asynchronous design is important in several applications of digital design. ""Real world"" interfaces and low power systems, where ""lazy evaluation"" style designs may extend the average life of a battery, are two examples. In addition, clock skew problems limit the performance and the flexibility of large scale synchronous systems. On the other hand asynchronous design is harder and more constrained than synchronous design, due to the hazard problem: asynchronous circuits are by definition sensitive to all signal changes, whe...","1995-01-26"
296,74382,"Complements to 'Pattern Recognition and Neural Networks'","B. D. Ripley;","Introduction Page 4: The book by Przytula & Prasanna (1993) discusses in detail the parallel implementation of neural networks. Page 16: Langley (1996) provides a book-length introduction to one viewpoint on machine learning. Langley & Simon (1995) and Bratko & Muggleton (1995) discuss applications of machine learning with claimed real-world benefits. Valentin et al. (1994) survey recent developments in face recognition. Arbib (1995) provides many short sketches of topics over a very wide range of neural networks, both artificial and biological. Chapter 2: Statistical Decision Theory Page 41: Lauritzen (1996, Chapter 6) gives an extensive treatment of conditional Gaussian distributions, and Edwards (1995) has a more practically-oriented account. Page 65--66: There has been a lot of interest in combining classifiers produced by the same method on different training sets. Bagging (Breiman, 1994, 1996","1996-08-23"
297,74385,"Embedding as a tool for Language Comparison","And Catuscia Palamidessi; Catuscia Palamidessi;","This paper addresses the problem of defining a formal tool to compare the expressive power of different concurrent constraint languages. We refine the notion of embedding by adding some ""reasonable"" conditions, suitable for concurrent frameworks. The new notion, called modular embedding, is used to define a preorder among these languages, representing different degrees of expressiveness. We show that this preorder is not trivial (i.e. it does not collapse into one equivalence class) by proving that Flat CP cannot be embedded into Flat GHC, and that Flat GHC cannot be embedded into a language without communication primitives in the guards, while the converses hold. 4 A; C; D; G; M;O;P;R; T : In calligraphic style. ss; ff ; dd: In slanted style. Sigma; Gamma; #; oe; ; /; ø; ff. S ; [; ""; ;; 2 j=; 6j=; ; 9 +; k; ~ +; ~ k; ! Gamma! W ; Gamma! ; ; Gamma! W ; Gamma! ; h; i; [[; ]]; d; e ffi; ?; ; 5 All reasonable programming languages are equivalent, since they are Turing...","1996-02-18"
298,74862,"Memory-Efficient Algorithms for the Verification of Temporal Properties","C. Courcoubetis; M. Vardi; M. Yannakakis; P. Wolper;","This paper addresses the problem of designing memory-efficient algorithms for the verification of temporal properties of finite-state programs. Both the programs and their desired temporal properties are modeled as automata on infinite words (Buchi automata). Verification is then reduced to checking the emptiness of the automaton resulting from the product of the program and the property. This problem is usually solved by computing the strongly connected components of the graph representing the product automaton. Here, we present algorithms which solve the emptiness problem without explicitly constructing the strongly connected components of the product graph. By allowing the algorithms to err with some probability, we A preliminary version of this paper appeared in: Computer-Aided Verification '90, E.M. Clarke and R.P. Kurshan Editors, DIMACS series in Discrete Mathematics and Theoretical Computer Science, vol. 3, American Mathematical Society and Association for Computing Machiner...","1998-09-27"
299,75234,"Commitment-Based Software Development","James Mcguire; Jon Schlossberg; Sherman Tyler; William Mark;","During the development of a system, software modules can be viewed in terms of their commitments: the constraints imposed by their own structure and behavior, and by their relationships with other modules (in terms of resource consumption, data requirements, etc.). The Comet system uses explicit representation and reasoning with commitments to aid the software design and development process -- in particular, to lead software developers to make decisions that result in reuse. Developers can examine the commitments that must be met in order to include an existing module, and can explore how commitments change when modules are modified. Comet has been applied to the domain of sensor-based tracker software. I. Introduction A major problem for software developers is judging how a change in a module affects and is affected by the rest of the design. Modules need to change for a variety of reasons: an existing system is modified, a change in an ongoing design is proposed, a bug is found, et...","1994-05-06"
300,75372,"Run-Time Monitoring of Real-Time Systems","Farnam Jahanian;","Introduction In designing real-time systems, we often make assumptions about the behavior of the system and its environment. These assumptions take many forms, such as upper bounds on interprocess communication delay, deadlines on the execution of tasks, or minimum separations between occurrences of two events. They are often made to deal with the unpredictability of the external environment or to simplify a problem that is otherwise intractable or very hard to solve. Such assumptions may be expressed as part of the formal specification of the system or as scheduling requirements on real-time computations. Despite the contributions of formal verification methods and real-time scheduling results in recent years, the need to perform run-time monitoring of these systems is not diminished, for several reasons: the execution environment of most systems is imperfect and the interaction with the external world introduces additional unpredictability; design assumptions can be violated","1997-03-28"
301,75491,"Approximating Value Trees in Structured Dynamic Programming","Craig Boutilier; Richard Dearden;","We propose and examine a method of approximate dynamic programming for Markov decision processes based on structured problem representations. We assume an MDP is represented using a dynamic Bayesian network, and construct value functions using decision trees as our function representation. The size of the representation is kept within acceptable limits by pruning these value trees so that leaves represent possible ranges of values, thus approximating the value functions produced during optimization. We propose a method for detecting convergence,prove errors bounds on the resulting approximately optimal value functions and policies, and describe some preliminary experimental results. 1 Introduction Markov decision processes (MDPs) have come to play an increasingly important role in AI research, forming the basic model for much recent research in decision-theoretic planning (DTP) and reinforcement learning (RL). The aim in both DTP and RL is to discover a policy for the behavior of an ag...","1996-04-16"
302,75785,"Generating Efficient Code From Data-Flow Programs","Christophe Ratel; Nicolas Halbwachs; Pascal Raymond;","This paper presents the techniques applied in compiling the synchronous dataflow language Lustre. The most original technique consists in synthesising an efficient control structure, by simulating the behavior of boolean variables at compiletime. Here, the techniques are explained on a small subset of Lustre. 1 Introduction Many authors [Kah74, Gra82, PP83, AW85] have advocated the advantages of data-flow languages, mainly due to their mathematical soundness, the ease of formal program construction and transformation and the absence of side effects. However, no such language is actually used, mainly because no good compilers exist for standard machines. The absence of assignment and control structures makes it difficult to produce efficient code from a data-flow program. We have argued elsewhere [BCHP86, CPHP87] that the declarative style allowed by data-flow languages makes them especially suitable for a class of real time programs: such programs can be found in domains (automatic co...","1994-06-10"
303,76113,"New Generation of UPPAAL","Carsten Weise; Fredrik Larsson; Johan Bengtsson; Kim Larsen; Paul Pettersson; Yi Wang;",". Uppaal is a tool-set for the design and analysis of real-time systems. In [6] a relatively complete description of Uppaal before 1997 has been given. This paper is focused on the most recent developments and also to complement the paper of [6]. 1 UPPAAL's Past: the History The first prototype of Uppaal, named Tab at the time, was developed at Uppsala University in 1993 by Wang Yi et al. Its theoretical foundation was presented in FORTE94 [11] and the initial design was to check safety properties that can be formalized as simple reachability properties for networks of timed automata. The restriction to this simple class of properties was in sharp contrast to other real-time verification tools at that time, which where developed to check timed bisimularities or formulae of timed modal ¯-calculi. However, the ambition of catering for more complicated formulae lead to extremely severe restrictions in the size of systems that could be verified by those tools. The essential ideas behind T...","1998-05-09"
304,76418,"Ten Commandments of Formal Methods","Jonathan P. Bowen; Michael G. Hinchey;","The formal methods community is in general very good at undertaking research into the mathematical aspects of formal methods, but not so good at promulgating the use of formal methods in an engineering environment and at an industrial scale. Technology transfer is an extremely important part of the overall effort necessary in the acceptance of formal techniques. This paper explores some of the more informal aspects of applying formal methods and presents some maxims with associated discussion that may help in the application of formal methods in an industrial setting. A significant bibliography is included, providing pointers to more technical and detailed aspects. Why does this magnificent applied science which saves work and makes life easier bring us so little happiness? The simple answer runs: because we have not yet learned to make sensible use of it. -- Albert Einstein","1994-09-29"
305,77030,"Limiting State Explosion with Filter-Based Refinement","David A. Schmidt; Matthew B. Dwyer;","We introduce filters, an abstract-interpretation variant, to incrementally refine a naively generated state space and help validate path properties of the space via model checking. Filters can be represented equivalently as (i) state-transition-based abstract interpretations, (ii) ""property automata,"" or (iii) path formulas in a CTL*- variant. We give examples of filters and show their application in the FLAVERS static analysis system. It is no accident that a compiler uses a control flow graph (cfg ) as its ""abstract interpretation of choice"" for a sequential program: A program's cfg possesses a manageable state space, and a variety of code improvements are enacted by conducting additional flow analyses on top of the cfg . Of course, such analyses are abstract interpretations (ai s) themselves, but what is noteworthy here is that the cfg is used as the base structure upon which another analysis is placed: one ai (the flow analysis) ""filters"" the other (the cfg ). Unfortunately, this ...","1997-09-19"
306,77337,"Mechanizing Programming Logics in Higher Order Logic","Michael J. C. Gordon;","Formal reasoning about computer programs can be based directly on the semantics of the programming language, or done in a special purpose logic like Hoare logic. The advantage of the first approach is that it guarantees that the formal reasoning applies to the language being used (it is well known, for example, that Hoare's assignment axiom fails to hold for most programming languages). The advantage of the second approach is that the proofs can be more direct and natural. In this paper, an attempt to get the advantages of both approaches is described. The rules of Hoare logic are mechanically derived from the semantics of a simple imperative programming language (using the HOL system). These rules form the basis for a simple program verifier in which verification conditions are generated by LCF-style tactics whose validations use the derived Hoare rules. Because Hoare logic is derived, rather than postulated, it is straightforward to mix semantic and axiomatic reasoning. It is also s...","1997-03-27"
307,77560,"VIS: A System for Verification and Synthesis","Abelardo Pardo; Adnan Aziz; Alberto Sangiovanni-vincentelli; Fabio Somenzi; Gary D. Hachtel; Gitanjali Swamy; Rajeev K. Ranjan; Robert K. Brayton; Shaz Qadeer; Stephen Edwards; Sunil Khatri; Szu-tsung Cheng; Thomas R. Shiple; Yuji Kukimoto;","ion Manual abstraction can be performed by giving a file containing the names of variables to abstract. For each variable appearing in the file, a new primary input node is created to drive all the nodes that were previously driven by the variable. Abstracting a net effectively allows it to take any value in its range, at every clock cycle. Fair CTL model checking and language emptiness check VIS performs fair CTL model checking under Buchi fairness constraints. In addition, VIS can perform language emptiness checking by model checking the formula EG true. The language of a design is given by sequences over the set of reachable states that do not violate the fairness constraint. The language emptiness check can be used to perform language containment by expressing the set of bad behaviors as another component of the system. If model checking or language emptiness fail, VIS reports the failure with a counterexample, (i.e., behavior seen in the system that does not satisfy the property...","1997-01-10"
308,77621,"Dynamic Reordering in a Breadth-First Manipulation Based BDD Package: Challenges and Solutions","Alberto Sangiovanni-vincentelli; Rajeev K. Ranjan; Robert K. Brayton; Wilsin Gosti;","The breadth-first manipulation technique has proven effective in dealing with very large sized BDDs. However, till now the lack of dynamic variable reordering has remained an obstacle in its acceptance. The goal of this work is to provide efficient techniques to address this issue. After identifying the problems with implementing variable swapping (the core operation in dynamic reordering) in breadth-first based packages, we propose techniques to handle the computational and memory overheads. We feel that combining dynamic reordering with the powerful manipulation algorithms of a breadth-first based scheme can significantly enhance the performance of BDD based algorithms. The efficiency of the proposed techniques is demonstrated on a range of examples. 1 Introduction The binary decision diagram (BDD) 1 serves as an important data structure in the representation of Boolean functions, and is widely used in various areas of computer-aided design -- logic synthesis, testing, simulation,...","1998-02-19"
309,78418,"Model-Checking for Real-Time Systems","Kim G. Larsen; Paul Pettersson; Wang Yi;",". Efficient automatic model--checking algorithms for real-time systems have been obtained in recent years based on the state--region graph technique of Alur, Courcoubetis and Dill. However, these algorithms are faced with two potential types of explosion arising from parallel composition: explosion in the space of control nodes, and explosion in the region space over clock-variables. This paper reports on work attacking these explosion problems by developing and combining compositional and symbolic model--checking techniques. The presented techniques provide the foundation for a new automatic verification tool Uppaal . Experimental results show that Uppaal is not only substantially faster than other real-time verification tools but also able to handle much larger systems. 1 Introduction Within the last decade model--checking has turned out to be a useful technique for verifying temporal properties of finite--state systems. Efficient model-- checking algorithms for finite--state syste...","1995-05-16"
310,79188,"Automatic Verification of Timed Circuits","Chris J. Myers; Tomas G. Rokicki;",". This paper presents a new formalism anda new algorithm for verifying timed circuits. The formalism, called orbital nets, allows hierarchical verification based on abehavioral semantics of timed trace theory. We present improvements to a geometric timing algorithm that take advantage of concurrency by using partial orders to reduce the time and space requirements of verification. This algorithm has been fully automated and incorporated into a design system for timed circuits, and experimental results demonstrate that this verification algorithm is practical for realistic examples. 1 Introduction Timing considerations are critical in circuit design. In the design of timed circuits, timing information is taken into account resulting in simpler circuits than their speedindependent counterparts in which gate delays are assumed to be unbounded [1]. Timing information must also be considered for designs with a mixed synchronous/asynchronous environment. Finally, even in speed-independent...","1998-08-03"
311,79248,"Verification of Arithmetic Circuits with Binary Moment Diagrams","Al E. Bryant; Yirng-an Chen;","Binary Moment Diagrams (BMDs) provide a canonical representations for linear functions similar to the way Binary Decision Diagrams (BDDs) represent Boolean functions. Within the class of linear functions, we can embed arbitrary functions from Boolean variables to integer values. BMDs can thus model the functionality of data path circuits operating over word-level data. Many important functions, including integer multiplication, that cannot be represented efficiently at the bit level with BDDs have simple representations at the word level with BMDs. Furthermore, BMDs can represent Boolean functions with around the same complexity as BDDs. We propose a hierarchical approach to verifying arithmetic circuits, where component modules are first shown to implement their word-level specifications. The overall circuit functionality is then verified by composing the component functions and comparing the result to the word-level circuit specification. Multipliers with word sizes of up to 256 bits hav...","1997-01-16"
312,79269,"Validation with Guided Search of the State Space","C. Han Yang; David L. Dill;","In practice, model checkers are most useful when they find bugs, not when they prove a property. However, because large portions of the state space of the design actually satisfy the specification, model checkers devote much effort verifying correct portions of the design. In this paper, we enhance the bug-finding capability of a model checker by using heuristics to search the states that are most likely to lead to an error, first. Reductions of 1 to 3 orders of magnitude in the number of states needed to find bugs in industrial designs have been observed. Consequently, these heuristics can extend the capability of model checkers to find bugs in designs. Keywords Model checking, Guided search, Verification 1. Introduction The complexity of modern chip designs has stretched the ability of the verification techniques and methodologies. Traditional verification techniques use simulators with handcrafted or random test vectors to validate the design. Unfortunately, generating handcraft...","1998-07-08"
313,80195,"Efficient Algorithms for Minimizing Cross Validation Error","Andrew W. Moore; Mary S. Lee;","Model selection is important in many areas of supervised learning. Given a dataset and a set of models for predicting with that dataset, we must choose the model which is expected to best predict future data. In some situations, such as online learning for control of robots or factories, data is cheap and human expertise costly. Cross validation can then be a highly effective method for automatic model selection. Large scale cross validation search can, however, be computationally expensive. This paper introduces new algorithms to reduce the computational burden of such searches. We show how experimental design methods can achieve this, using a technique similar to a Bayesian version of Kaelbling's Interval Estimation. Several improvements are then given, including (1) the use of blocking to quickly spot near-identical models, and (2) schemata search: a new method for quickly finding families of relevant features. Experiments are presented for robot data and noisy synthetic datasets. T...","1994-04-26"
314,80348,"Fine-Grained Mobility in the Emerald System","Andrew Black; Eric Jul; Henry Levy; Norman Hutchinson;","Emerald is an object-based language and system designed for the construction of distributed programs. An explicit goal of Emerald is support for object mobility; objects in Emerald can freely move within the system to take advantage of distribution and dynamically changing environments. We say that Emerald has fine-grained mobility because Emerald objects can be small data objects as well as process objects. Fine-grained mobility allows us to apply mobility in new ways but presents implementation problems as well. This paper discusses the benefits of fine-grained mobility, the Emerald language and run-time mechanisms that support mobility, and techniques for implementing mobility that do not degrade the performance of local operations. Performance measurements of the current implementation are included. 1 Introduction Process migration has been implemented or described as a goal of several distributed systems [19, 13, 15, 18, 7, 23, 8]. In these systems, entire address spaces are move...","1995-11-03"
315,80371,"Automatic Parameter Selection by Minimizing Estimated Error","George H. John; Ron Kohavi;","We address the problem of finding the parameter settings that will result in optimal performance of a given learning algorithm using a particular dataset as training data. We describe a ""wrapper"" method, considering determination of the best parameters as a discrete function optimization problem. The method uses best-first search and cross validation to wrap around the basic induction algorithm: the search explores the space of parameter values, running the basic algorithm many times on training and holdout sets produced by cross-validation to get an estimate of the expected error of each parameter setting. Thus, the final selected parameter settings are tuned for the specific induction algorithm and dataset being studied. We report experiments with this method on 33 datasets selected from the UCI and StatLog collections using C4.5 as the basic induction algorithm. At a 90% confidence level, our method improves the performance of C4.5 on nine domains, degrades performance on one, and is...","1997-08-25"
316,80978,"System-Synthesis using Hardware/Software Codesign","H. Gunther; H. T. Vierhaus; H. Veit; J. Wilberg; P. Ploger; R. Camposano; U. Steinhausen; U. Westerholz;","In this paper we present the basic ideas and concepts of a system synthesis tool. The main goal is to partition the specification into interacting and communicating hardware and software moduls. The internal graph based data structure enables the implementation of different analysis and synthesis algorithms. On the basis of a hardware library containing complex components possible hardware/software configurations are discussed. 1 Introduction System development is a complex design task mostly starting at a high level of abstraction. Beginning with a specification and user constraints, the goal is to find an implementation that fulfills all requirements. The decision to realize the given problem as a ffl full hardware (HW) solution, ffl full software (SW) solution, or ffl mix of hardware and software can be taken by human experience and knowledge or with the assistance of a CAD tool. This contribution focuses on the third alternative and describes our approach to generate the final ...","1994-10-24"
317,81143,"The Frame Problem and Bayesian Network Action Representations","Craig Boutilier;",". We examine a number of techniques for representing actions with stochastic effects using Bayesian networks and influence diagrams. We compare these techniques according to ease of specification and size of the representation required for the complete specification of the dynamics of a particular system, paying particular attention the role of persistence relationships. We precisely characterize two components of the frame problem for Bayes nets and stochastic actions, propose several ways to deal with these problems, and compare our solutions with Reiter 's solution to the frame problem for the situation calculus. The result is a set of techniques that permit both ease of specification and compact representation of probabilistic system dynamics that is of comparable size (and timbre) to Reiter's representation (i.e., with no explicit frame axioms). 1 Introduction Reasoning about action has been a central problem in artificial intelligence since its inception. Since the earliest att...","1996-03-05"
318,81221,"Prioritized Goal Decomposition of Markov Decision Processes: Toward a Synthesis of Classical and Decision Theoretic Planning","Christopher Geib; Craig Boutilier; Ronen I. Brafman;","We describe an approach to goal decomposition for a certain class of Markov decision processes (MDPs). An abstraction mechanism is used to generate abstract MDPs associated with different objectives, and several methods for merging the policies for these different objectives are considered. In one technique, causal (least-commitment) structures are generated for abstract policies and plan merging techniques, exploiting the relaxation of policy commitments reflected in this structure, are used to piece the results into a single policy. Abstract value functions provide guidance if plan repair is needed. This work makes some first steps toward the synthesis of classical and decision theoretic planning methods. 1 Introduction Markov decision processes (MDPs) have become a standard conceptual and computational model for decision theoretic planning (DTP) problems, allowing one to model uncertainty, competing goals, and process-oriented objectives. One of the key drawbacks of MDPs, vis-a-vis ...","1997-04-28"
319,81619,"Term Rewriting Systems","J. W. Klop;","Term Rewriting Systems play an important role in various areas, such as abstract data type specifications, implementations of functional programming languages and automated deduction. In this chapter we introduce several of the basic concepts and facts for TRS's. Specifically, we discuss Abstract Reduction Systems","1997-10-30"
320,82291,"Hardware generation and partitioning effects in the COSYMA system","J. Henkel; R. Ernst; Th. Benner;","This paper reports on the current results that have been gained by a fully automatically hardware/software partitioning process in our co--synthesis system COSYMA [ErHe92]. By means of some examples we will describe the role of the hardware/software communication overhead and the way the partitioning procedure is influenced by communication. 1 Introduction The range of applications for embedded real-time systems is steadily increasing: car electronics, office automation, telecommunication etc., benefit from progress in embedded system performance with the consequence of more functionality and complexity at lower prices. Hardware/software co-design plays a key role in this development: a real-time system is no longer viewed as a system consisting of hardware on the one side and software on the other side. Instead, the system is described by its behavior, optimization goals like area minimization, low power consumption etc. and time constraints, formulated in a high level description. I...","1997-06-18"
321,82447,"Constraints as a Tool for Distributed Scheduling","Mark Wallace;","Constraints programming is well-placed to take a key role in the development of distributed scheduling systems. Problem constraints can be directly encoded as program constraints. Specialised algorithms can be naturally embedded and - more importantly - they can be concurrently executed as active constraints. Finally constraint-based computation provides the dynamic control which is necessary to allow a program to take advantage of the actual data at runtime to modify its analysis of the problem. The simplicity and efficiency of constraints programming has been proved on standard job-shop scheduling benchmarks, and ""local consistency"" techniques contribute dramatically to this efficiency. Local consistency is enforced locally and concurrently for the different problem constraints. The concurrency of the constraint behaviour makes constraints particularly suitable for distributed computation. The final problem is to put together locally consistent solutions into a globally consistent s...","1997-11-13"
322,82926,"The Active Database Management System Manifesto: A Rulebase of ADBMS Features","Klaus R. Dittrich; Stella Gatziu;",". Active database systems have been a hot research topic for quite some years now. However, while ""active functionality"" has been claimed for many systems, and notions such as ""active objects"" or ""events"" are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of ""active database management system"" as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas. 1 Introduction Active database management systems (ADBMSs) [e.g., 4, 6, 15] have recently become a ve...","1995-08-11"
323,83184,"Finding Optimal Solutions to the Twenty-Four Puzzle","Larry A. Taylor; Richard E. Korf;","We have found the first optimal solutions to a complete set of random instances of the Twenty-Four Puzzle, the 5 \Theta 5 version of the well-known sliding-tile puzzles. Our new contribution to this problem is a more powerful admissible heuristic function. We present a general theory for the automatic discovery of such heuristics, which is based on considering multiple subgoals simultaneously. In addition, we apply a technique for pruning duplicate nodes in depth-first search using a finite-state machine. Finally, we observe that as heuristic search problems are scaled up, more powerful heuristic functions become both necessary and cost-effective. Introduction The sliding-tile puzzles, such as the Eight and Fifteen Puzzle, have long served as testbeds for heuristic search in AI. A square frame is filled with numbered tiles, leaving one position empty, called the blank. Any tile that is horizontally or vertically adjacent to the blank can be slid into the blank position. The task is to...","1999-03-30"
324,83263,"Formal Methods: State of the Art and Future Directions","Edmund M. Clarke; Jeannette M. Wing;","ing with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or permissions@acm.org. 2 Delta E.M. Clarke and J.M. Wing About Programs---Mechanical verification, Specification techniques; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic---Mechanical theorem proving General Terms: Software engineering, formal methods, hardware verification Additional Key Words and Phrases: Software specification, model checking, theorem proving 1. INTRODUCTION Hardware and software systems will inevitably grow in scale and functionality. Because of this increase in complexity, the likelihood of subtle errors is much greater. Moreover, some of these errors may cause catastrophic loss of money, time, or even huma...","1996-11-05"
325,83509,"Clock Difference Diagrams","Carsten Weise; Justin Pearson; Kim G. Larsen; Wang Yi;","This paper deals with a new data structure to express constraints of conjunctions of disjunctions of such temporal constraints. 2 Preliminary Results","1999-02-01"
326,83730,"An Implementation of Three Algorithms for Timing Verification Based on Automata Emptiness","C. Courcoubetis; D. Dill; H. Wong-toi; N. Halbwachs; R. Alur;","This papers describes modifications to and the implementation of algorithms previously described in [1, 11]. We first describe three generic (untimed) algorithms for constructing graphs of the reachable states of a system, and how these graphs can be used for verification. They all have as input an implicit description of a transition system. We then apply these algorithms to real-time systems. The first algorithm performs a straightforward reachability analysis on sets of states of the system, rather than on individual states. This corresponds to stepping symbolically through the system many states at a time. In the case of a real-time system this procedure constructs a graph where each node is the union of some regions of the regions graph. There is therefore no need for an a priori partitioning of the state space into individual regions; however, this approach potentially leads to exponentially worse complexity since its potential state space is the power set of regions [1]. The other two algorithms we consider are minimization algorithms [12, 13, 11]. These simultaneously perform reachability analysis and minimization from an implicit system description. These can lead to great savings when the minimized graph is much smaller than the explicit reachable graph. Our paradigm for verification is to test for the emptiness of the set of all timed system executions that violate a requirements specification. One way to specify and verify non-terminating processes is to model them as languages of !-sequences of events [14, 15, 16, 1, 17, 18]. Modular processes can be constructed via composition operations involving language intersection. Specifications are also given as languages: they contain all acceptable event sequences. Program correctness is then just language contain...","1996-08-06"
327,84264,"Searching for Dependencies in Bayesian Classifiers","Michael J. Pazzani;","Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data. 23.1 Introduction The Bayesian classifier (Duda & Hart, 1973) is a probabilistic method for classification. It can be used to determine the pr...","1995-12-29"
328,84338,"Characterization of Temporal Property Classes","Amir Pnueli; Edward Chang; Zohar Manna;",". This paper presents two novel characterizations of the classes of properties of reactive systems in terms of their expression by temporal logic. The first family of characterizations concerns the safety-progress classification, which describes a hierarchy within the set of temporal properties. Previous characterizations of this hierarchy depended critically on the use of past temporal operators. The characterization presented here identifies the future formulas that belong to each class. This characterization is shown to be complete. The second characterization concerns the safety-liveness classification, which partitions temporal properties into the classes of safety and liveness. While automata-theoretic and temporal logic characterizations of the safety class have been known for some time, a complete characterization of the liveness class by temporal logic remained open. This paper provides such a characterization. 1 Introduction Reactive systems are systems that are expected to...","1999-02-05"
329,85473,"A Comparison of Logistic Regression to Decision-Tree Induction in a Medical Domain","Harry P. Selker; John L. Griffith; Ralph B. D'agostino; William J. Long;","This paper compares the performance of logistic regression to decision-tree induction in classifying patients as having acute cardiac ischemia. This comparison was performed using the database of 5,773 patients originally used to develop the logistic-regression tool and test it prospectively. Both the ability to classify cases and ability to estimate the probability of ischemia were compared on the default tree generated by the C4 version of ID3. They were also compared on a tree optimized on the learning set by increased pruning of overspecified branches, and on a tree incorporating clinical considerations. Both the LR tool and the improved trees performed at a level fairly close to that of the physicians, although the LR tool definitely performed better than the decision tree. There were a number of differences in the performance of the two methods, shedding light on their strengths and weaknesses. 1 Introduction The use of data to develop decision procedures has a long history with...","1994-04-13"
330,85578,"Retargetable Instruction Scheduling for Pipelined Processors","David Gordon Bradlee; Robert R. Henry; Susan J. Eggers;","Retargetable Instruction Scheduling for Pipelined Processors by David Gordon Bradlee Chairperson of the Supervisory Committee: Professor Susan J. Eggers Department of Computer Science and Engineering Retargetable code generators for complex instruction set computers (CISCs) have focused on sophisticated pattern matching code selection, because CISCs provide many machine instruction sequence choices. Recent pipelined processors, known as reduced instruction set computers (RISCs), provide fewer instruction sequence choices, but expose pipeline and functional unit costs to the compiler. For RISCs the compiler's emphasis must be shifted from code selection to instruction scheduling, resulting in code generation issues that are different than those for CISCs. In particular, the machine description language for a retargetable RISC compiler must contain scheduling requirements. Also, the interaction between register allocation and instruction scheduling is significant. This dissertation com...","1994-03-15"
331,85761,"Evaluating Deadlock Detection Methods for Concurrent Software","James C. Corbett;","Static analysis of concurrent programs has been hindered by the well known state explosion problem. Although many different techniques have been proposed to combat this state explosion, there is little empirical data comparing the performance of the methods. This information is essential for assessing the practical value of a technique and for choosing the best method for a particular problem. In this paper, we carry out an evaluation of three techniques for combating the state explosion problem in deadlock detection: reachability search with a partial order state space reduction, symbolic model checking, and inequality necessary conditions. We justify the method used for the comparison, and carefully analyze several sources of potential bias. The results of our evaluation provide valuable data on the kinds of programs to which each technique might best be applied. Furthermore, we believe that the methodological issues we discuss are of general significance in comparison of analysis te...","1996-09-15"
332,85875,"An Improved Algorithm for the Evaluation of Fixpoint Expressions","A. Browne; D. E. Long; E. M. Clarke; S. Jha; W. Marrero;","Many automated finite-state verification procedures can be viewed as fixpoint computations over a finite lattice (typically the powerset of the set of system states). For this reason, fixpoint calculi such as those proposed by Kozen and Park have proven useful, both as ways to describe verification algorithms and as specification formalisms in their own right. We consider the problem of evaluating expressions in these calculi over a given model. A naive algorithm for this task may require time n q , where n is the maximum length of a chain in the lattice and q is the depth of fixpoint nesting. In 1986, Emerson and Lei presented a method requiring about n d steps, where d is the This research was sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Material Command,USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. The views and conclusions contained in this document are those of the authors and should not be ...","1999-01-15"
333,85884,"Computing Bisimulations for Stochastic Process Algebras using Symbolic Representations","H. Hermanns; M. Siegle;","Stochastic process algebras have been introduced in order to enable compositional performance analysis. The size of the state space is a limiting factor, especially if the system consists of many cooperating components. To fight state space explosion, various proposals for compositional aggregation have been made. They rely on minimisation with respect to a congruence relation. This paper addresses the computational complexity of minimisation algorithms and explains how efficient, BDD-based data structures can be employed for this purpose. Keywords: Stochastic Process Algebra, Binary Decision Diagram, Bisimulation, Performance Analysis, Markov Chain. 1 Introduction Compositionalapplication of stochastic process algebras (SPA) is particularly successful if the system structure can be exploited during Markov chain generation. For this purpose, congruence relations have been developed which justify minimisation of components without touching behavioural properties. Examples of such relat...","1998-10-06"
334,86259,"Reuse of a Formal Model for Requirements Validation","Robyn R. Lutz;","This paper reports experience from how a project engaged in the process of requirements analysis for evolutionary builds can reuse the formally specified design model produced for a similar, earlier project in the same domain. Two levels of reuse are described here. First, a formally specified generic design model was generated on one project to systematically capture the design commonality in a set of software monitors onboard a spacecraft. These monitors periodically check for faults and invoke recovery software when needed. The paper summarizes the use of the design model to validate the software design of the various monitors on that first project. Secondly, the paper describes how the formal design model created for the first project was reused on a second, subsequent project. The model was reused to validate the evolutionary requirements for the second project's software monitors, which were being developed in a series of builds. Some mismatches due to the very different architec...","1997-08-25"
335,86564,"Approximate Reachability Don't Cares for CTL Model Checking","Carl Pixley; Fabio Somenzi; Gary D. Hachtel; Jae-young Jang;","RDCs (Reachability Don't Cares) can have a dramatic impact on the cost of CTL model checking [18]. Unfortunately, RDCs, being a global property, are often much more difficult to compute than the satisfying set of typical CTL formulas. We address this problem through the use of Approximate Reachability Don't Cares (ARDCs), computed with the algorithms developed for the VERITAS sequential synthesis package [4, 5]. Approximate Reachable states represent an upper bound on the set of true reachable states, and thus a lower bound on the set of unreachable (Don't Care) states. ARDCs can be 10X to 100X (or much more for very large circuits) cheaper to compute than RDCs, and in some cases have the same dramatic effect on CTL model checking as the real RDCs. We also discuss the application of ARDCs to the problem of exact computation of the RDCs themselves. Experiments on industrial benchmarks show that order of magnitude speedups are possible, and occur frequently. The experimental results pres...","1998-10-23"
336,86700,"A Tutorial on Using PVS for Hardware Verification","J. M. Rushby; M. K. Srivas; N. Shankar; S. Owre;",". PVS stands for ""Prototype Verification System."" It consists of a specification language integrated with support tools and a theorem prover. PVS tries to provide the mechanization needed to apply formal methods both rigorously and productively. This tutorial serves to introduce PVS and its use in the context of hardware verification. In the first section, we briefly sketch the purposes for which PVS is intended and the rationale behind its design, mention some of the uses that we and others are making of it. We give an overview of the PVS specification language and proof checker. The PVS language, system, and theorem prover each have their own reference manuals, 1;2 ;3 which you will need to study in order to make productive use of the system. A pocket reference card, summarizing all the features of the PVS language, system, and prover is also available. The purpose of this tutorial is not to describe in detail the features of PVS and how to use the system. Rather, its purpose is to...","1994-11-02"
337,86872,"Rewrite Systems","Jean-pierre Jouannaud; Nachum Dershowitz;","Completion Completion has recently been put in a more abstract framework [ Bachmair-et al, 1986 ] , an approach we adopt here. As in traditional proof theory (cf. [ Takeuti, 1987 ] ), proofs are reduced, in some well-founded sense, by replacing locally maximal subproofs with smaller ones, until a normal-form proof is obtained. In completion, the axioms used are in a constant state of flux; these changes are expressed as inference rules, which add a dynamic character to establishing the existence of reducible subproofs. This view of completion, then, has two main components: an inference system, used in the completion process to generate new rewrite rules, and a rewrite relation that shows how any proof can be normalized, as long as the appropriate rules have been generated. An inference rule (for our purposes) is a binary relation between pairs (E; R), where E is a set of equations and R is a set of rewrite rules. (Rules or equations that differ only in the names of their variable are,...","1995-01-27"
338,86928,"On the Verification of Qualitative Properties of Probabilistic Processes under Fairness Constraints","Christel Baier; Marta Kwiatkowska;","We consider sequential and concurrent probabilistic processes and propose a general notion of fairness with respect to probabilistic choice, which allows to express various notions of fairness such as process fairness and event fairness. We show the soundness of proving the validity of qualitative properties of probabilistic processes under fairness constraints in the sense that whenever all fair executions of a probabilistic process fulfill a certain linear time property E then E holds for almost all executions (i.e. E holds with probability 1). It follows that in order to verify probabilistic processes w.r.t. linear time specifications, it suffices to establish that -- for some instance of our general notion of fairness -- all fair executions satisfy the specification. This generalizes the soundness results for extreme and ff-fairness established in [25] and [27] respectively. Furthermore, we show that ff-fairness of [27] is the only fairness notion which is complete for validity of...","1998-02-25"
339,87064,"Unifying SAT-based and Graph-based Planning","Bart Selman; Henry Kautz;","The Blackbox planning system unifies the planning as satisfiability framework (Kautz and Selman 1992, 1996) with the plan graph approach to STRIPS planning (Blum and Furst 1995). We show that STRIPS problems can be directly translated into SAT and efficiently solved using new randomized systematic solvers. For certain computationally challenging benchmark problems this unified approach outperforms both SATPLAN and Graphplan alone. We also demonstrate that polynomialtime SAT simplification algorithms applied to the encoded problem instances are a powerful complement to the ""mutex"" propagation algorithm that works directly on the plan graph. 1 Introduction It has often been observed that the classical AI planning problem (that is, planning with complete and certain information) is a form of logical deduction. Because early attempts to use general theorem provers to solve planning problems proved impractical, research became focused on specialized planning algorithms. Sometimes the rela...","1999-04-01"
340,87233,"Factored Edge-Valued Binary Decision Diagrams","Massoud Pedram; Paul Tafertshofer;","Factored Edge-Valued Binary Decision Diagrams form an extension to Edge-Valued Binary Decision Diagrams. By associating both an additive and a multiplicative weight with the edges, FEVBDDs can be used to represent a wider range of functions concisely. As a result, the computational complexity for certain operations can be significantly reduced compared to EVBDDs. Additionally, the introduction of multiplicative edge weights allows us to directly represent the so-called complement edges which are used in OBDDs, thus providing a one to one mapping of all OBDDs to FEVBDDs. Applications such as integer linear programming and logic verification that have been proposed for EVBDDs also benefit from the extension. We also present a complete matrix package based on FEVBDDs and apply the package to the problem of solving the Chapman-Kolmogorov equations. Keywords: Ordered Binary Decision Diagrams, Pseudo-Boolean Functions, Affine Property, Logic Verification, Integer Linear Programming, Matrix O...","1997-11-25"
341,87265,"Combination Techniques for Non-Disjoint Equational Theories","Christophe Ringeissen; Eric Domenjoud; Francis Klay;","ion variables which are variables coming from an abstraction, either during preprocessing or during the algorithm itself. 3. Introduced variables which are variables introduced by the unification algorithms for each theory. We make the very natural assumption that the unification algorithm for each theory may recognize initial, abstraction and introduced variables and never assigns an introduced variable to a non-introduced one or an abstraction variable to an initial one. With this assumption, our combination algorithm will always make an introduced variable appear in at most one Gamma i . We may thus also suppose that the domain of each solution does not contain an introduced variable. This does not compromise the soundness of our algorithm. The combination algorithm is described by the two rules given in figure 2. In the rule UnifSolve i , ae SF is obtained by abstracting aliens in the range of ae by fresh variables. ae F i is the substitution such that xae = xae SF ae F i for al...","1994-12-01"
342,87485,"Robust Linear Discriminant Trees","George H. John;","We present a new method for the induction of classification trees with linear discriminants as the partitioning function at each internal node. This paper presents two main contributions: first, a novel objective function called soft entropy which is used to identify optimal coefficients for the linear discriminants, and second, a novel method for removing outliers called iterative re-filtering which boosts performance on many datasets. These two ideas are presented in the context of a single learning algorithm called DT-SEPIR. 1 Introduction In the Fifth International Workshop on Artificial Intelligence and Statistics, Ft. Lauderdale, FL, January 1995. Proceedings were printed only for conference attendees. A slightly shortened version appears in D. Fisher and H. Lenz (Eds.), Learning From Data: Artificial Intelligence and Statistics V, Springer-Verlag, New York, 1996. Recursive partitioning classifiers, or decision trees, are an important nonparametric function representation in Sta...","1999-02-05"
343,87538,"Selection of Relevant Features in Machine Learning","Pat Langley;","In this paper, we review the problem of selecting relevant features for use in machine learning. We describe this problem in terms of heuristic search through a space of feature sets, and we identify four dimensions along which approaches to the problem can vary. We consider recent work on feature selection in terms of this framework, then close with some challenges for future work in the area. 1. The Problem of Irrelevant Features The selection of relevant features, and the elimination of irrelevant ones, is a central problem in machine learning. Before an induction algorithm can move beyond the training data to make predictions about novel test cases, it must decide which attributes to use in these predictions and which to ignore. Intuitively, one would like the learner to use only those attributes that are `relevant' to the target concept. There have been a few attempts to define `relevance' in the context of machine learning, as John, Kohavi, and Pfleger (1994) have noted in their...","1997-04-13"
344,87741,"An Overview of Cooperative Answering","Jack Minker; Parke Godfrey; Terry Gaasterl;","Databases and information systems are often hard to use because they do not explicitly attempt to cooperate with their users. Direct answers to database and knowledge base queries may not always be the best answers. Instead, an answer with extra or alternative information may be more useful and less misleading to a user. This paper surveys foundational work that has been done toward endowing intelligent information systems with the ability to exhibit cooperative behavior. Grice's maxims of cooperative conversation, which provided a starting point for the field of cooperative answering, are presented along with relevant work in natural language dialogue systems, database query answering systems, and logic programming and deductive databases. The paper gives a detailed account of cooperative techniques that have been developed for considering users' beliefs and expecations, presuppositions, and misconceptions. Also, work in intensional answering and generalizing queries and answers is co...","1998-12-08"
345,88318,"Verification of Asynchronous Circuits by BDD-based Model Checking of Petri Nets","Enric Pastor; Jordi Cortadella; Oriol Roig;",". This paper presents a methodology for the verification of speed-independent asynchronous circuits against a Petri net specification. The technique is based on symbolic reachability analysis, modeling both the specification and the gate-level network behavior by means of boolean functions. These functions are efficiently handled by using Binary Decision Diagrams. Algorithms for verifying the correctness of designs, as well as several circuit properties are proposed. Finally, the applicability of our verification method has been proven by checking the correctness of different benchmarks. 1 Introduction During these last few years, asynchronous circuits have gained interest due to their promising advantages, such as local synchronization, elimination of the clock skew problem, faster and less power-consuming circuits, and high degree of modularity. However, the concurrent nature of asynchronous circuits makes them difficult to design because all transitions must be taken into account ...","1996-10-23"
346,88938,"A Logic Programming Language with Lambda-Abstraction, Function Variables, and Simple Unification","Dale Miller;","ion, Function Variables, and Simple Unification Dale Miller Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104--6389 USA Abstract: It has been argued elsewhere that a logic programming language with function variables and -abstractions within terms makes a good meta-programming language, especially when an object-language contains notions of bound variables and scope. The Prolog logic programming language and the related Elf and Isabelle systems provide meta-programs with both function variables and -abstractions by containing implementations of higher-order unification. This paper presents a logic programming language, called L , that also contains both function variables and -abstractions, although certain restrictions are placed on occurrences of function variables. As a result of these restrictions, an implementation of L does not need to implement full higherorder unification. Instead, an extension to first-order unification that res...","1996-06-04"
347,89365,"Automated Analysis of Concurrent Systems with the Constrained Expression Toolset","George S. Avrunin; Jack C. Wileden; James C. Corbett; Laura K. Dillon; Ugo A. Buy;","The constrained expression approach to analysis of concurrent software systems has several attractive features, including the facts that it can be used with a variety of design and programming languages and that it does not require a complete enumeration of the set of reachable states of the concurrent system. This paper reports on the construction of a toolset automating the main constrained expression analysis techniques and the results of experiments with that toolset. The toolset is capable of carrying out completely automated analyses of a variety of concurrent systems, starting from source code in an Ada-like design language and producing system traces displaying the properties represented by the analyst 's queries. It has been successfully used with designs that involve hundreds of concurrent processes. This paper appeared in IEEE Trans. Softw. Eng., vol. 17, no. 11, pp. 1204--1222, 1991. Research partially supported by NSF grant CCR-8806970 and ONR grant N00014-89-J1064. ...","1996-01-23"
348,89425,"EXPECT: Explicit Representations for Flexible Acquisition","Bill Swartout; Yolanda Gil;",": To create more powerful knowledge acquisition systems, we not only need better acquisition tools, but we need to change the architecture of the knowledge based systems we create so that their structure will provide better support for acquisition. Current acquisition tools permit users to modify factual knowledge but they provide limited support for modifying problem solving knowledge. In this paper, we argue that this limitation (and others) stem from the use of incomplete models of problem solving knowledge and inflexible specification of the interdependencies between problem solving and factual knowledge. We describe the EXPECT architecture which addresses these problems by providing an explicit representation for problem solving knowledge and intent. Using this more explicit representation, EXPECT can automatically derive the interdependencies between problem solving and factual knowledge. By deriving these interdependencies from the structure of the knowledge-based system itself ...","1995-04-22"
349,89713,"Model Checking Software Systems: A Case Study","Jeannette M. Wing;","Model checking is a proven successful technology for verifying hardware. It works, however, on only finite state machines, and most software systems have infinitely many states. Our approach to applying model checking to software hinges on identifying appropriate abstractions that exploit the nature of both the system, S, and the property, OE, to be verified. We check OE on an abstracted, but finite, model of S. Following this approach we verified three cache coherence protocols used in distributed file systems. These protocols have to satisfy this property: ""If a client believes that a cached file is valid then the authorized server believes that the client's copy is valid."" In our finite model of the system, we need only represent the ""beliefs"" that a client and a server have about a cached file; we can abstract from the caches, the files' contents, and even the files themselves. Moreover, by successive application of the generalization rule from predicate logic, we need only conside...","1995-07-11"
350,90456,"Techniques For Efficient Formal Verification Using Binary Decision Diagrams","Alan John Hu; Vaughan R. Pratt;","The appeal of automatic formal verification is that it's automatic --- minimal human labor and expertise should be needed to get useful results and counterexamples. BDD(binary decision diagram)-based approaches have promised to allow automatic verification of complex, real systems. For large classes of problems, however, (including many distributed protocols, multiprocessor systems, and network architectures) this promise has yet to be fulfilled. Indeed, the few successes have required extensive time and effort from sophisticated researchers in the field. Clearly, techniques are needed that are more sophisticated than the obvious direct implementation of theoretical results. This thesis addresses that need, emphasizing an application domain that has been particularly difficult for BDD-based methods --- high-level models of systems or distributed protocols --- rather than gate-level descriptions of circuits. Additionally, the emphasis is on providing useful debugging information for the...","1998-12-29"
351,91066,"Partial Constraint Satisfaction","Eugene C. Freuder; Richard J. Wallace;",". A constraint satisfaction problem involves finding values for variables subject to constraints on which combinations of values are allowed. In some cases it may be impossible or impractical to solve these problems completely. We may seek to partially solve the problem, in particular by satisfying a maximal number of constraints. Standard backtracking and local consistency techniques for solving constraint satisfaction problems can be adapted to cope with, and take advantage of, the differences between partial and complete constraint satisfaction. Extensive experimentation on maximal satisfaction problems illuminates the relative and absolute effectiveness of these methods. A general model of partial constraint satisfaction is proposed. 1 Introduction Constraint satisfaction involves finding values for problem variables subject to constraints on acceptable combinations of values. Constraint satisfaction has wide application in artificial intelligence, in areas ranging from temporal r...","1996-05-28"
352,91436,"Learning the CLASSIC Description Logic: Theoretical and Experimental Results","Haym Hirsh; William W. Cohen;","We present a series of theoretical and experimental results on the learnability of description logics. We first extend previous formal learnability results on simple description logics to C-Classic, a description logic expressive enough to be practically useful. We then experimentally evaluate two extensions of a learning algorithm suggested by the formal analysis. The first extension learns C-Classic descriptions from individuals. (The formal results assume that examples are themselves descriptions.) The second extension learns disjunctions of C-Classic descriptions from individuals. The experiments, which were conducted using several hundred target concepts from a number of domains, indicate that both extensions reliably learn complex natural concepts. 1 INTRODUCTION One well-known family of formalisms for representing knowledge are description logics, sometimes also called terminological logics or KL-ONE-type languages. Description logics have been applied in a number of contexts...","1996-07-11"
353,92192,"Higher-Level Specification and Verification With BDDs","Alan J. Hu; Andreas J. Drexler; C. Han Yang; David L. Dill;","Currently, many are investigating promising verification methods based on Boolean decision diagrams (BDDs). Using BDDs, however, requires modeling the system under verification in terms of Boolean formulas. This modeling can be difficult and error-prone, especially when dealing with constructs like arithmetic, sequential control flow, and complex data structures. We present new techniques for automatically translating these constructs into BDDs. Furthermore, these techniques generate Boolean next-state relations in a form that allows efficient image computation without building the full BDD for the next-state relation, thereby side-stepping the commonly-encountered BDD-size blowup of next-state relations. 1 Introduction With the high complexity of hardware designs and protocols, improved debugging tools and methodologies are critical to avoiding the expenses and delays resulting from discovering bugs late in the design phase [9, 4]. Simulation catches some problems, but bugs frequent...","1996-05-21"
354,92245,"The SkyBlue Constraint Solver and Its Applications","Michael Sannella;","The SkyBlue constraint solver is an efficient incremental algorithm that uses local propagation to maintain sets of required and preferential constraints. SkyBlue is a successor to the DeltaBlue algorithm, which was used as the constraint solver in the ThingLab II user interface development environment. Like DeltaBlue, SkyBlue represents constraints between variables by sets of short procedures (methods) and incrementally resatisfies the set of constraints as individual constraints are added and removed. DeltaBlue has two significant limitations: cycles of constraints are prohibited, and constraint methods can only have a single output variable. SkyBlue relaxes these restrictions, allowing cycles of constraints to be constructed (although SkyBlue may not be able to satisfy all of the constraints in a cycle) and supporting multi-output methods. This paper presents the SkyBlue algorithm and discusses several applications that have been built using SkyBlue. 1 Introduction The DeltaBlue a...","1993-05-10"
355,92433,"Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism","David D. Clark; Lixia Zhang; Scott Shenker;","This paper considers the support of real-time applications in an Integrated Services Packet Network (ISPN). We first review the characteristics of real-time applications. We observe that, contrary to the popular view that real-time applications necessarily require a fixed delay bound, some realtime applications are more flexible and can adapt to current network conditions. We then propose an ISPN architecture that supports two distinct kinds of real-time service: guaranteed service, which is the traditional form of realtime service discussed in most of the literature and involves pre-computed worst-case delay bounds, and predicted service which uses the measured performance of the network in computing delay bounds. We then propose a packet scheduling mechanism that can support both of these real-time services as well as accommodate datagram traffic. We also discuss two other aspects of an overall ISPN architecture: the service interface and the admission control criteria. 1 Introduct...","1994-09-20"
356,92606,"Abstraction Mechanisms for Hardware Verification","Thomas F. Melham;","ion Mechanisms for Hardware Verification Thomas F. Melham University of Cambridge Computer Laboratory New Museums Site, Pembroke Street Cambridge, CB2 3QG, England Abstract: It is argued that techniques for proving the correctness of hardware designs must use abstraction mechanisms for relating formal descriptions at different levels of detail. Four such abstraction mechanisms and their formalization in higher order logic are discussed. Introduction Recent advances in microelectronics have given designers of digital hardware the potential to build electronic devices of unprecedented size and complexity. With increasing size and complexity, however, it becomes increasingly difficult to ensure that such systems will not malfunction because of design errors. This problem has prompted some researchers to look for a firm theoretical basis for correct design of hardware systems. Mathematical methods have been developed to model the functional behaviour of electronic devices and to verify,...","1995-04-27"
357,92901,"Syntax Independent Connections","Jean Goubault;","We present a new approach to classical first-order theorem proving, and derive a method from it, which can handle formulas for which the textual representation is no help in guiding the proof. Inspired by Prawitz's method, it is close in spirit to Bibel's connection method. However, the strategy we use is different: at each stage in the search for a proof, the prover minimizes the branching factor, and thus the loss of information. We show experimental results on classical test problems, and analyse them. We show that the parameters of the method are measures of the degree of difficulty of the theorem to prove. This leads us to justify future improvements. 1 Introduction Both the connection method [4] or the method of general matings [2] rely on the textual form of the given formula to guide the search, but what happens when the syntax does not help? Indeed, although the syntax of human-entered formulas is likely to guide the proof effectively, it is not the case for computer-generate...","1997-04-17"
358,93250,"Deriving Production Rules for Constraint Maintenance","Jennifer Widom; Stefano Ceri;",". Traditionally, integrity constraints in database systems are maintained either by rolling back any transaction that produces an inconsistent state or by disallowing or modifying operations that may produce an inconsistent state. An alternative approach is to provide automatic ""repair "" of inconsistent states using production rules. For each constraint, a production rule is used to detect constraint violation and to initiate database operations that restore consistency. We describe an SQL-based language for defining integrity constraints and a framework for translating these constraints into constraint-maintaining production rules. Some parts of the translation are automatic while other parts require user intervention. Based on the semantics of our set-oriented production rules language and under certain assumptions, we prove that at the end of each transaction the rules are guaranteed to produce a state satisfying all defined constraints. We apply our approach to a good-sized example...","1999-02-05"
359,93428,"Logic in Computer Science: Modelling and Reasoning about Systems","Mark Ryan; Michael Huth;","ion. ACM Transactions on Programming Languages and Systems, 16(5):1512--1542, September 1994. Bibliography 401 [Che80] B. F. Chellas. Modal Logic -- an Introduction. Cambridge University Press, 1980. [Dam96] D. R. Dams. Abstract Interpretation and Partition Refinement for Model Checking. PhD thesis, Institute for Programming research and Algorithmics. Eindhoven University of Technology, July 1996. [Dij76] E. W. Dijkstra. A Discipline of Programming. Prentice Hall, 1976. [DP96] R. Davies and F. Pfenning. A Modal Analysis of Staged Computation. In 23rd Annual ACM Symposium on Principles of Programming Languages. ACM Press, January 1996. [EN94] R. Elmasri and S. B. Navathe. Fundamentals of Database Systems. Benjamin/Cummings, 1994. [FHMV95] Ronald Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi. Reasoning about Knowledge. MIT Press, Cambridge, 1995. [Fit93] M. Fitting. Basic modal logic. In D. Gabbay, C. Hogger, and J. Robinson, editors, Handbook of Logic in Artificial In...","1999-02-12"
360,93456,"Learning in Boltzmann Trees","Lawrence Saul; Michael Jordan;","We introduce a large family of Boltzmann machines that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results on the problems of N-bit parity and the detection of hidden symmetries. 1 Introduction Boltzmann machines (Ackley, Hinton, & Sejnowski, 1985) have several compelling virtues. Unlike simple perceptrons, they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines--- as originally conceived---also have some serious drawbacks...","1995-02-01"
361,93821,"A Configurable Automatic Instrumentation Tool for ANSI C","Clinton L. Jeffery; Kevin S. Templer;","Automatic software instrumentation is usually done at the machine level or is targeted at specific program behavior for use with a particular monitoring application. This paper describes CCI, an automatic software instrumentation tool for ANSI C designed to serve a broad range of program execution monitors. CCI supports high level instrumentation for both application-specific behavior as well as standard libraries and data types. The event generation mechanism is defined by the execution monitor which uses CCI, providing flexibility for different monitors' execution models. Code explosion and the runtime cost of instrumentation are reduced by declarative configuration facilities that allow the monitor to select specific events to be instrumented. Higher level events can be defined by combining lower level events with information obtained from semantic analysis of the instrumented program. 1 Introduction Program execution monitoring is an important tool for use in debugging, performanc...","1998-07-28"
362,94231,"Formulas For Calculating Supremal Controllable And Normal Sublanguages","F. Lin; R. D. Brandt; R. Kumar; S. I. Marcus; V. Garg;","Supremal controllable and normal sublanguages have been shown to play an important role in supervisor synthesis. In this paper, we discuss the computation of supremal controllable and normal sublanguages. We derive formulas for both supremal controllable sublanguages and supremal normal sublanguages when the languages involved are closed. As a result, those languages can be computed without applying recursive algorithms. We also discuss the computational aspects of these formulas. 1. INTRODUCTION In supervisory control, discrete event systems are modeled by controlled automata, and their behaviors described by the associated formal languages [1]- [3], [5]-[17] Control is exercised by a supervisor, whose action is to enable or disable events so that the controlled system generates some prespecified desired language. The subset of all events that can be selectively disabled by the supervisor is the set of controllable events. The supervisor may also be constrained to observe only even...","1996-12-13"
363,95136,"Compiling Modechart Specifications","Aloysius K. Mok; Carlos Puchol; Douglas A. Stuart;","The Modechart specification language is a formalism for the specification of real-time systems. A toolset for specification, analysis and simulation for Modechart specifications exists for supporting the design and construction of real-time systems [CHLR93]. This paper introduces a new tool in the the toolset: a compiler for a class of Modechart specifications, namely, that of deterministic system specifications, extended by a subclass of the nondeterministic system specifications. The object code that the compiler generates is in Esterel, a member of the synchronous family of programming languages for real-time systems. We discuss a broad approach to the implementation of timing specifications, providing a range of implementation options, from the basic time step unrolling of states in Esterel, to the use of system timers. The compiler presented herein allows the specifier to obtain a correct implementation of a modechart program, including timing constraints. 1 Introduction While de...","1995-10-04"
364,95200,"Goal-Directed Classification using Linear Machine Decision Trees","Bruce A. Draper; Carla E. Brodley; Paul E. Utgoff;","Recent work in feature-based classification has focused on non-parametric techniques that can classify instances even when the underlying feature distributions are unknown. The inference algorithms for training these techniques, however, are designed to maximize the accuracy of the classifier, with all errors weighted equally. In many applications, certain errors are far more costly than others, and the need arises for non-parametric classification techniques that can be trained to optimize task-specific cost functions. This paper reviews the Linear Machine Decision Tree (LMDT) algorithm for inducing multivariate decision trees, and shows how LMDT can be altered to induce decision trees that minimize arbitrary misclassification cost functions (MCFs). Demonstrations of pixel classification in outdoor scenes show how MCFs can optimize the performance of embedded classifiers within the context of larger image understanding systems. Keywords: Decision Trees, Non-Parametric Classification, ...","1995-02-01"
365,95649,"Implementing the Davis-Putnam Algorithm by Tries","Hantao Zhang; Mark E. Stickel;","The Davis-Putnam method is one of the major practical methods for the satisfiability (SAT) problem of propositional logic. We show how to implement the DavisPutnam method efficiently using the trie data structure for propositional clauses by presenting seven implementations of the method. We propose a new technique for implementing unit propagation whose complexity is sublinear to the number of occurrences of the variable in the input. We present the performance of our programs on some quasigroup problems. The efficiency of our programs allowed us to solve some open quasigroup problems. 1 Introduction In recent years, there has been considerable renewed interest in the satisfiability (SAT) problem of propositional logic. The SAT problem is known to be difficult to solve---it is the first known NP-complete problem. Because the SAT problem is fundamental to many practical problems in mathematics, computer science, and electrical engineering, efficient methods that can solve a large subs...","1998-11-17"
366,95737,"On the semantic foundations of Probabilistic VERUS","Christel Baier; Edmund M. Clarke; Vasiliki Hartonas-garmhausen;","In this paper we explain the semantic foundations of the tool Probabilistic VERUS [Har98]. A Probabilistic VERUS program P consists of sequential randomized processes S 1 ; : : : ; S k that are executed in parallel and that communicate via shared variables. First we give an operational semantics for the sequential components on the basis of a transition relation defined in the classical SOS-style `a la Plotkin [Plo81] which we use to specify the behaviour of the P by a Markov chain whose transitions stand for the cumulative effect of the activities of the components S 1 ; : : : S k within one time step. Second, we provide a denotational semantics for P that is based on a (denotational) least fixed point semantics for the sequential components which formalizes the input/output behaviour of the sequential components within one time step. While the operational (declarative) semantics might be the one that a designer (who provides the input for the tool) has in mind the denotational (pro...","1998-06-09"
367,95883,"A Compiler for Application-Specific Signal Processors","Ken Rimey; Paul N. Hilfinger;","We have built a compiler that generates code for horizontal-instruction-word, application-specific signal processors. The designer of such an application specific processor tunes the architecture using as feedback the compiled signal-processing code. Accordingly, the compiler is user-retargetable and designed for use with a plastic, irregular style of architecture. 1 Introduction Designers now have the option of implementing digital signal processing, control, and other dedicated systems as application-specific integrated circuits (ASIC's) to reduce unit cost. Moreover, to reduce design cost, ASIC's may incorporate programmable processors if the required sample rate is not too high. The architecture of such an application-specific processor can be tuned for the program that it will run. An application-specific processor should be just fast enough for the application, as small as possible, and tunable. Reducing the processor size frees space for integrating additional peripheral circui...","1998-02-08"
368,96539,"Genetic Algorithms as a Tool for Feature Selection in Machine Learning","Haleh Vafaie; Kenneth De Jong;","This paper describes an approach being explored to improve the usefulness of machine learning techniques for generating classification rules for complex, real world data. The approach involves the use of genetic algorithms as a ""front end"" to traditional rule induction systems in order to identify and select the best subset of features to be used by the rule induction system. This approach has been implemented and tested on difficult texture classification problems. The results are encouraging and indicate significant advantages to the presented approach in this domain. 1.0 Introduction In recent years there has been a significant increase in research on automatic image recognition in more realistic contexts involving noise, changing lighting conditions, and shifting viewpoints. The corresponding increase in difficulty in designing effective classification procedures for the important components of these more complex recognition problems has led to an interest in machine techniques a...","1997-10-01"
369,96767,"Model-Checking in Dense Real-time","Costas Courcoubetis; David Dill; Rajeev Alur;",". Model-checking is a method of verifying concurrent systems in which a state-transition graph model of the system behavior is compared with a temporal logic formula. This paper extends model-checking for the branching-time logic CTL to the analysis of real-time systems, whose correctness depends on the magnitudes of the timing delays. For specifications, we extend the syntax of CTL to allow quantitative temporal operators such as 93!5 , meaning ""possibly within 5 time units."" The formulas of the resulting logic, Timed CTL (TCTL), are interpreted over continuous computation trees , trees in which paths are maps from the set of nonnegative reals to system states. To model finite-state systems we introduce timed graphs --- state-transition graphs annotated with timing constraints. As our main result, we develop an algorithm for model-checking, for determining the truth of a TCTL-formula with respect to a timed graph. We argue that choosing a dense domain instead of a discrete domain to m...","1997-06-17"
370,97060,"An Empirical Evaluation of Three Methods for Deadlock Analysis of Ada Tasking Programs","James C. Corbett;","Static analysis of Ada tasking programs has been hindered by the well known state explosion problem that arises in the verification of concurrent systems. Many different techniques have been proposed to combat this state explosion. All proposed methods excel on certain kinds of systems, but there is little empirical data comparing the performance of the methods. In this paper, we select one representative from each of three very different approaches to the state explosion problem: partial-orders (representing state-space reductions), symbolic model checking (representing OBDD-based approaches), and inequality necessary conditions (representing integer programming-based approaches). We apply the methods to several scalable concurrency examples from the literature and to one real Ada tasking program. The results of these experiments are presented and their significance is discussed. 1 Introduction Ada tasks arm software developers with the power, and dangers, of concurrency. With this p...","1994-09-09"
371,97150,"Binary Decision Diagrams on Network of Workstations","Alberto Sangiovanni-vincentelli; Jagesh V. Sanghavi; Rajeev K. Ranjan; Robert K. Brayton;","The success of all binary decision diagram (BDD) based synthesis and verification algorithms depend on the ability to efficiently manipulate very large BDDs. We present algorithms for manipulation of very large Binary Decision Diagrams (BDDs) on a network of workstations (NOW). ANOW provides a collection of main memories and disks which can be used effectively to create and manipulate very large BDDs. To make efficient use of memory resources of a NOW, while completing execution in a reasonable amount of wall clock time, extension of breadth-first technique is used to manipulate BDDs. BDDs are partitioned such that nodes for a set of consecutive variables are assigned to the same workstation. We present experimental results to demonstrate the capability of such an approach and point towards the potential impact for manipulating very large BDDs. 1 Introduction The manipulation of boolean functions is one of the most important operations in several areas of computer-aided design such a...","1996-06-01"
372,97410,"The Argos Language: Graphical Representation of Automata and Description of Reactive Systems","F. Maraninchi;","We present the Argos graphical synchronous language for the description of reactive systems, and the Argonaute environment associated with it. Systems like communication protocols, real time process controllers or man/machine interfaces contain a reactive kernel. Its behaviour can be described in a convenient manner by an automaton , for formal validation purposes. But, in general, complex systems cannot be described directly as automata. The Statecharts [4,5] and Argos [7,8] are automata-based languages. The high level constructs of the language deal with states and transitions directly. A consequence of this choice is the graphical syntax, since the best representation of small automata is graphical. A consequence of this graphical syntax is the need for graphical constructs: the constructs of the language must allow the decomposition of a system into small parts that can be represented directly by automata, and they must be given a readable graphical syntax. 1 Introduction The term ...","1992-07-24"
373,97863,"Implicit State Enumeration for FSMs with Datapaths","Adnan Aziz; James H. Kukula; Thomas R. Shiple;","We show how the classic BDD-based technique of implicit state enumeration for FSMs can be generalized to an automata-based approach for implicit state enumeration of FSMs interacting with datapaths of unbounded width. We present experimental results showing that our automata representation of an unbounded width datapath can be 10x more compact than the BDD representation of the corresponding 32-bit datapath. 1 Introduction The BDD-based approach to functional verification of finite state systems has enjoyed some success. This approach proceeds by first building BDDs for the next state functions and then using implicit state enumeration to explore the state transition graph [12]. For systems with up to a few hundred flip-flops this technique can be vastly more efficient than techniques that manipulate states explicitly. However, this approach usually fails for systems with wide numeric datapaths because the BDDs become too large. We focus on systems whose datapaths consist of addition ...","1998-08-27"
374,98185,"Minimizable Timed Automata","Frits Va; Jan Springintveld;",". State minimization plays a fundamental role in both classical automata theory and in the theory of reactive systems. Many algorithms and results are based on the fact that for each finite automaton there exists an equivalent minimum state automaton that can be effectively computed and that is unique up to isomorphism. Timed safety automata (TSA's) [5], finite automata with clocks, have been used extensively for the specification and verification of real-time systems. However, there does not always exist a unique minimum state TSA that is equivalent to a given TSA. This problem occurs irrespective of the selected notions of state (including or excluding clock values) and equivalence on states (language equivalence, bisimulation equivalence, etc.). Henzinger, Kopke and Wong-Toi [4] convincingly showed that if states do not include clock values, state minimization for timed automata is neither useful nor interesting. In this paper, we discuss state minimization for states that do includ...","1996-07-03"
375,99113,"A Formal Verification Environment for Railway Signaling System Design","Ansaldo Trasporti Genova Napoli;",". A fundamental problem in the design and development of embedded control systems is the verification of safety requirements. Formal methods, offering a mathematical way to specify and analyze the behavior of a system, together with the related support tools can successfully be applied in the formal proof that a system is safe. However, the complexity of real systems is such that automated tools often fail to formally validate such systems. This paper outlines an experience on formal specification and verification carried out in a pilot project aiming at the validation of a railway computer based interlocking system. Both the specification and the verification phases were carried out in the JACK (Just Another Concurrency Kit) integrated environment. The formal specification of the system was done by means of process algebra terms. The formal verification of the safety requirements was done first by giving a logical specification of such safety requirements, and then by means of model c...","1998-04-01"
376,100299,"A Parallel Bottom-up Clustering Algorithm with Applications to Circuit Partitioning in VLSI Design","Jason Cong;","In this paper, we present a bottom-up clustering algorithm based on recursive collapsing of small cliques in a graph. The sizes of the small cliques are derived using random graph theory. This clustering algorithm leads to a natural parallel implementation in which multiple processors are used to identify clusters simultaneously. We also present a cluster-based partitioning method in which our clustering algorithm is used as a preprocessing step to both the bisection algorithm by Fiduccia and Mattheyses and a ratio-cut algorithm by Wei and Cheng. Our results show that cluster-based partitioning obtains cut sizes up to 49.6% smaller than the bisection algorithm, and obtains ratio cut sizes up to 66.8% smaller than the ratio-cut algorithm. Moreover, we show that cluster-based partitioning produces much stabler results than direct partitioning. 1 Introduction 1.1 Motivation A cluster is a group of strongly connected components in a circuit. The goal of clustering algorithms is to ident...","1995-07-09"
377,100967,"Design of heterogeneous ICs for mobile and personal communication systems","Bill Lin; Francky Catthoor; Gert Goossens; Ivo Bolsens;","Mobile and personal communication systems form key market areas for the electronics industry of the nineties. Stringent requirements in terms of flexibility, performance and power dissipation, are driving the development of integrated circuits into the direction of heterogeneous single-chip solutions. New IC architectures are emerging which contain the core of a powerful programmable processor, complemented with dedicated hardware, memory and interface structures. In this tutorial we will discuss the real-life design of a heterogeneous IC for an industrial telecom application : a reconfigurable mobile terminal for satellite communication. Based on this practical design experience, we will subsequently discuss a methodology for the design of heterogeneous ICs. Design steps that will be addressed include : system specification and refinement, data path and communication synthesis, and code generation for embedded processor cores. 1 Introduction The mobile and personal communication sys...","1994-08-12"
378,101705,"Synthesis for Testability Techniques for Asynchronous Circuits","Alberto Sangiovanni-vincentelli; Kurt Keutzer; Luciano Lavagno;","Our goal is to synthesize hazard-free asynchronous circuits that are testable in the very stringent hazard-free robust path-delay-fault model. From a synthesis perspective producing circuits satisfying two very stringent requirements, namely, hazard-free operation and hazard-free robust path-delay-fault-testability, poses an especially exciting challenge. In this paper we present techniques which guarantee both hazard-free operation and hazard-free robust path-delay-fault testability, at the expense of possibly adding test inputs, and give a set of heuristics which can improve hazard-free robust path-delay-fault testability without requiring such inputs. We also present a procedure that guarantees testability in the less stringent robust gate-delay-fault model. 1 Introduction In this paper we are concerned with the problem of synthesizing asynchronous sequential circuits from a high level specification, the Signal Transition Graph (STG, [3]). In [13, 12] we presented a set of algorith...","1995-01-26"
379,101863,"On the Synthesis of Strategies in Infinite Games","Wolfgang Thomas;",". Infinite two-person games are a natural framework for the study of reactive nonterminating programs. The effective construction of winning strategies in such games is an approach to the synthesis of reactive programs. We describe the automata theoretic setting of infinite games (given by ""game graphs""), outline a new construction of winning strategies in finite-state games, and formulate some questions which arise for games over effectively presented infinite graphs. 1 Introduction One of the origins of automata theory over infinite strings was the interest in verifying and synthesizing switching circuits. These circuits were considered as transforming infinite input sequences into output sequences, and systems of restricted arithmetic served as specification formalisms ([Ch63]). With Buchi's decision procedure for the monadic second-order theory S1S of one successor ([Bu62]), it turned out that the ""solution problem"" (in more recent terminology: the verification problem or model ch...","1997-08-18"
380,102458,"Formal Hardware Verification with BDDs: An Introduction","null","This paper is a brief introduction to the main paradigms for using BDDs in formal hardware verification. The paper addresses two audiences: for people doing theoretical BDD research, the paper gives a glimpse of the problems in the main application area, and for people building hardware, the paper gives a peek under the hood of the formal verification technologies that are rapidly gaining industrial importance. Topics described include combinational equivalence checking, symbolic simulation, sequential equivalence checking, model checking, and symbolic trajectory evaluation.","1997-07-12"
381,102886,"Using Decision Trees to Improve Case-Based Learning","Claire Cardie;","This paper shows that decision trees can be used to improve the performance of casebased learning (CBL) systems. We introduce a performance task for machine learning systems called semi-flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems. In semi-flexible prediction, learning should improve prediction of a specific set of features known a priori rather than a single known feature (as in classification) or an arbitrary set of features (as in conceptual clustering). We describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees, CBL, and a hybrid approach that combines the two. In the hybrid approach, decision trees are used to specify the features to be included in k-nearest neighbor case retrieval. Results from the experiments show that the hybrid approach outperforms both the decision ...","1997-10-03"
382,102966,"Decision Tree Induction: How Effective is the Greedy Heuristic?","Sreerama Murthy; Steven Salzberg;","Most existing decision tree systems use a greedy approach to induce trees --- locally optimal splits are induced at every node of the tree. Although the greedy approach is suboptimal, it is believed to produce reasonably good trees. In the current work, we attempt to verify this belief. We quantify the goodness of greedy tree induction empirically, using the popular decision tree algorithms, C4.5 and CART. We induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees, which in turn are found using a novel map coloring idea. We measure the effect on greedy induction of variables such as the underlying concept complexity, training set size, noise and dimensionality. Our experiments show, among other things, that the expected classification cost of a greedily induced tree is consistently very close to that of the optimal tree. Introduction Decision trees are known to be effective classifiers in a variety of domains. Most of the methods ...","1995-05-10"
383,103700,"Verifying SCR Requirements Specifications Using State Exploration","Constance Heitmeyer; Ramesh Bharadwaj;","Researchers at the Naval Research Laboratory (NRL) have been developing a formal method, known as the SCR (Software Cost Reduction) method, to specify the requirements of software systems using tables. NRL has developed a formal state machine model defining the SCR semantics and support tools for analysis and validation. Recently, a verification capability was added to the SCR toolset. Users can now invoke the Spin model checker within the toolset to establish properties of a specification. This paper describes the results of our initial experiments to verify properties of SCR requirements specifications using Spin. After reviewing the SCR requirements method and introducing our formal requirements model, we describe how SCR specifications can be translated into an imperative programming notation. We also describe how we limit state explosion by verifying abstractions of the original requirements specification. These abstractions are derived using the formula to be verified and special...","1997-02-21"
384,103914,"Formal Methods and the Certification of Critical Systems","John Rushby;","This report was prepared to supplement a forthcoming chapter on formal methods in the FAA Digital Systems Validation Handbook 1 . Its purpose is to outline the technical basis for formal methods in computer science, to explain the use of formal methods in the specification and verification of software and hardware requirements, designs, and implementations, to identify the benefits, weaknesses, and difficulties in applying these methods to digital systems used in critical applications, and to suggest factors for consideration when formal methods are offered in support of certification. The report assumes a serious interest in the engineering of critical systems, and a willingness to read occasional mathematical formulas and specialized terminology, but assumes no special background in formal logic or mathematical specification techniques. (An appendix provides a rapid introduction to formal logic for those to whom this topic is new.) The discussion should be accessible to most people...","1998-04-17"
385,104047,"Model Checking Safety Critical Software with SPIN: an Application to a Railway Interlocking System.","Alessandro Cimatti; Ansaldo Segnalamento Ferroviario (asf; Ansaldo Trasporti (atr; Dario Romano; Fausto Giunchiglia; Fernando Torielli; Giorgio Mongardi; Paolo Traverso;","This paper reports on an experience in formal verification using spin. The analyzed system is the Safety Logic of an interlocking system for the control of railway stations developed by Ansaldo. The Safety Logic is a process-based software architecture, which can be configured to implement different functions and control stations of different topology. In this paper we describe how a promela model has been devised, which retains the configurability features of this architecture. Furthermore, we discuss the verification with spin of significant process configurations. 1 Introduction This paper describes a joint project between Ansaldo and IRST. The goal of the project was the evaluation of the possibility to integrate formal methods technology within the development cycle of a safety critical application. Particularly relevant was the assessment of formal methods techniques as an advanced debugging tool for the design. 2 The project focuses on a complex real-world safety critical app...","1997-04-08"
386,104814,"A Stochastic Process Algebra Based Modelling Tool","Holger Hermanns; Vassilis Mertsiotakis;","The incorporation of time into classical process algebras aiming at the integration of functional design and performance analysis has become very popular recently. There are many ways to include time in process algebras. Current research in this area concentrates mainly on the annotation of actions with exponentially distributed random variables. This allows us to make use of a large repertoire of analysis algorithms. This paper presents some first results of ongoing work which aims at providing a tool for the efficient performance evaluation and functional analysis of computer and communication systems based on the stochastic process algebra paradigm. It provides facilities for model creation, reachability analysis, as well as several numerical algorithms for the solution of the underlying Markov chain and the computation of characteristic performance measures. 1 Introduction Stochastic process algebras (SPA) have been introduced as an extension of classical process algebras, like CCS ...","1995-07-26"
387,104824,"Reduction and Slicing of Hierarchical State Machines","Mats P. E. Heimdahl; Michael W. Whalen;",". Formal specification languages are often criticized for being difficult to understand, difficult to use, and unacceptable by software practitioners. Notations based on state machines, such as, Statecharts, Requirements State Machine Language (RSML), and SCR, are suitable for modeling of embedded systems and eliminate many of the main drawbacks of formal specification languages. Although a specification language can help eliminate accidental complexity, the inherent complexity of many of today's systems inevitably leads to large and complex specifications. Thus, there is a need for mechanisms to simplify a formal specification and present information to analysts and reviewers in digestible chunks. In this paper, we present a two tiered approach to slicing (or simplification) of hierarchical finite state machines. We allow an analyst to simplify a specification based on a scenario. The remaining behavior, called an interpretation of the specification, can then be sliced to extract the ...","1997-07-24"
388,105046,"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection","Ron Kohavi;","We review accuracy estimation methods and compare the two most common methods: cross validation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leave one -out cross-validation. We report on a largescale experiment---over half a million runs of C4.5 and a Naive-Bayes algorithm---to estimate the effects of different parameters on these algorithms on real-world datasets. For cross validation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 1 Introduction It can not be emphasized enough that no claim ...","1998-08-10"
389,105095,"Towards Constraint Satisfaction through Logic Programs and the Stable Model Semantics","Patrik Simons;","Logic programs with the stable model semantics can be thought of as a new paradigm for constraint satisfaction, where the rules of a program are seen as constraints on the stable models. In this work the paradigm is realized by developing an efficient procedure for computing the stable models of ground logic programs. A strong pruning technique based on two deductive closures is introduced. The technique is further strengthened by the introduction of backjumping, which is an improvement over chronological backtracking, and lookahead, a new pruning method. Moreover, a strong heuristic is derived. The two deductive closures are given linear-time implementations that provide a linear-space implementation method for the decision procedure. A high lower bound on the least upper bound on the complexity of the procedure is found. In order to generalize the procedure such that it can handle programs with variables, an algorithm for grounding a function-free range restricted logic program that...","1997-08-29"
390,105173,"Using Abstractions for Decision-Theoretic Planning with Time Constraints","Craig Boutilier; Richard Dearden;","Recently Markov decision processes and optimal control policies have been applied to the problem of decision-theoretic planning. However, the classical methods for generating optimal policies are highly intractable, requiring explicit enumeration of large state spaces. We explore a method for generating abstractions that allow approximately optimal policies to be constructed","1994-05-31"
391,105199,"A Linear-Time Model-Checking Algorithm for the Alternation-Free Modal Mu-Calculus","Bernhard Steffen; Rance Cleaveland;","We develop a model-checking algorithm for a logic that permits propositions to be defined using greatest and least fixed points of mutually recursive systems of equations. This logic is as expressive as the alternation-free fragment of the modal mu-calculus identified by Emerson and Lei, and it may therefore be used to encode a number of temporal logics and behavioral preorders. Our algorithm determines whether a process satisfies a formula in time proportional to the product of the sizes of the process and the formula; this improves on the best known algorithm for similar fixed-point logics. 1 Introduction Behavioral equivalences and preorders, and temporal logics, have been used extensively in automated verification tools for finite-state processes [3, 12, 18, 19, 20]. The relations are typically used to relate a high-level specification process to a more detailed implementation process, while the logics enable system designers to formulate collections of properties that implementa...","1995-05-01"
392,105376,"Using Compositional Preorders in the Verification of Sliding Window Protocol","Roope Kaivola;","The main obstacle to automatic verification of temporal logic properties of finite-state systems is the state explosion problem. One way to alleviate this is to replace components of a system with smaller ones and verify the required properties from the smaller system. This approach leads to notions of compositional property-preserving equivalences and preorders. Previously we have shown that the NDFD preorder is the weakest preorder which is compositional w.r.t. standard operators and preserves nexttime-less linear temporal logic properties. In this paper we describe a case study where NDFD preorder was used to verify semiautomatically both safety and liveness properties of the Sliding Window protocol for arbitrary channel lengths and realistic parameter values. In this process we located a previously undiscovered fault leading to lack of liveness in a version of the protocol. 1 Introduction A promising approach to verification of finite-state concurrent systems is the use of proposi...","1997-12-05"
393,105655,"Experience with Predicate Abstraction","David L. Dill; Satyaki Das;","ion ? Satyaki Das 1 , David L. Dill 1 , and Seungjoon Park 2 1 Computer Systems Laboratory, Stanford University, Stanford, CA 94305 2 RIACS, NASA Ames Research Center, Moffett Field, CA 94035 Abstract. This reports some experiences with a recently-implemented prototype system for verification using predicate abstraction, based on the method of Graf and Saidi [9]. Systems are described using a language of iterated guarded commands, called MurOE GammaGamma (since it is a simplified version of our MurOE protocol description language). The system makes use of two libraries: SVC [1] (an efficient decision procedure for quantifier free first-order logic) and the CMU BDD library. The use of these libraries increases the scope of problems that can be handled by predicate abstraction through increased efficiency, especially in SVC, which is typically called thousands of times. The verification system also provides limited support for quantifiers in formulas. The system has been appl...","1999-03-26"
394,105780,"BDD extensions for stochastic transition systems","Markus Siegle;",": A BDD (Binary Decision Diagram) is a compact canonical representation of a Boolean function. While BDDs are well-established in the area of functional system verification, their use for the purpose of performance analysis is a new idea. We use BDDs to represent labelled transition systems which arise from higher-level model specifications such as stochastic process algebras or structured stochastic Petri nets. BDDs offer a compact representation of transition systems with very large state space. They are therefore promising candidates for alleviating the problem of state space explosion. However, as a survey of the relevant literature shows, the question of how to code stochastic information in a BDD context had not yet been answered satisfactorily. We offer a new solution to this problem, concentrating on the Markovian case. A new data structure, Decision node BDD (DNBDD), is introduced and used to represent stochastic transition systems. It is shown that DNBDD have important advanta...","1997-09-10"
395,105842,"Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction","Michael J. Pazzani; Patrick M. Murphy;","We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees. 1. Introduction The top-down induction of decision trees is an approach to machine learning that has been used on a variety of real world tasks. Decision trees are well-suited for such tasks since they scale fairly well with th...","1994-07-06"
396,105962,"A Scheme for Real-Time Channel Establishment in Wide-Area Networks","Dinesh C. Verma; Domenico Ferrari;","Multimedia communication involving digital audio and/or digital video has rather strict delay requirements. A real-time channel is defined in this paper as a simplex connection between a source and a destination characterized by parameters representing the performance requirements of the client. A real-time service is capable of creating realtime channels on demand and guaranteeing their performance. These guarantees often take the form of lower bounds on the bandwidth allocated to a channel and upper bounds on the delays to be experienced by a packet on the channel. In this paper, we study the feasibility of providing real-time services on a packet switched store-and-forward wide-area network with general topology. We describe a scheme for the establishment of channels with deterministic or statistical delay bounds, and present the results of the simulation experiments we ran to evaluate it. The results are encouraging: our approach satisfies the guarantees even in worst-case situation...","1990-11-01"
397,106034,"HYTECH: The Next Generation","Howard Wong-toi; Pei-hsin Ho; Thomas A. Henzinger;",". We describe a new implementation of HyTech 1 , a symbolic model checker for hybrid systems. Given a parametric description of an embedded system as a collection of communicating automata, HyTech automatically computes the conditions on the parameters under which the system satisfies its safety and timing requirements. While the original HyTech prototype was based on the symbolic algebra tool Mathematica, the new implementation is written in C ++ and builds on geometric algorithms instead of formula manipulation. The new HyTech offers a cleaner and more expressive input language, greater portability, superior performance (typically two to three orders of magnitude), and new features such as diagnostic error-trace generation. We illustrate the effectiveness of the new implementation by applying HyTech to the automatic parametric analysis of the generic railroad crossing benchmark problem [HJL93] and to an active structure control algorithm [ECB94]. 1 Introduction There has been in...","1996-06-24"
398,106259,"Efficient BDD Algorithms for FSM Synthesis and Verification","Adnan Aziz; Bernard Plessier; Carl Pixley; Rajeev K. Ranjan; Robert K. Brayton;","We describe a set of BDD based algorithms for efficient FSM synthesis and verification. We establish that the core computation in both synthesis and verification is forming the image and pre-image of sets of states under the transition relation characterizing the design. To make these steps as efficient as possible, we address BDD variable ordering, use of partitioned transition relations, and use of clustering. We provide an integrated set of algorithms and give references and comparisons with previous work. We report experimental results on a series of seven industrial examples containing from 28 to 172 binary valued latches. 1 Introduction The advent of modern VLSI CAD tools has radically changed the process of designing digital systems. The first CAD tools automated the final stages of design, such as placement and routing. As the low level steps became better understood, the focus shifted to the higher stages. In particular logic synthesis, the science of optimizing designs (for ...","1995-04-21"
399,108055,"Representation Design and Brute-force Induction in a Boeing Manufacturing Domain","Oren Etzioni; Patricia Riddle; Richard Segal;","We applied inductive classification techniques to data collected in a Boeing plant with the goal of uncovering possible flaws in the manufacturing process. This application led us to explore two aspects of classical decision-tree induction: (1) Preprocessing and postprocessing and (2) brute-force induction. For preprocessing and postprocessing, much of our effort was focused on the pre-processing of raw data to make it suitable for induction and the post-processing of learned rules to make them useful to factory personnel. For brute-force induction, in contrast with standard methods, which perform a greedy search of the space of decision trees, we formulated an algorithm that conducts an exhaustive, depth-bounded search for accurate predictive rules. We demonstrate the efficacy of our approach with specific examples of learned rules and by quantitative comparisons with decision-tree algorithms (C4 and CART). 3 This research was funded in part by a Boeing Computer Services contract wi...","1998-01-07"
400,108101,"SMART: Simulation and Markovian Analyzer for Reliability and Timing","Andrew S. Miner; Gianfranco Ciardo;","SMART is a new tool designed to allow various high-level stochastic modeling formalisms (such as stochastic Petri nets and queueing networks) to be described in a uniform environment and solved using a variety of solution techniques, including numerical methods and simulation. Since SMART is intended as a research tool, it is written in a modular way that permits the easy integration of new solution algorithms. I. SMART Language Models are described to SMART using a strongly-typed, declarative language. The three basic predefined types for the objects defined in SMART are: ffl bool: true or false. ffl int: integer values. ffl real: real values (machine-dependent precision). Composite types can be defined using the concepts of: ffl sets: collection of homogeneous objects. ffl arrays: multidimensional data structures of homogeneous objects indexed by the elements of a set. ffl aggregates: analogous to the Pascal ""record"". A type can be further modified by the following natures, w...","1998-11-29"
401,108173,"Robust Linear Programming Discrimination Of Two Linearly Inseparable Sets","Kristin P. Bennett; O. L. Mangasarian;","INTRODUCTION We consider the two point-sets A and B in the n-dimensional real space R n represented by the m Theta n matrix A and the k Theta n matrix B respectively. Our principal objective here is to formulate a single linear program with the following properties: (i) If the convex hulls of A and B are disjoint, a strictly separating plane is obtained. (ii) If the convex hulls of A and B intersect, a plane is obtained that minimizes some measure of misclassification points, for all possible cases. (iii) No extraneous constraints are imposed on the linear program that rule out any specific case from consideration. Most linear programming formulations 6,5,12,4 have property (i)","1999-01-19"
402,108202,"An Approach to Automatic Detection of Software Failures in Real-Time Systems","R. E. Seviora; T. Savor;","Software supervision is an approach to automatic detection of software failures. A software supervisor observes the inputs and outputs of a target system. It uses a model of correct behavior, derived from the target system's requirements specification. Discrepancies between specified and observed behaviors are reported as failures. The tradeoff between the computational complexity of supervision and the latency of failure reporting is discussed in this paper. Supervisor computational complexity can be significantly reduced at the expense of increased failure reporting latency. For applications such as software testing, this is a practical tradeoff. Such a supervisor is called an out-of-time supervisor. This paper describes the data flows, algorithms, operation and evaluation of an out-of-time supervisor for communicating finite state machine based requirements specifications. A prototype supervisor was used to monitor the operation a small telephone exchange control program. For a fail...","1997-06-13"
403,108813,"Cooperative Explanation in Deductive Databases","Terry Gaasterland;","this paper, we will discuss two mechanisms to augment an initial logic formula which has been produced by the basic cooperative answering system of Gal. The first mechanism ensures that each piece of cooperative information in the formula is properly motivated. If cooperative information is not linked to the original query in some way, it can be surprising to the user. Further information which links the cooperative information to the user's query can be found in the search tree and incorporated into the logic formula. The second mechanism lets the user guide the presentation of the response. The user may only be interested in parts of the collected cooperative information; after seeing part of the extra information, the user may want to see more. A mechanism to ""query"" the cooperative logic formula allows it to be presented as the user desires. Finally, we briefly mention some of the transformations that we have developed to organize the logic formula and prepare it for natural language generation. In addition, we provide a chart that details the sources and uses of cooperative information available in a deductive database system. 2 Definitions","1998-10-22"
404,108844,"On the Design of a Low-Cost Video-on-Demand Storage System","Avi Silberschatz; Rajeev Rastogi;","Recent advances in storage technology, coupled with the dramatic increase in the bandwidth of networks, make it now possible to provide ""video on demand"" service to viewers. A video on demand server is a computer system that stores videos in compressed digital form and provides support for different portions of compressed video data to be accessed and transmitted concurrently. In this paper, we present a low-cost storage architecture for a video on demand server that relies principally on disks. The high bandwidths of disks in conjunction with a clever strategy for striping videos on them is utilized in order to enable simultaneous access and transmission of different portions of a video, separated by fixed time intervals. We also present a wide range of schemes for implementing VCR-like functions including fast-forward, rewind and pause. Finally, we extend our schemes to the case when videos have different rate requirements. 1 Introduction The video on demand (VOD) concept has beco...","1996-08-06"
405,109288,"Supervised and Unsupervised Discretization of Continuous Features","James Dougherty; Mehran Sahami; Ron Kohavi;","Many supervised machine learning algorithms require a discrete feature space. In this paper, we review previous work on continuous feature discretization, identify defining characteristics of the methods, and conduct an empirical evaluation of several methods. We compare binning, an unsupervised discretization method, to entropy-based and purity-based methods, which are supervised algorithms. We found that the performance of the Naive-Bayes algorithm significantly improved when features were discretized using an entropy-based method. In fact, over the 16 tested datasets, the discretized version of Naive-Bayes slightly outperformed C4.5 on average. We also show that in some cases, the performance of the C4.5 induction algorithm significantly improved if features were discretized in advance; in our experiments, the performance never significantly degraded, an interesting phenomenon considering the fact that C4.5 is capable of locally discretizing features. 1 Introduction Many algorithm...","1998-09-21"
406,109578,"Where the REALLY Hard Problems Are","Bob Kanefsky; Peter Cheeseman; William M. Taylor;","It is well known that for many NP-complete problems, such as K-Sat, etc., typical cases are easy to solve; so that computationally hard cases must be rare (assuming P 6= NP ). This paper shows that NP-complete problems can be summarized by at least one ""order parameter "", and that the hard problems occur at a critical value of such a parameter. This critical value separates two regions of characteristically different properties. For example, for K-colorability, the critical value separates overconstrained from underconstrained random graphs, and it marks the value at which the probability of a solution changes abruptly from near 0 to near 1. It is the high density of well separated almost solutions (local minima) at this boundary that cause search algorithms to ""thrash"". This boundary is a type of phase transition and we show that it is preserved under mappings between problems. We show that for some P problems either there is no phase transition or it occurs for bounded N (and so bound...","1995-08-15"
407,109733,"An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators","Andrew W. Moore; Daniel J. Hill; Michael P. Johnson;","The generalization error of a function approximator, feature set or smoother can be estimated directly by the leave-one-out cross-validation error. For memory-based methods, this is computationally feasible. We describe an initial version of a general memory-based learning system (GMBL): a large collection of learners brought into a widely applicable machine-learning family. We present ongoing investigations into search algorithms which, given a dataset, find the family members and features that generalize best. We also describe GMBL's application to two noisy, difficult problems---predicting car engine emissions from pressure waves, and controlling a robot billiards player with redundant state variables. 1 Introduction The main engineering benefit of machine learning is its application to autonomous systems in which human decision making is minimized. Function approximation plays a large and successful role in this process. However, many other human decisions are needed even for si...","1994-03-31"
408,109872,"Applications of Machine Learning and Rule Induction","Herbert A. Simon; Pat Langley;","An important area of application for machine learning is in automating the acquisition of knowledge bases required for expert systems. In this paper, we review the major paradigms for machine learning, including neural networks, instance-based methods, genetic learning, rule induction, and analytic approaches. We consider rule induction in greater detail and review some of its recent applications, in each case stating the problem, how rule induction was used, and the status of the resulting expert system. In closing, we identify the main stages in fielding an applied learning system and draw some lessons from successful applications. Introduction Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide ...","1997-04-13"
409,109902,"Solving Polynomial Systems Using a Branch and Prune Approach","D. Kapur; D. Mcallester; P. Van Hentenryck;","This paper presents Newton, a branch & prune algorithm to find all isolated solutions of a system of polynomial constraints. Newton can be characterized as a global search method which uses intervals for numerical correctness and for pruning the search space early. The pruning in Newton consists in enforcing at each node of the search tree a unique local consistency condition, called box-consistency, which approximates the notion of arc-consistency well-known in artificial intelligence. Box-consistency is parametrized by an interval extension of the constraint and can be instantiated to produce Hansen-Segupta narrowing operator (used in interval methods) as well as new operators which are more effective when the computation is far from a solution. Newton has been evaluated on a variety of benchmarks from kinematics, chemistry, combustion, economics, and mechanics. On these benchmarks, it outperforms the interval methods we are aware of and compares well with state-of-the-art continuati...","1996-09-04"
410,110159,"Domain Theory","Achim Jung; Samson Abramsky;","bases were introduced in [Smyth, 1977] where they are called ""R-structures"". Examples of abstract bases are concrete bases of continuous domains, of course, where the relation OE is the restriction of the order of approximation. Axiom (INT) is satisfied because of Lemma 2.2.15 and because we have required bases in domains to have directed sets of approximants for each element. Other examples are partially ordered sets, where (INT) is satisfied because of reflexivity. We will shortly identify posets as being exactly the bases of compact elements of algebraic domains. In what follows we will use the terminology developed at the beginning of this chapter, even though the relation OE on an abstract basis need neither be reflexive nor antisymmetric. This is convenient but in some instances looks more innocent than it is. An ideal A in a basis, for example, has the property (following from directedness) that for every x 2 A there is another element y 2 A with x OE y. In posets this doesn't ...","1995-09-27"
411,110303,"Temporal Proof Methodologies for Real-time Systems","Amir Pnueli; Thomas A. Henzinger; Zohar Manna;",". We extend the specification language of temporal logic, the corresponding verification framework, and the underlying computational model to deal with real-time properties of concurrent and reactive systems. A global, discrete, and asynchronous clock is incorporated into the model by defining the abstract notion of a real-time transition system as a conservative extension of traditional transition systems: qualitative fairness requirements are replaced (and superseded) by quantitative lower-bound and upperbound real-time requirements for transitions. We show how to model real-time systems that communicate either through shared variables or by message passing, and how to represent the important real-time constructs of priorities (interrupts), scheduling, and timeouts in this framework. Two styles for the specification of real-time properties are presented. The first style uses bounded versions of the temporal operators; the real-time requirements expressed in this style are classified ...","1999-02-05"
412,110648,"Constraint Management in a Declarative Design Method for 3D Scene Sketch Modeling","Rue Marcel Sembat;","In this paper, we present a dynamic model associated with an intelligent CAD system aiming at the modeling of an architectural scene sketch. Our design methodology has been developed to simulate the process of a user who tries to give a description of a scene from a set of mental images. The scene creation is based on a script which describes the environment from the point of view of an observer who moves across the scene. The system is based on a declarative method viewed as a stepwise refinement process. For the scene representation, a qualitative model is used to describe the objects in terms of attributes, functions, methods and components. The links between objects and their components are expressed by a hierarchical structure, and a description of spatial configurations is given by using locative relations. The set of solutions consistent with the description is usually infinite. So, either one scene consistent with this description is calculated and visualized, or reasons of inc...","1993-05-10"
413,110731,"Tecton: A Framework for Specifying and Verifying Generic System Components","David R. Musser; Deepak Kapur;","This paper presents the syntax and semantics of a small language for describing and using abstract concepts in formal software development and hardware design. The language provides definition, abbreviation, extension, and lemma constructs, which have general mathematical descriptive power, plus a computation-specific realization construct. The semantics, which is denotational, includes specification of the requirements (""legality conditions "") that must be met when using each construct. The syntax and semantics are such that a corresponding proof theory requires only first order and inductive proof methods, rather than general higher order techniques as required in some frameworks. The language and some of the main proof issues are illustrated with an extended example of a behavioral and structural description of a carry-lookahead adder circuit, with the circuit realization given in terms of a generic parallel-prefix circuit. Partially supported by NSF Grant Number CCR-8906678. A pr...","1993-01-05"
414,110951,"Automatic Generation of DAG Parallelism","Michael Hind; Ron Cytron; Wilson Hsieh;","This paper extends the notion of shared and private","1996-11-30"
415,111418,"Global Rebuilding of OBDDs - Avoiding Memory Requirement Maxima","Christoph Meinel; Jochen Bern;","It is well-known that the size of an ordered binary decision diagram (OBDD) may depend crucially on the order in which the variables occur. In the paper, we describe an implementation of an output--efficient algorithm that transforms an OBDD P representing a Boolean function f with respect to one variable ordering ß into an OBDDQ that represents f with respect to another variable ordering oe. The algorithm runs in average time O(jP jjQj) and requires O(jP j + jQj) space. The importance of the algorithm gets demonstrated by means of experimental results on basically two different applications. In one of them, the algorithm is used merely once. Such transformations are needed to test equivalence or to perform synthesis on OBDDs in which variables appear in different orders. The other application shows a way how to decrease the size of intermediate OBDD representations of a given circuit in the course of its symbolic simulation. Here the algorithm is used dynamically, whenever the size o...","1995-10-06"
416,112426,"Extracting Refined Rules from Knowledge-Based Neural Networks","Geoffrey G. Towell; Jude W. Shavlik;","Neural networks, despite their empirically-proven abilities, have been little used for the refinement of existing knowledge because this task requires a three-step process. First, knowledge must be inserted into a neural network. Second, the network must be refined. Third, the refined knowledge must be extracted from the network. We have previously described a method for the first step of this process. Standard neural learning techniques can accomplish the second step. In this paper, we propose and empirically evaluate a method for the final, and possibly most difficult, step. Our method efficiently extracts symbolic rules from trained neural networks. The four major results of empirical tests of this method are that the extracted rules: (1) closely reproduce the accuracy of the network from which they are extracted; (2) are superior to the rules produced by methods that directly refine symbolic rules; (3) are superior to those produced by previous techniques for extracting rules from ...","1998-12-21"
417,113069,"Analysis of Timed Systems Based on Time-Abstracting Bisimulations","S. Tripakis; S. Yovine;",". We adapt a generic minimal model generation algorithm to compute the coarsest finite model of the underlying infinite transition system of a timed automaton. This model is minimal modulo a time abstracting bisimulation. Our algorithm uses a refinement method that avoids set complementation, and is considerably more efficient than previous ones. We use the constructed minimal model for verification purposes by defining abstraction criteria that allow to further reduce the model and to compare it to a specification. 1 Introduction Behavioral equivalences based on bisimulation relations have proven useful for verifying the correctness of concurrent systems. They allow comparing an implementation to a usually more abstract specification both represented as labeled transition systems. This approach also allows reducing the size of the system by identifying equivalent states which is crucial to avoid the explosion of the state-space. Since the introduction of strong bisimulation in [Mil80]...","1998-11-10"
418,113469,"Learning Ordered Binary Decision Diagrams","David Guijarro; Pau Gargallo;","This note studies the learnability of ordered binary decision diagrams (obdds). We give a polynomial-time algorithm using membership and equivalence queries that finds the minimum obdd for the target respecting a given ordering. We also prove that both types of queries and the restriction to a given ordering are necessary if we want minimality in the output, unless P=NP. If learning has to occur with respect to the optimal variable ordering, polynomial-time learnability implies the approximability of two NP-hard optimization problems: the problem of finding the optimal variable ordering for a given obdd and the Optimal Linear Arrangement problem on graphs. 1 Introduction The representation of boolean functions as ordered binary decision diagrams (obdds) has received great attention recently. This representation has nice computational properties for fixed variable ordering such as the existence of a minimum canonical form and, efficient algorithms for elementary boolean operations, sat...","1995-04-18"
419,113712,"Combinational Logic-Level Verification using Boolean Expression Diagrams","Henrik Hulgaard; Poul Frederick; Reif Andersen; Williams Henrik;","Boolean Expression Diagrams (BEDs) is a new data structure for representing and manipulating Boolean functions. BEDs are a generalization of Binary Decision Diagrams (BDDs) that are capable of representing any Boolean circuit in linear space and still maintain many of the desirable properties of BDDs. This paper demonstrates that BEDs are well suited for solving the combinational logic level verification problem which is, given two combinational circuits, to determine whether they implement the same Boolean functions. Based on all combinational circuits in the ISCAS 85 and LGSynth 91 benchmarks, we demonstrate that BEDs outperform both standard BDD approaches and the techniques specifically developed to exploit structural similarities for efficiently solving the problem. 1 Introduction The combinational logic-level verification problem is to determine whether two given combinational circuits implement the same Boolean function. Let f and g represent the Boolean functions of the two co...","1997-07-03"
420,113889,"The Weakest Compositional Semantic Equivalence Preserving Nexttime-less Linear Temporal Logic","Antti Valmari; Roope Kaivola;",". Temporal logic model checking is a useful method for verifying properties of finite-state concurrent systems. However, due to the state explosion problem modular methods like compositional minimisation based on semantic congruences are essential in making the verification task manageable. In this paper we show that the so-called CFFD-equivalence defined by initial stability, infinite traces, divergence traces and stable failures is exactly the weakest compositional equivalence preserving nexttime-less linear temporal logic with an extra operator distinguishing deadlocks from divergences. Furthermore, a slight modification of CFFD, called the NDFD-equivalence, is exactly the weakest compositional equivalence preserving standard next timeless linear temporal logic. 1 Introduction Many important correctness considerations of concurrent systems lend themselves to representing the system by a finite-state model, and consequently, to automatic verification. However, due to the state-explo...","1997-12-05"
421,114095,"A Model-based Approach to Reactive Self-Configuring Systems","Brian C. Williams; Urang Nayak;","This paper describes Livingstone, an implemented kernel for a model-based reactive self-configuring autonomous system. It presents a formal characterization of Livingstone's representation formalism, and reports on our experience with the implementation in a variety of domains. Livingstone provides a reactive system that performs significant deduction in the sense/response loop by drawing on our past experience at building fast propositional conflict-based algorithms for model-based diagnosis, and by framing a model-based configuration manager as a propositional feedback controller that generates focused, optimal responses. Livingstone's representation formalism achieves broad coverage of hybrid hardware/software systems by coupling the transition system models underlying concurrent reactive languages with the qualitative representations developed in model-based reasoning. Livingstone automates a wide variety of tasks using a single model and a single core algorithm, thus making signif...","1996-04-17"
422,114285,"Petri Net Analysis Using Boolean Manipulation","Enric Pastor; Jordi Cortadella; Oriol Roig; Rosa M. Badia;",". This paper presents a novel analysis approach for bounded Petri nets. The net behavior is modeled by boolean functions, thus reducing reasoning about Petri nets to boolean calculation. The state explosion problem is managed by using Binary Decision Diagrams (BDDs), which are capable to represent large sets of markings in small data structures. The ability of Petri nets to model systems, the flexibility and generality of boolean algebras, and the efficient implementation of BDDs, provide a general environment to handle a large variety of problems. Examples are presented that show how all the reachable states (10 18 ) of a Petri net can be efficiently calculated and represented with a small BDD (10 3 nodes). Properties requiring an exhaustive analysis of the state space can be verified in polynomial time in the size of the BDD. 1 Introduction Petri nets were initially proposed by C.A. Petri in 1962 for describing information processing systems, characterized as being concurrent, a...","1994-06-17"
423,114690,"Exploiting Symmetries In Stochastic Process Algebras","Holger Hermanns; Marina Ribaudo;","Stochastic Process Algebras have been introduced to enable compositional performance analysis of parallel and distributed systems. As with other high level modelling formalisms, state space explosion is a frequently observed problem, especially if the system consists of many cooperating components. However, if the components are identical replicas of each others, the state space can be reduced by means of equivalence preserving aggregation. This paper introduces symmetric parallel composition, an operator to specify sets of identical replicas cooperating in parallel. Its operational semantics is consistent with usual parallel composition whereas the state space explosion problem is drastically reduced. We illustrate this beneficial effect, and provide an interpretation of symmetric parallel composition in terms of Petri Nets. INTRODUCTION Continuous time Markov chains (CTMC), which are widely used as performance models in many diverse areas, are usually generated from high level descr...","1998-07-08"
424,116523,"The Well-Founded Semantics for General Logic Programs","Allen Van Gelder; John S. Schlipf; Kenneth A. Ross;","A general logic program (abbreviated to ""program"" hereafter) is a set of rules that have both positive and negative subgoals. It is common to view a deductive database as a general logic program consisting of rules (IDB) sitting above elementary relations (EDB, facts). It is desirable to associate one Herbrand model with a program and think of that model as the ""meaning of the program,"" or its ""declarative semantics."" Ideally, queries directed to the program would be answered in accordance with this model. Recent research indicates that some programs do not have a ""satisfactory"" total model; for such programs, the question of an appropriate partial model arises. We introduce unfounded sets and well-founded partial models, and define the well-founded semantics of a program to be its well-founded partial model. If the well-founded partial model is in fact a total model, we call it the well-founded model. We show that the class of programs possessing a total well-founded model properly in...","1997-07-14"
425,116858,"A New Logical Framework for Deductive Planning","Susanne Biundo; Werner Stephan;","In this paper we present a logical framework for defining consistent axiomatizations of planning domains. A language to define basic actions and structured plans is embedded in a logic. This allows general properties of a whole planning scenario to be proved as well as plans to be formed deductively. In particular, frame assertions and domain constraints as invariants of the basic actions can be formulated and proved. Even for complex plans most frame assertions are obtained by purely syntactic analysis. In such cases the formal proof can be generated in a uniform way. The formalism we introduce is especially useful when treating recursive plans. A tactical theorem prover, the Karlsruhe Interactive Verifier KIV is used to implement this logical framework. 1 Introduction In this paper we present a logical framework for defining consistent axiomatizations of planning domains. An effective mechanism for defining the basic operations in a constructive way is embedded in a logic that allo...","1997-01-13"
426,117083,"Towards the Systematic Development of Description Logic Reasoners: CLASP reconstructed","Alex Borgida;","It is argued that considerable benefits can be obtained by making KR&R systems easily extendable, so that new language constructs can be added on a per-application basis. In order to achieve this extensibility we need techniques for formally specifying the extensions, and for modularly and locally modifying the implementations. We present two such techniques applicable to the family of reasoners based on Description/Terminological Logics, namely natural semantics rules of inference, and the protodl customizable KBMS architecture. The bulk of the paper aims to demonstrate the efficacy of these techniques, together with some heuristics for their use, by showing how we could reconstruct a previously proposed description logic: Devanbu and Litman's extension to classic, called clasp, designed to reason about actions and plans. In the process, we uncover a few deficiencies in the original proposal, and provide for the first time a formal semantics for clasp. 1 Introduction and Aims Knowl...","1997-01-08"
427,117522,"Engineering Simple, Efficient Code Generator Generator","Christopher Fraser Laboratories; David R. Hanson; Todd A. Proebsting;","This paper describes a program called iburg that reads a burg specification and writes a matcher that does DP at compile time. The matcher is hard coded, a technique that has proven effective with other types of code generators [9, 12]. iburg was built to test early versions of what evolved into","1995-05-18"
428,117609,"Hierarchical mixtures of experts and the EM algorithm","Michael I. Jordan; Robert A. Jacobs;","We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain.","1995-05-14"
429,117810,"On Biases in Estimating Multi-Valued Attributes","Igor Kononenko;","We analyse the biases of eleven measures for estimating the quality of the multi-valued attributes. The values of information gain, J- measure, gini-index, and relevance tend to linearly increase with the number of values of an attribute. The values of gain-ratio, distance measure, Relief , and the weight of evidence decrease for informative attributes and increase for irrelevant attributes. The bias of the statistic tests based on the chi-square distribution is similar but these functions are not able to discriminate among the attributes of different quality. We also introduce a new function based on the MDL principle whose value slightly decreases with the increasing number of attribute's values. 1 Introduction In top down induction of decision trees various impurity functions are used to estimate the quality of attributes in order to select the ""best"" one to split on. However, various heuristics tend to overestimate the multi-valued attributes. One possible approach to this proble...","1996-06-11"
430,118050,"Learning Symbolic Rules Using Artificial Neural Networks","Jude W. Shavlik; Mark W. Craven;","A distinct advantage of symbolic learning algorithms over artificial neural networks is that typically the concept representations they form are more easily understood by humans. One approach to understanding the representations formed by neural networks is to extract symbolic rules from trained networks. In this paper we describe and investigate an approach for extracting rules from networks that uses (1) the NofM extraction algorithm, and (2) the network training method of soft weight-sharing. Previously, the NofM algorithm had been successfully applied only to knowledge-based neural networks. Our experiments demonstrate that our extracted rules generalize better than rules learned using the C4.5 system. In addition to being accurate, our extracted rules are also reasonably comprehensible. 1 INTRODUCTION Artificial neural networks (ANNs) have been successfully applied to real-world problems as varied as steering a motor vehicle (Pomerleau, 1991) and learning to pronounce English tex...","1997-04-16"
431,118221,"Induction of Selective Bayesian Classifiers","Pat Langley; Stephanie Sage;","In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that carries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research. Introduction In recent years, there has been growing interest in probabilistic methods for induction. Such techniques have a number of clear attractions: they accommodate the flexible nature of many natural concepts; they have inherent resilience ...","1997-04-13"
432,118517,"Multicategory Discrimination via Linear Programming","Kristin P. Bennett; O. L. Mangasarian;","A single linear program is proposed for discriminating between the elements of k disjoint point sets in the n-dimensional real space R n : When the conical hulls of the k sets are (k Gamma 1)-point disjoint in R n+1 , a k-piece piecewise-linear surface generated by the linear program completely separates the k sets. This improves on a previous linear programming approach which required that each set be linearly separable from the remaining k Gamma 1 sets. When the conical hulls of the k sets are not (k Gamma 1)-point disjoint, the proposed linear program generates an error-minimizing piecewise-linear separator for the k sets. For this case it is shown that the null solution is never a unique solver of the linear program and occurs only under the rather rare condition when the mean of each point set equals the mean of the means of the other k Gamma 1 sets. This makes the proposed linear computational programming formulation useful for approximately discriminating between k sets...","1999-02-10"
433,118612,"Use of Decision-Tree Induction for Process Optimization and Knowledge Refinement of an Industrial Process","null","Development of expert systems involves knowledge acquisition which can be supported by applying machine learning techniques. This paper presents the basic idea of using decision-tree induction in process optimization and development of the domain model of electrochemical machining (ECM). It further discusses how decision-tree induction is used to build and refine the knowledge base of the process. The idea of developing an intelligent supervisory system with a learning component (IMAFO, Intelligent MAnufacturing FOreman) that is already implemented, is briefly introduced. The results of applying IMAFO for analyzing data form the ECM process are presented. How the domain model of the process (electrochemical machining) is built from the initial known information and how the results of decision-tree induction can be used to optimize the model of the process and further refine the knowledge base are shown. Two examples are given to demonstrate how new rules (to be included in the knowledg...","1997-10-14"
434,118823,"Implementing a CTL Model Checker","Keijo Heljanko;","This paper discusses the implementation of a branching time temporal logic CTL model checker for the PROD Pr/T-Net Reachability analysis tool. A new algorithm for model checking CTL is presented. This algorithm doesn't need the converse of the transition relation as the EMC algorithm does [4]. The algorithm also provides a counterexample and witness facility using one-pass reachability graph traversal. The ALMC local model checking algorithm as presented in [10] uses a two-pass algorithm. The new algorithm presented here is a global model checking algorithm and requires less memory in the worst case than the local model checking ALMC algorithm.","1996-11-14"
435,118905,"Efficient Code Certification","Dexter Kozen;","We introduce a simple and efficient approach to the certification of compiled code. We ensure a basic but nontrivial level of code safety, including control flow safety, memory safety, and stack safety. The system is designed to be simple, efficient, and (most importantly) relatively painless to incorporate into existing compilers. Although less expressive than the proof carrying code of Necula and Lee or typed assembly language of Morrisett et al., our certificates are compact and relatively easy to produce and to verify. Unlike JAVA bytecode, our system operates at the level of native code; it is not interpreted and no further compilation is necessary. 1 Introduction An exciting recent development in program verification is the notion of certified code. When downloading executable code from an untrusted foreign source, a client also downloads a certificate that is checked before running the code to ensure that it is safe to run locally. The certificate is produced by the supplier ...","1998-01-08"
436,118993,"Implementation and Performance of Munin","John B. Carter; John K. Bennett; Willy Zwaenepoel;","Munin is a distributed shared memory (DSM) system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors. Munin is unique among existing DSM systems in its use of multiple consistency protocols and in its use of release consistency. In Munin, shared program variables are annotated with their expected access pattern, and these annotations are then used by the runtime system to choose a consistency protocol best suited to that access pattern. Release consistency allows Munin to mask network latency and reduce the number of messages required to keep memory consistent. Munin's multiprotocol release consistency is implemented in software using a delayed update queue that buffers and merges pending outgoing writes. A sixteen-processor prototype of Munin is currently operational. We evaluate its implementation and describe the execution of two Munin programs that achieve performance within ten percent of message passing implementations of...","1998-07-26"
437,119041,"Heuristic Minimization of BDDs Using Don't Cares","Alberto L. Sangiovanni-vincentelli; Ramin Hojati; Robert K. Brayton; Thomas R. Shiple;","We present heuristic algorithms for finding a minimum BDD size cover of an incompletely specified function, assuming the variable ordering is fixed. In some algorithms based on BDDs, incompletely specified functions arise for which any cover of the function will suffice. Choosing a cover that has a small BDD representation may yield significant performance gains. We present a systematic study of this problem, establishing a unified framework for heuristic algorithms, proving optimality in some cases,and presenting experimental results. 1 Introduction The problem addressed is, given an incompletely specified Boolean function F , find a cover for F whose reduced ordered binary decision diagram [2] (hereafter, BDD) representation is minimum. F is described by a pair of completely specified Boolean functions f and c, such that any cover of F must contain f Delta c and must be contained by f + c. The usual interpretation is that we care about the value of f where c is true, and we don't...","1994-04-28"
438,119886,"Formal Verification of an Interactive Consistency Algorithm for the Draper FTP Architecture Under a Hybrid Fault Model","John Rushby; Patrick Lincoln;","Fault-tolerant systems for critical applications should tolerate as many kinds of faults and as large a number of faults as possible, while using as little hardware as feasible. And they should be provided with strong assurances for their correctness. Byzantine fault-tolerant architectures are attractive because they tolerate any kind fault, but they are rather expensive: at least 3m + 1 processors are required to withstand m arbitrary faults. Two recent developments mitigate some of the costs: algorithms that operate under a hybrid fault model tolerate more faults for a given number of processors than classical Byzantine fault-tolerant algorithms, and asymmetric architectures tolerate a given number of faults with less hardware than conventional architectures. In this paper we combine these two developments and present an algorithm for achieving interactive consistency (the problem of distributing sensor samples consistently in the presence of faults) under a hybrid fault model on an ...","1994-07-06"
439,119933,"Completeness and Consistency in Hierarchical State-Based Requirements","Mats P. E. Heimdahl; Nancy G. Leveson;","This paper describes methods for automatically analyzing formal, state-based requirements specifications for some aspects of completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. Statespace explosion problems are eliminated by applying the analysis at a high level of abstraction; i.e., instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision-avoidance system required on all commercial aircraft with more than 30 passengers that fly in U.S. airspace. 1 Introduction A software requirements specification should be a comprehensive statement of a software system's intended...","1996-05-01"
440,120053,"Exploiting Variable Dependency in Local Search","Bart Selman; David Mcallester; Henry Kautz;","Stochastic search has recently been shown to be successful for solving large boolean satisfiability problems. However, systematic methods tend to be more effective in problem domains with a large number of dependent variables: that is, variables whose truth values are directly determined by a smaller set of independent variables. In systematic search, truth values can be efficiently propagated from the independent to the dependent variables by unit propagation. Such propagation is more expensive in traditional stochastic procedures. In this paper we propose a mechanism for effectively dealing with dependent variables in stochastic search. We also provide empirical data showing the procedure outperforms the best previous stochastic and systematic search procedures on large formulas with a high ratio of dependent to independent variables. 1 Introduction Recent years have seen significant progress in our ability to solve large propositional satisfiability problems. Randomly generated pro...","1998-03-17"
441,120172,"Hardware Timing Verification using KRONOS","Oded Maler; Sergio Yovine;","In this paper we describe the KRONOS system, a tool for verifying real-time properties based on the model of timed automata. As an example, we show how KRONOS is applied to the verification of a MOS circuit under various delay assumptions. 1 Introduction The use of finite-state automata as models of synchronous circuits is as old as computer science. Recently, verification methods for finite-state systems have proliferated from academia to industry with the progress in efficient techniques for symbolic model-checking [10], [3]. Timed automata, that is, automata augmented with fictitious clocks that can measure the time between various events, were introduced by Dill and Alur [7], [2] as models for real-time systems and asynchronous circuits. At this real-time level of modeling, systems are not viewed anymore as generators of sequences over an abstract time domain, but rather as generators of signals over the real time domain. This allows us to speak quantitatively about the implementati...","1996-05-03"
442,120221,"The Use of Description Logics in KBSE systems","Mark A. Jones; Premkumar T. Devanbu;","The increasing size and complexity of many software systems demand a greater emphasis on capturing and maintaining knowledge at many different levels within the software development process. This knowledge includes descriptions of the hardware and software components and their behavior, external and internal design specifications, and support for system testing. The knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use formally represented knowledge, with associated inference procedures, to support the various subactivities of software development. As they grow in scale, KBSE systems must balance expressivity and inferential power with the real demands of knowledge base construction, maintenance, performance and comprehensibility. Description Logics (DL's) possess several features -- a terminological orientation, a formal semantics and efficient reasoning procedures -- which offer an effective tradeoff of these factors. We discuss three KBSE ...","1996-09-17"
443,120273,"A Counter Example to the Stronger Version of the Binary Tree Hypothesis","Igor Kononenko;","The paper describes a counter example to the hypothesis which states that a greedy decision tree generation algorithm that constructs binary decision trees and branches on a single attribute-value pair rather than on all values of the selected attribute will always lead to a tree with fewer leaves for any given training set. We show also that RELIEFF is less myopic than other impurity functions and that it enables the induction algorithm that generates binary decision trees to reconstruct optimal (the smallest) decision trees in more cases. Keywords: binary decision trees, counter example, RELIEFF 1 Introduction Typical machine learning algorithms that are used to discover classification rules from databases use a top down induction of decision trees approach. The smallest decision tree among all the trees induced in a top down manner that have the same classification accuracy is considered the best. Its prediction capability should be the best according to the minimal description le...","1996-06-11"
444,120860,"Verification of Timed Systems Using POSETs","Chris J. Myers; Wendy Belluomini;",". This paper presents a new algorithm for efficiently verifying timed systems. The new algorithm represents timing information using geometric regions and explores the timed state space by considering partially ordered sets of events rather than linear sequences. This approach avoids the explosion of timed states typical of highly concurrent systems by dramatically reducing the ratio of timed states to untimed states in a system. A general class of timed systems which include both event and level causality can be specified and verified. This algorithm is applied to several recent timed benchmarks showing orders of magnitude improvement in runtime and memory usage. 1 Introduction The fundamental difficulty in verification is controlling the state explosion problem. The state spaces involved in verifying reasonably sized systems are large even if the timing behavior of the system is not considered. The problem gets even more complex when verification is done on timed systems. However, v...","1998-08-03"
445,120884,"Reactive Modules","Rajeev Alur; Thomas A. Henzinger;",". We present a formal model for concurrent systems. The model represents synchronous and asynchronous components in a uniform framework that supports compositional (assume-guarantee) and hierarchical (stepwise-refinement) design and verification. While synchronous models are based on a notion of atomic computation step, and asynchronous models remove that notion by introducing stuttering, our model is based on a flexible notion of what constitutes a computation step: by applying an abstraction operator to a system, arbitrarily many consecutive steps can be collapsed into a single step. The abstraction operator, which may turn an asynchronous system into a synchronous one, allows us to describe systems at various levels of temporal detail. For describing systems at various levels of spatial detail, we use a hiding operator that may turn a synchronous system into an asynchronous one. We illustrate the model with diverse examples from synchronous circuits, asynchronous shared-memory progr...","1998-07-28"
446,121770,"Size and Structure of Random OBDDs","Clemens Gropl; Hans Jurgen; Promel Anand Srivastav;","We investigate the size and structure of ordered binary decision diagrams (OBDDs) for random Boolean functions. Previous work of Wegener [Weg], pioneered by Liaw and Lin [LL], showed that for almost all values of n , the number of its variables, the expected OBDD-size of a random Boolean function chosen according to the uniform distribution is equal to its worst-case size up to terms of lower order. Our main result is that this phenomenon, also known as strong Shannon effect, does not hold iff n is of the form n = 2 h + h + a , where a = O(1) . The probability bounds from [Weg] are improved using martingale inequalities. Also, the oscillation of the worst-case size of OBDDs is described, a phenomenon not covered by [LL] and Heap and Mercer [HM]. Keywords. Binary decision diagrams, Boolean functions, read-once branching programs, reduction rules, Shannon effect, size complexity, variable ordering. 1 Introduction A Boolean function is a mapping f : f0; 1g n ! f0; 1g depending on Bo...","1997-07-02"
447,121831,"Cooperative Responses In Deductive Databases","Annie Gal;","This thesis 1 addresses the problem of full utilization of the semantics already present in a database, and particularly, integrity constraints, to respond more cooperatively to a questioner. This thesis provides a new use for database integrity constraints, primarily useful for maintaining database consistency during updates. We specify a general approach to the problem. We explain how to collect information relevant to a user's query, so as to give the user a cooperative response. Then, we describe rules used to select from among the collected information, to provide a clear and concise informative answer to the questioner. A general set of heuristics rules are offered for that purpose. An implementation of this approach in the form of a natural language interface is provided. Our approach to generating cooperative responses is domain-independent, although it uses domain specific information (contained in the form of integrity constraints, the user's query and the database itself). ...","1999-03-19"
448,122014,"An Efficient Reactive Planner for Synthesizing Reactive Plans","And Froduald Kabanza; Patrice Godefroid;","We present a nonlinear forward-search method suitable for planning the reactions of an agent operating in a highly unpredictable environment. We show that this method is more efficient than existing linear methods. We then introduce the notion of safety and liveness rules. This makes possible a sharper exploitation of the information retrieved when exploring the future of the agent. Introduction Classically, a plan is a set of actions to guide an agent from its current situation to another situation called the goal. If the result of these actions is not always the expected one, the agent is said to be operating in an unpredictable environment. Under this assumption, the agent may be deviated at any time from the intermediate situations expected in its plan. Whenever there is such a deviation, the agent has to replan from its new current situation. In real-time applications, the agent does not always have the time to replan. This prompted the development of new agent architectures whe...","1997-05-07"
449,122552,"Subsumption and Indexing in Constraint Query Languages with Linear Arithmetic Constraints","Divesh Srivastava;","Bottom-up evaluation of a program-query pair in a constraint query language (CQL) starts with the facts in the database and repeatedly applies the rules of the program, in iterations, to compute new facts, until we have reached a fixpoint. Checking if a fixpoint has been reached amounts to checking if any ""new"" facts were computed in an iteration. Such a check also enhances efficiency in that subsumed facts can be discarded, and not be used to make any further derivations in subsequent iterations, if we use Semi-naive evaluation. We show that the problem of subsumption in CQLs with linear arithmetic constraints is coNP complete, and present a deterministic algorithm, based on the divide and conquer strategy, for this problem. We also identify polynomial-time sufficient conditions for subsumption and nonsubsumption in CQLs with linear arithmetic constraints. We adapt indexing strategies from spatial databases for efficiently indexing facts in such a CQL: such indexing is crucial for per...","1996-12-10"
450,122701,"Verifying Parameterized Networks using Abstraction and Regular Languages","And O. Grumberg; And S. Jha; E. M. Clarke;","ion and Regular Languages ? E. M. Clarke 1 and O. Grumberg 2 and S. Jha 1 1 Carnegie Mellon University, Pittsburgh, PA 15213 2 Computer Science Dept, The Technion, Haifa 32000, Israel Abstract. This paper describes a technique based on network grammars and abstraction to verify families of state-transition systems. The family of state-transition systems is represented by a context-free network grammar. Using the structure of the network grammar our technique constructs an invariant which simulates all the state-transition systems in the family. A novel idea used in this paper is to use regular languages to express state properties. We have implemented our techniques and verified two non-trivial examples. 1 Introduction Automatic verification of state-transition systems using temporal logic model checking has been investigated by numerous authors [3, 4, 5, 12, 16]. The basic model checking problem is easy to state Given a state-transition system P and a temporal formula f , de...","1995-06-07"
451,122844,"Feature Selection for Case-Based Classification of Cloud Types: An Empirical Comparison","David W. Aha; Richard L. Bankert;","Accurate weather prediction is crucial for many activities, including Naval operations. Researchers within the meteorological division of the Naval Research Laboratory have developed and fielded several expert systems for problems such as fog and turbulence forecasting, and tropical storm movement. They are currently developing an automated system for satellite image interpretation, part of which involves cloud classification. Their cloud classification database contains 204 high-level features, but contains only a few thousand instances. The predictive accuracy of classifiers can be improved on this task by employing a feature selection algorithm. We explain why non-parametric case-based classifiers are excellent choices for use in feature selection algorithms. We then describe a set of such algorithms that use case-based classifiers, empirically compare them, and introduce novel extensions of backward sequential selection that allows it to scale to this task. Several of the approache...","1994-08-12"
452,122883,"Synthesis and Simulation of Digital Systems Containing Interacting Hardware and Software Components","De Micheli; Jr. Giovanni;","Synthesis of systems containing application-specific as well as reprogrammable components, such as off-the-shelf microprocessors, provides a promising approach to realization of complex systems using a minimal amount of application-specific hardware while still meeting the required performance constraints. We describe an approach to synthesis of such hardware-software systems starting from a behavioral description as input. The input system model is partitioned into hardware and software components based on imposed performance constraints. Synchronization between various elements of a mixed system design is one of the key issues that any synthesis system must address. In this paper, we consider software and interface synchronization schemes that facilitate communication between system components. We present tools to perform synthesis and simulation of a system description into hardware and software components. In particular, we describe a program, Poseidon, that performs concurrent eve...","1996-02-15"
453,123606,"On Configuring a Single Disk Continuous Media Server","Cyrus Shahabi; Seon Ho Kim; Shahram Gh;","The past decade has witnessed a proliferation of repositories that store and retrieve continuous media data types, e.g., audio and video objects. These repositories are expected to play a major role in several emerging applications, e.g., library information systems, educational applications, entertainment industry, etc. To support the display of a video object, the system partitions each object into fixed size blocks. All blocks of an object reside permanently on the disk drive. When displaying an object, the system stages the blocks of the object into memory one at a time for immediate display. In the presence of multiple displays referencing different objects, the bandwidth of the disk drive is multiplexed among requests. This introduces disk seeks that reduce the useful utilization of the disk bandwidth, resulting in a lower number of simultaneous displays (throughput). This paper characterizes the impact of disk seeks on the throughput of the system. It describes REBECA as a mecha...","1998-12-19"
454,123736,"Estimating Attributes: Analysis and Extensions of RELIEF","Igor Kononenko;",". In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem. 1 Introduction This paper deals with the problem of estimating the quality of attributes with strong dependencies to other attributes which seems to be the key issue of machine learning in general. Namely, for particular problems (e.q. parity problems of higher degrees) the discovering of dependencies between attributes may be unfeasible due to combinatorial explosion. In such cases efficient heuris...","1996-05-31"
455,124021,"Probabilistic Hill-Climbing: Theory and Applications","Russell Greiner;","Many learning systems search through a space of possible performance elements, seeking an element with high expected utility. As the task of finding the globally optimal element is usually intractable, many practical learning systems use hill-climbing to find a local optimum. Unfortunately, even this is difficult, as it depends on the distribution of problems, which is typically unknown. This paper addresses the task of approximating this hill-climbing search when the utility function can only be estimated by sampling. We present an algorithm that returns an element that is, with provably high probability, essentially a local optimum. We then demonstrate the generality of this algorithm by sketching three meaningful applications, that respectively find an element whose efficiency, accuracy or completeness is nearly optimal. These results suggest approaches to solving the utility problem from explanation-based learning, the multiple extension problem from nonmonotonic reasoning and the ...","1992-08-05"
456,124043,"Lookahead and Pathology in Decision Tree Induction","Sreerama K. Murthy; Steven Salzberg;","The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we study an alternative approach, in which the algorithms use limited lookahead to decide what test to use at a node. We systematically compare, using a very large number of decision trees, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main results of our experiments are: (i) the greedy approach produces trees that are just as accurate as trees produced with the much more expensive lookahead step; and (ii) decision tree induction exhibits pathology, in the sense that lookahead can produce trees that are both larger and less accurate than trees produced without it. 1. Introduction The standard algorithm for constructing decision trees from a set of examples is greedy induction --- a tree is induced top-down with locally optimal choices made at each node, with...","1995-03-29"
457,124562,"Omega-MKRP - A Proof Development Environment","Dan Nesmith; Erica Melis; Jorg Siekmann; Jorn Richts; Manfred Kerber; Michael Kohlhase; Xiaorong Huang;","Introduction In the following we describe the basic ideas underlyingOmegaGamma mkrp, an interactive proof development environment [4]. The requirements for this system were derived from our experiences in proving an interrelated collection of theorems of a typical textbook on semi-groups and automata [2] with the first-order theorem prover MKRP [6]. An important finding was that although current automated theorem provers have evidently reached the power to solve non-trivial problems, they do not provide sufficient assistance for proving the theorems contained in such a textbook. On account of this, we believe that significantly more support for proof development can be provided by a system with the following two features: Firstly the system must provide a comfortable human-oriented problem-solving environment. In particular, a human user should be able to specify the problem to be solved in a natural way and communicate on proof search strategies with the sy","1995-04-24"
458,124735,"On The Complexity Of The Hidden Weighted Bit Function For Various BDD Models","Beate Bollig; Ingo Wegener; Martin Sauerho;","Ordered binary decision diagrams (OBDDs) and several more general BDD models have turned out to be representations of Boolean functions which are useful in applications like verication, timing analysis, test pattern generation or combinatorial optimization. The hidden weighted bit function (HWB) is of particular interest, since it seems to be the simplest function with exponential OBDD size. The complexity of this function with respect to dierent circuit models, formulas, and various BDD models is discussed. Supported by DFG grant We 1066/8-1. 1. INTRODUCTION If one likes to have short representations of Boolean functions, circuits are the most powerful model. But if one likes to work with these representations, one additionally needs eÆcient algorithms for certain problems, among them satisability test, equivalence test, and synthesis, i. e., the combination of two or more representations by a Boolean operation. For this purpose ordered binary decision diagrams (OBDDs) introd...","1999-03-09"
459,125089,"Timing Analysis of Industrial Real-Time Systems","E. Clarke; M. Minea; S. Campos; W. Marrero;","In this paper, we describe a formal method for modelling real-time systems and a procedure to compute the model's timing characteristics automatically. We present algorithms that compute exact bounds on the delay between two specified events. We also describe an algorithm to count the minimum and maximum number of times an event occurs between a given starting condition and an ending condition. These algorithms are based on symbolic model checking techniques [6, 24] which have been successfully used to find bugs in several industrial designs. Such techniques can be used to search exhaustively state spaces with up to 10 30 states. To illustrate the usefulness of our method, we describe the timing analysis for a patient monitoring system with more than 10 13 states. We also present the timing analysis and verification for an aircraft controller. The sizes of the examples we verify demonstrate that our tool can be applied to realistic industrial designs. 1 Introduction Symbolic model...","1999-01-15"
460,125643,"Functional languages and very fine grained parallelism: Initial results","Jon Mountjoy; Marcel Beemster;","A functional language compiler can be used as a powerful tool in the scheduling of programs for hardware capable of fine grained instruction level parallelism. There have been many attempts to effectively utilise functional languages as a means of easily programming parallel hardware. These attempts have generally concentrated on parallelising the underlying reduction strategies to achieve this goal. This paper explores an alternative use of a functional language compiler; as a tool for producing annotated code which can be effectively scheduled for the class of transport triggered architectures. These architectures are capable of much fine grained instruction level parallelism, and it is the task of the compiler to schedule code to utilise this power, in a manner similar to that of a compiler for a VLIW architecture. We believe that a functional language compiler has more freedom in the generation of code than imperative language compilers, due to the level of abstraction provided by ...","1995-01-09"
461,126336,"The Boyer-Moore Theorem Prover and Its Interactive Enhancement","J Strother Moore; Matt Kaufmann; Robert S. Boyer;",". The so-called ""Boyer-Moore Theorem Prover"" (otherwise known as ""Nqthm"") has been used to perform a variety of verification tasks for two decades. We give an overview of both this system and an interactive enhancement of it, ""Pc-Nqthm,"" from a number of perspectives. First we introduce the logic in which theorems are proved. Then we briefly describe the two mechanized theorem proving systems. Next, we present a simple but illustrative example in some detail in order to give an impression of how these systems may be used successfully. Finally, we give extremely short descriptions of a large number of applications of these systems, in order to give an idea of the breadth of their uses. This paper is intended as an informal introduction to systems that have been described in detail and similarly summarized in many other books and papers; no new results are reported here. Our intention here is merely to present Nqthm to a new audience. This research was supported in part by ONR Contract N...","1995-09-16"
462,126858,"Verifying the Performance of the PCI Local Bus using Symbolic Techniques","E. Clarke; M. Minea; S. Campos; W. Marrero;","Symbolic model checking is a successful technique for checking properties of large finite-state systems. This method has been used to verify a number of real-world hardware designs. This methodology, however, is not able to determine timing or performance properties directly. Since these properties are extremely important in the design of high-performance systems and in time-critical applications, we have extended model checking techniques to produce timing information. These results allow a more detailed analysis of a model than is possible with tools that simply determine whether a property is satisfied of not. We present algorithms that determine the exact bounds on the delay between two specified events and the number of occurrences of another event in all such intervals. To demonstrate how our method works, we have modelled the PCI local bus and analyzed its temporal behavior. These results show the usefulness of this technique in analyzing complex modern designs. This research wa...","1997-10-24"
463,127138,"Explaining Program Execution in Deductive Systems","Divesh Srivastava; Praveen Seshadri; Raghu Ramakrishnan; Tarun Arora; William G. Roth;",". Programs in deductive database and programming systems have a natural meaning that is based upon their mathematical reading as logical rules. High-level `explanations' of a program evaluation/execution can be constructed to provide added functionality: (1) To debug a program by following a chain of deductions leading to an unexpected (and possibly incorrect) conclusion; (2) To follow the derivation of certain correct conclusions to determine why and how they are reached; (3) To identify consequences of a (typically, incorrect or unexpected) fact. This functionality can be utilized either to perform post-mortem analysis of a session, or to interactively develop programs by running queries and viewing their deductions simultaneously. `Explanations' of programs are especially important in the context of deductive databases for three reasons: (1) These programs could involve recursion, and hence, the chain of inferences is often not evident. (2) When the input data set is large, it is v...","1996-12-10"
464,127612,"Hardware-Verification using First Order BDDs","Klaus Schneider; Thomas Kropf;","Binary decision diagrams (BDDs) are a well known method for representing and comparing boolean functions. Although BDDs are known to be very compact, in all known approaches for hardware verification, BDD-based calculi are restricted to propositional logic. This logic is insufficient for the verification of abstract data types, time abstraction and also for hierarchical verification. In this paper, the lifting of graphs based on shannon expansions and the related binary decision diagrams to first order logic is described and the soundness and correctness theorems are stated. The power of these techniques in the domain of hardware verification is shown by a case study using a hierarchical circuit. Keyword Codes: I.2.3; F.4.1 Keywords: Hardware Verification; Deduction and Theorem Proving; Mathematical Logic 1 Introduction Most automated approaches to hardware-verification are limited to propositional logic or temporal extensions of it (e.g. [BCMD90]), since these logics are decidable. A...","1997-12-19"
465,127834,"Explanation-Based Learning and Reinforcement Learning: A Unified View","Nicholas S. Flann; Thomas G. Dietterich;","In speedup-learning problems, where full descriptions of operators are always known, both explanation-based learning (EBL) and reinforcement learning (RL) can be applied. This paper shows that both methods involve fundamentally the same process of propagating information backward from the goal toward the starting state. RL performs this propagation on a state-by-state basis, while EBL computes the weakest preconditions of operators, and hence, performs this propagation on a region-by-region basis. Based on the observation that RL is a form of asynchronous dynamic programming, this paper shows how to develop a dynamic programming version of EBL, which we call Explanation-Based Reinforcement Learning (EBRL). The paper compares batch and online versions of EBRL to batch and online versions of RL and to standard EBL. The results show that EBRL combines the strengths of EBL (fast learning and the ability to scale to large state spaces) with the strengths of RL (learning of optimal policies)...","1995-04-21"
466,127847,"A Constraint Solver in Finite Algebras and Its Combination With Unification Algorithms","Christophe Ringeissen;","In the context of constraint logic programming and theorem proving, the development of constraint solvers on algebraic domains and their combination is of prime interest. A constraint solver in finite algebras is presented for a constraint language including equations, disequations and inequations on finite domains. The method takes advantage of the embedding of a finite algebra in a primal algebra that can be presented, up to an isomorphism, by an equational presentation. We also show how to combine this constraint solver in finite algebras with other unification algorithms, by extending the techniques used for the combination of unification. 1 Introduction Finite algebras provide valuable domains for constraint logic programming. Unification in this context has attracted considerable interest for its applications: it is of practical relevance for manipulating hardware descriptions and solving formulas of propositional calculus; its implementation in constraint logic programming lang...","1996-09-04"
467,128379,"Graphs in METAFrame: The Unifying Power of Polymorphism","A. Dannecker; B. Steffen; C. Friedrich; D. Koschutzki; F. Schreiber; Lehrstuhl Fur Programmiersysteme; M. Von Der Beeck; T. Margaria; V. Braun;",". We present a highly polymorphic tool for the construction, synthesis, structuring, manipulation, investigation, and (symbolic) execution of graphs. The flexibility of this tool, which mainly arises as a consequence of combining complex graph labelings expressing the intended semantics with hierarchy and customized graphical node representations, is illustrated along a representative choice of application scenarios. 1 Introduction Graphs are theoretically well-studied and widely used in practice to model e.g. dependence, hierarchy and computation. Particularly powerful are semantics based versions of labelled graphs, 1 which adequately represent structures like flow graphs, transition systems, automata, partial orders, and Petri nets. Therefore a number of tools like Autograph [15], the Concurrency Factory [2], SDT [18] and PEP [6] has been developed supporting these structures. However, except for a few cases, where a small set of similar structures is supported, they are limited t...","1999-02-11"
468,128963,"Training A 3-Node Neural Network Is NP-Complete","Avrim L. Blum; Ronald L. Rivest;",": We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with non-linear functions such as sigmoids. Keywords: Neural networks, computational complexity, NP-completeness, intractability, learning, training, mu...","1998-01-26"
469,129196,"Symbolic Model Checking of Concurrent Probabilistic Systems Using MTBDDs and Simplex","David Parker; Gethin Norman; Marta Kwiatkowska; Roberto Segala;","Symbolic model checking for purely probabilistic processes using MTBDDs [12] was introduced in [4] and further developed in [20, 3]. In this paper we consider models for concurrent probabilistic systems similar to those of [28, 7, 5] and the concurrent Markov chains of [35, 13], which extend the purely probabilistic processes through the addition of nondeterministic choice. As a specification formalism we use probabilistic branching time temporal logic PBTL of [5, 7], which allows to express properties such as ""under any scheduling of nondeterministic choices, the probability of OE holding until / is true is at least 0.78"". In [5, 7] it is shown that the verification of ""until"" properties can be reduced to a linear programming problem and solved with the help of e.g. the simplex algorithm, but no symbolic model checking is considered. Based on the algorithms of [5, 7], we derive symbolic model checking procedure for PBTL over concurrent probabilistic systems using MTBDDs. We furthermor...","1999-01-22"
470,129309,"On The Power Of Different Types Of Restricted Branching Programs","And Ingo Wegener; Beate Bollig; Detlef Sieling; Martin Sauerhoff;","Almost the same types of restricted branching programs (or binary decision diagrams BDDs) are considered in complexity theory and in applications like hardware verification. These models are read-once branching programs (free BDDs) and certain types of oblivious branching programs (ordered and indexed BDDs with k layers). The complexity of the satisfiability problem for these restricted branching programs is investigated and tight hierarchy results are proved for the classes of functions representable by k layers of ordered or indexed BDDs of polynomial size. Supported in part by DFG grant We 1066/7--1 and 2. 1. INTRODUCTION Branching programs are a well established computation model for discrete functions. Definition 1: A branching program G for a function f : A n ! B m , where A = f0; : : : ; a Gamma 1g and B = f0; : : : ; b Gamma 1g is a directed acyclic graph. The sink nodes are labeled by constants from B. The inner nodes are labeled by variables from X = fx 1 ; : : :...","1995-01-19"
471,130238,"Completeness and Consistency Analysis of State-Based Requirements","Mats P. E. Heimdahl; Nancy G. Leveson;","This paper describes methods for automatically analyzing formal, state-based requirements specifications for completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State space explosion problems are eliminated by applying the analysis at a high level of abstraction; i.e, instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision avoidance system required on all commercial aircraft with more than 30 passengers that fly in U.S. airspace. 1 Introduction This paper describes methods and tools for automatically analyzing software requirements for completeness and consistency. C...","1995-01-13"
472,130506,"Set-Oriented Production Rules in Relational Database Systems","Jennifer Widom; Sheldon J. Finkelstein;","We propose incorporating a production rules facility into a relational database system. Such a facility allows definition of database operations that are automatically executed whenever certain conditions are met. In keeping with the set-oriented approach of relational data manipulation languages, our production rules are also set-oriented---they are triggered by sets of changes to the database and may perform sets of changes. The condition and action parts of our production rules may refer to the current state of the database as well as to the sets of changes triggering the rules. We define a syntax for production rule definition as an extension to SQL. A model of system behavior is used to give an exact semantics for production rule execution, taking into account externally-generated operations, selftriggering rules, and simultaneous triggering of multiple rules. 1 Introduction Recently, there has been considerable interest in integrating production rules systems and database manag...","1996-06-26"
473,130901,"Oracle Semantics for Prolog","Michael Codish; Michael Maher; Roberto Barbuti; Roberto Giacobazzi;",". This paper proposes to specify semantic definitions for Prolog in terms of an oracle which provides information on which clauses are to be applied to resolve a given goal. The approach is quite general. It is applicable to pure Prolog to define both operational and declarative semantics; and can be viewed as a basis for both sequential and parallel implementations. Previous semantic definitions for Prolog typically attempt to encode the sequential depth-first search of the language into various mathematical frameworks. A clause is applied in such semantics only if it is chosen under the search strategy. We prefer instead to specify in a more declarative way the condition upon which a clause is to be applied. The decision whether or not to apply a clause may be viewed as a query to an oracle which may be specified from within the semantics or reasoned about from outside. This approach results in simple and concise semantic definitions which are more useful for arguing the correctness ...","1998-07-12"
474,131044,"A Comparison of ID3 and Backpropagation for English Text-to-Speech Mapping","Hermann Hild; Thomas G. Dietterich;",". The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be closely matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially in this domain. Keywords: ID3, backpropagation, experimental comparisons, text-to-speech 1. Introduction There is no universal learning algorithm th...","1994-09-13"
475,131548,"Automatic Symbolic Verification of Embedded Systems","Pei-hsin Ho; Rajeev Alur; Thomas A. Henzinger;",". We present a model checking procedure and its implementation for the automatic verification of embedded systems. Systems are represented by Hybrid Automata ---machines with finite control and real-valued variables modeling continuous environment parameters such as time, pressure, and temperature. System properties are specified in a real-time temporal logic and verified by symbolic computation. The verification procedure, implemented in Mathematica, is used to prove digital controllers and distributed algorithms correct. The verifier checks safety, liveness, time-bounded, and duration properties of hybrid automata. 1 Introduction A hybrid system consists of a discrete program that is embedded in a continuously changing environment and interacts with the environment in real time. More and more real-life processes, from elevators to aircraft, are controlled by such programs. Obviously, correctness is of vital importance for hybrid systems. Yet traditional program verification methods ...","1996-02-23"
476,133447,"Decision Graphs - An Extension of Decision Trees","Jonathan J. Oliver;",": In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain. 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10]. We formalize ...","1995-06-06"
477,134177,"Algorithmic Analysis of Nonlinear Hybrid Systems","Howard Wong-toi; Pei-hsin Ho; Thomas A. Henzinger;",". Hybrid systems are digital real-time systems that are embedded in analog environments. Model-checking tools are available for the automatic analysis of linear hybrid automata, whose environment variables are subject to piecewise-constant polyhedral differential inclusions. In most embedded systems, however, the environment variables have differential inclusions that vary with the values of the variables, e.g. x = x. Such inclusions are prohibited in the linear hybrid automaton model. We present two methods for translating nonlinear hybrid systems into linear hybrid automata. Properties of the nonlinear systems can then be inferred from the automatic analysis of the translated linear hybrid automata. The first method, called clock translation, replaces constraints on nonlinear variables by constraints on clock variables. The clock translation is efficient but has limited applicability. The second method, called linear phase-portrait approximation, conservatively overapproximates the...","1998-04-28"
478,134468,"Verifying Automata Specifications of Probabilistic Real-time Systems","Costas Courcoubetis; David Dill; Rajeev Alur;",". We present a model-checking algorithm for a system presented as a generalized semi-Markov process and a specification given as a deterministic timed automaton. This leads to a method for automatic verification of timing properties of finite-state probabilistic real-time systems. Keywords: Real-time, Probabilistic systems, Automatic verification. 1 Introduction There is increasing awareness that unexpected behavior from interacting processes can cause serious problems. This observation applies not only to programs and digital systems, but also to physical processes, such as robots, automobiles, manufacturing processes, and so on. Indeed, as digital systems become smaller and cheaper, their use to control and interact with physical processes will inevitably increase. Formal verification of these systems seeks mathematical methods for reasoning about their behavior. Automatic formal verification is particularly promising, because it requires far less labor than the manual techniques. ...","1996-08-06"
479,134503,"An Automata-Theoretic Approach to Fair Realizability and Synthesis","Fair Realizability; Moshe Y. Vardi;",". Over the last few years, the automata-theoretic approach to realizability checking and synthesis of reactive modules, developed by Pnueli and Rosner, by Abadi, Lamport, and Wolper, and by Dill and Wong-Toi, has been quite successful, handling both the synchronous and the asynchronous cases. In this approach one transforms the specification into a tree automaton. The specification is realizable if this tree automaton is nonempty, i.e., accepts some infinite tree, and a finite representation of an infinite tree accepted by this automaton can be viewed as a finite-state program realizing the specification. Anuchitanukul and Manna argued that the automata-theoretic approach cannot handle realizability checking and synthesis under fairness assumptions. In this paper we show to the contrary that the automata-theoretic approach can handle realizability checking and synthesis under a variety of fairness assumption. 1 Introduction One of the most significant developments in the area of progr...","1996-07-18"
480,134881,"The Verus Tool: A Quantitative Approach to the Formal Verification of Real-Time Systems","Edmund Clarke; Marius Minea;","Introduction The task of checking if a computer system satisfies its timing specifications is extremely important. These systems are often used in critical applications where failure to meet a deadline can have serious or even fatal consequences. This work describes Verus, an efficient tool for performing this verification task. Using our tool, the system being verified is specified in the Verus language and then compiled into a state-transition graph. A symbolic model checker allows the verification of untimed properties expressed in CTL [8]. Time bounded properties can be verified using RTCTL model checking [7]. Moreover, algorithms derived from symbolic model checking are used to compute quantitative information about the model [1]. The information produced allows the user to check the temporal correctness of the model: schedulability of the tasks of the system can be determined by computing their response time; reaction times to events and s","1998-09-10"
481,134982,"Teaching Coloured Petri Nets - a Gentle Introduction to. . .","null",". This paper is about the two compulsory project assignments set to the students in an undergraduate course on distributed systems. In the first assignment the students design and validate a non-trivial layered protocol by means of Coloured Petri Nets, and in the second they implement the designed protocol in an object-oriented language. From the two assignments the students experience that Coloured Petri Nets, as a formal method, are useful for designing and analysing distributed systems. In the course students are introduced to basic concepts and techniques for distributed systems, and it is explained that such systems are often too complex to manage without using formal methods. In this paper we also report on our experience with teaching the course and describe the didactic methods applied. Based on the obtained experience we conclude that the combination of distributed systems and Coloured Petri Nets is fruitful --- the two areas complement each other. Although our experiences ori...","1997-04-16"
482,135290,"Heterogeneous Uncertainty Sampling for Supervised Learning","David D. Lewis; Haym Hirsh; Jason Catlett; W. Cohen;","Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger. 1 Introduction Machine learning algorithms have been used to build classification rules from data sets consisting of hundreds of thousands of instances [4]. In some applications unlabeled training instances are abundant but the cost of labeling an instance with its class is high. In the informatio...","1996-11-26"
483,135666,"Module Checking","Moshe Y. Vardi; Orna Kupferman;","In computer system design, we distinguish between closed and open systems. A closed system is a system whose behavior is completely determined by the state of the system. An open system is a system that interacts with its environment and whose behavior depends on this interaction. The ability of temporal logics to describe an ongoing interaction of a reactive program with its environment makes them particularly appropriate for the specification of open systems. Nevertheless, model-checking algorithms used for the verification of closed systems are not appropriate for the verification of open systems. Correct model checking of open systems should check the system with respect to arbitrary environments and should take into account uncertainty regarding the environment. This is not the case with current model-checking algorithms and tools. In this paper we introduce and examine the problem of model checking of open systems (module checking , for short). We show that while module checkin...","1998-02-07"
484,135793,"Robust Feature Selection Algorithms","Haleh Vafaie; Kenneth De Jong;","Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency. 1. Introduction Finding an optimal set of features from a large set of candidate features is a problem which occurs in many contexts. In modeling one would like to identify and exploit minimal models of the phenomena under study. In control theory, minimizing the number of control pa...","1997-10-01"
485,135897,"A Tutorial on Learning Bayesian Networks","David Heckerman;","We examine a graphical representation of uncertain knowledge called a Bayesian network. The representation is easy to construct and interpret, yet has formal probabilistic semantics making it suitable for statistical manipulation. We show how we can use the representation to learn new knowledge by combining domain knowledge with statistical data. 1 Introduction Many techniques for learning rely heavily on data. In contrast, the knowledge encoded in expert systems usually comes solely from an expert. In this paper, we examine a knowledge representation, called a Bayesian network, that lets us have the best of both worlds. Namely, the representation allows us to learn new knowledge by combining expert domain knowledge and statistical data. A Bayesian network is a graphical representation of uncertain knowledge that most people find easy to construct and interpret. In addition, the representation has formal probabilistic semantics, making it suitable for statistical manipulation (Howard,...","1995-10-05"
486,136771,"How to Dynamically Merge Markov Decision Processes","David Cohn; Satinder Singh;","We are frequently called upon to perform multiple tasks that compete for our attention and resource. Often we know the optimal solution to each task in isolation. In this paper, we describe how this knowledge can be exploited to efficiently find good solutions for doing the tasks in parallel. We formulate this problem as that of dynamically merging multiple Markov decision processes (MDPs) into a composite MDP, and present a new theoretically-sound dynamic programming algorithm for finding an optimal policy for the composite MDP. We analyze various aspects of our algorithm and illustrate its use on a simple merging problem.","1998-09-08"
487,136975,"Timing Verification by Successive Approximation","A. Itai; M. Yannakakis; R. Alur; R. P. Kurshan;",". We present an algorithm for verifying that a model M with timing constraints satisfies a given temporal property T . The model M is given as a parallel composition of !-automata P i , where each automaton P i is constrained by bounds on delays. The property T is given as an !-automaton as well, and the verification problem is posed as a language inclusion question L(M ) ` L(T ). In constructing the composition M of the constrained automata P i , one needs to rule out the behaviors that are inconsistent with the delay bounds, and this step is (provably) computationally expensive. We propose an iterative solution which involves generating successive approximations M j to M , with containment L(M ) ` L(M j ) and monotone convergence L(M j ) ! L(M ) within a bounded number of steps. As the succession progresses, the approximations M j become more complex. At any step of the iteration one may get a proof or a counterexample to the original language inclusion question. The described algori...","1997-06-17"
488,137324,"SDL Modelling with High Level Petri Nets","Nisse Husberg;","SDL (the Specification and Description Language standardized by ITU) is modelled using High Level Petri nets. The aim is to translate the full TeleNokia SDL to the input langage of PROD, the reachability graph analyzer of the Digital Systems Laboratory at the Technical University in Helsinki. The problems in modelling programming concepts used in the concurrent specification language SDL are investigated. - 1 Introduction The Specification and Description Language SDL has been used with considerable success in concurrent and distributed communication systems. Originally is was used only as a standardized language for giving specifications of the behaviour of communication systems, but gradually it has been more formalized and implemented. Thus it is possible to translate the specification to machine code and execute it. This work is focused on one special implementation, the TeleNokia Specification and Description Language TNSDL. TNSDL is based on SDL-88 [SDL88], but with some modific...","1996-10-10"
489,137611,"Formally Verifying a Microprocessor Using a Simulation Methodology","Derek L. Beatty; Randal E. Bryant;","Formal verification is becoming a useful means of validating designs. We have developed a methodology for formally verifying data intensive circuits (e.g., processors) with sophisticated timing (e.g., pipelining) against high-level declarative specifications. Previously, formally verifying a microprocessor required the use of an automatic theorem prover, but our technique requires little more than a symbolic simulator. We have formally verified a pre-existing 16-bit CISC microprocessor circuit extracted from the fabricated layout. Introduction Previously, symbolic switch-level simulation has been used to verify some small or simple data-intensive circuits (RAMs, stacks, register files, ALUs, and simple pipelines) [2, 3]. In doing so, the necessary simulation patterns were developed by hand or by using ad-hoc techniques, and it was then argued that the patterns were sufficient, and that their generation could be automated. We have developed sufficient theory to fully support such claims...","1970-01-01"
490,137673,"Verifying Systems with Replicated Components in Mur phi","C. Norris Ip; David L. Dill;",". We present an extension to the Mur' verifier to verify systems with replicated identical components.Verification is by explicit state enumeration in an abstract state space where states do not record the exact numbers of components. Through a new datatype, called RepetitiveID, the user can suggest the use of such an abstraction to verify a system of fixed size. Mur' automatically checks the soundness of the abstract state graph, and automatically constructs the abstract state graph using the system description. Using a simple run time check, Mur' can also determine if it can generalize the verification result of a system with fixed size to systems of larger sizes, including the system with infinite number of components. 1 Introduction Finite-state systems such as cache coherence protocols, communication protocols or hardware controllers are often designed to be scalable, so that a description gives a family of different systems, each member of which has a different number of replic...","1996-06-13"
491,137863,"Power Analysis of Embedded Software: A First Step Towards Software Power Minimization","Andrew Wolfe; Sharad Malik; Vivek Tiwari;","Embedded computer systems are characterized by the presence of a dedicated processor and the software that runs on it. Power constraints are increasingly becoming the critical component of the design specification of these systems. At present, however, power analysis tools can only be applied at the lower levels of the design -- the circuit or gate level. It is either impractical or impossible to use the lower level tools to estimate the power cost of the software component of the system. This paper describes the first systematic attempt to model this power cost. A power analysis technique is developed that has been applied to two commercial microprocessors -- Intel 486DX2 and Fujitsu SPARClite 934. This technique can be employed to evaluate the power cost of embedded software and also be used to search the design space in software power optimization. 1 Introduction Embedded computer systems are characterized by the presence of a dedicated processor which executes application specific...","1995-04-03"
492,138276,"Experience With a Learning Personal Assistant","David Zabowski; Dayne Freitag; John Mcdermott; Rich Caruana; Tom Mitchell;","Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user. For example, an assistant that helps schedule a particular user's calendar will have to know that user's scheduling preferences. This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants. We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience. Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users. Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants.","1994-04-19"
493,138453,"The Unison Algorithm: Fast Evaluation of Boolean Expressions","null","We present a Unison algorithm to evaluate Boolean expressions. This novel algorithm, based on the total differential of a Boolean function, enables fast evaluation of Boolean expressions in software. Any combination of Boolean operations can be packed into the bits of one computer word and evaluated in parallel by bitwise logical operations. Sample runs of the Unison algorithm show that many Boolean operations can be evaluated in one clock cycle. The Unison algorithm is able to evaluate Boolean expressions at an execution speed that is comparable to compiled evaluation while retaining the flexibility of interpreted approaches. Keywords: Boolean differential, Boolean evaluation, Boolean expressions, Unison algorithm. 1 Introduction Efficient evaluation of Boolean expressions is an important part of many software systems. Some applications include: data base search [11], programming environments [6], modeling of digital circuits and software systems [2], debugging [9], event recording ...","1996-10-02"
494,138779,"Property Specification Patterns for Finite-State Verification","George S. Avrunin; James C. Corbett; Matthew B. Dwyer;","Finite-state verification (e.g., model checking) provides a powerful means to detect errors that are often subtle and difficult to reproduce. Nevertheless, the transition of this technology from research to practice has been slow. While there are a number of potential causes for reluctance in adopting such formal methods in practice, we believe that a primary cause rests with the fact that practitioners are unfamiliar with specification processes, notations, and strategies. Recent years have seen growing success in leveraging experience with design and coding patterns. We propose a pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification. 1 Introduction Formal specification and verification have been active areas of research for over two decades. While formal approaches offer practitioners some significant advantages over the current state-of-the-practice, they have not been widely adopted. In addition to a lack of def...","1997-10-14"
495,138988,"Classifiers: A Theoretical and Empirical Study","Wray Buntine;","This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed. 1 Introduction Systems for learning classification trees [ Quinlan, 1986; Cestnik et al., 1987 ] are common in machine learning, statistics and pattern recognition. Despite these suc...","1995-09-17"
496,139134,"Symbolic Controller Synthesis for Discrete and Timed Systems","Eugene Asarin; Oded Maler;",". This paper presents algorithms for the symbolic synthesis of discrete and real-time controllers. At the semantic level the controller is synthesized by finding a winning strategy for certain games defined by automata or by timed-automata. The algorithms for finding such strategies need, this way or another, to search the state-space of the system which grows exponentially with the number of components. Symbolic methods allow such a search to be conducted without necessarily enumerating the state-space. This is achieved by representing sets of states using formulae (syntactic objects) over state variables. Although in the worst case such methods are as bad as enumerative ones, many huge practical problems can be treated by fine-tuned symbolic methods. In this paper the scope of these methods is extended from analysis to synthesis and from purely discrete systems to real-time systems. We believe that these results will pave the way for the application of program synthesis techniques to...","1995-07-12"
497,139321,"High Performance BDD Package Based on Exploiting Memory Hierarchy","Alberto Sangiovanni-vincentelli; Jagesh V. Sanghavi; Rajeev K. Ranjan; Robert K. Brayton;","The success of binary decision diagram (BDD) based algorithms for synthesis and/or verification depend on the availability of a high performance package to manipulate very large BDDs. State-ofthe -art BDD packages, based on the conventional depth-first technique, limit the size of the BDDs due to a disorderly memory access patterns that results in unacceptably high elapsed time when the BDD size exceeds the main memory capacity. We present a high performance BDD package that enables manipulation of very large BDDs by using an iterative breadth-first technique directed towards localizing the memory accesses to exploit the memory system hierarchy. The new memory-oriented performance features of this package are 1) an architecture independent customized memory management scheme, 2) the ability to issue multiple independent BDD operations (superscalarity), and 3) the ability to perform multiple BDD operations even when the operands of some BDD operations are the result of some other operat...","1996-03-15"
498,139852,"On the Limitations of Ordered Representations of Functions","Jayram S. Thathachar;","We demonstrate the limitations of the various ordered representations that have been considered in the literature for symbolic model checking including OBDDs [Bry86], MTBDDs [CMZ + 93], EVBDDs [LS92], *-BMDs [BC95] and HDDs [CFZ95] by giving a variety of simple and natural functions for which each representation requires exponential size to represent them. We also show that there is a simple regular language that requires exponential size to be represented by any *-BMD; note that OBDDs can represent any regular language in linear size. 1. Introduction In recent years, symbolic model checking has become one of the most important techniques for formal verification of hardware systems. Bryant [Bry86] introduced the OBDD representation for functions and showed that OBDDs can represent a rich class of functions succinctly and allow various operations to be performed efficiently. Subsequently, it was shown that OBDDs can also be used to handle the state explosion problem in symbolic model...","1996-10-11"
499,140292,"Symbolic Model Checking of Infinite State Systems Using Presburger Arithmetic","Richard Gerber; Tevfik Bultan; William Pugh;","Model checking is a powerful technique for analyzing large, finite-state systems. In an infinite transition system, however, many basic properties are undecidable. In this paper we present a new symbolic model checker which conservatively evaluates safety and liveness properties on infinite-state programs. We use Presburger formulas to symbolically encode a program's transition system, as well as its model-checking computations. All fixpoint calculations are executed symbolically, and their convergence is guaranteed by using approximation techniques. We demonstrate the promise of this technology on some well-known infinite-state concurrency problems. 1 This work was supported in part by ONR grant N00014-94-10228, NSF YI CCR-9357850 and a Packard Fellowship. 1 Introduction In recent years, there has been a surge of progress in the area of automated analysis for finite-state systems. Several reasons for this success are: (1) the development of powerful techniques such as model-check...","1997-01-25"
500,141506,"Oblivious Decision Trees and Abstract Cases","null","In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.","1997-04-13"
501,141812,"Type-Safe Linking and Modular Assembly Language","Greg Morrisett; Neal Glew;","Linking is a low-level task that is usually vaguely specified, if at all, by language definitions. However, the security of web browsers and other extensible systems depends crucially upon a set of checks that must be performed at link time. Building upon the simple, but elegant ideas of Cardelli, and module constructs from high-level languages, we present a formal model of typed object files and a set of inference rules that are sufficient to guarantee that type safety is preserved by the linking process. Whereas Cardelli's link calculus is built on top of the simply-typed lambda calculus, our object files are based upon typed assembly language so that we may model important low-level implementation issues. Furthermore, unlike Cardelli, we provide support for abstract types and higher-order type constructors -- features critical for building extensible systems or modern programming languages such as ML.","1999-04-07"
502,142026,"On-The-Fly Verification Of Finite Transition Systems","Claude Jard; Jean-claude Fernandez; Laurent Mounier; Thierry J;",": The analysis of programs by the exhaustive inspection of reachable states in a finite state graph is a well-understood procedure It is actually implemented in several industrial tools but one of their main limitations is the size of the memory needed to exhaustively build the state graphs of the programs. For numerous properties such as Buchi acceptance (in the deterministic case) and behavioral equivalence, it is not necessary to explicitly build this graph and an exhaustive depth--first traversal is often sufficient. In order to avoid retraversing states, it is however important to store in memory some of the already visited states and randomly replace them (to keep the memory size bounded and avoid a performance falling down) In most cases this depth--first traversal with replacement can push back significantly the limits of verification tools. Key-words: Verification, Finite Transition Systems, model-checking, bisimulation, depth-first search. (R'esum'e : tsvp) This work was par...","1993-04-13"
503,142248,"Verifying Invariants Using Theorem Proving","Hassen Saidi; Susanne Graf;",". Our goal is to use a theorem prover in order to verify invariance properties of distributed systems in a ""model checking like"" manner. A system S is described by a set of sequential components, each one given by a transition relation and a predicate Init defining the set of initial states. In order to verify that P is an invariant of S, we try to compute, in a model checking like manner, the weakest predicate P 0 stronger than P and weaker than Init which is an inductive invariant, that is, whenever P 0 is true in some state, then P 0 remains true after the execution of any possible transition. The fact that P is an invariant can be expressed by a set of predicates (having no more quantifiers than P ) on the set of program variables, one for every possible transition of the system. In order to prove these predicates, we use either automatic or assisted theorem proving depending on their nature. We show in this paper how this can be done in an efficient way using the Prototype V...","1996-05-10"
504,142710,"Learning to Predict by the Methods of Temporal Differences","Richard S. Sutton;",". This article introduces a class of incremental learning procedures specialized for prediction---that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied ar...","1995-03-07"
505,143383,"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation","Andrew W. Moore; Oded Maron;","Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memory based learning algorithms, but we also argue that it is applicable to any class of model selection problems. 1 Introduction Model selection addresses ""high level"" decisions about how best to tune learning algorithm architectures for particular tasks. Such decisions include which...","1995-11-14"
506,143562,"""On the Fly"" Verification of Behavioural Equivalences and Preorders","null","This paper describes decision procedures for bisimulation and simulation relations between two transition systems. The algorithms proposed here do not need to previously construct them: the verification can be performed during their generation. In addition, a diagnosis is computed when the two transitions systems are not equivalent. 1 Introduction One of the successful approaches used for the verification of systems of communicating processes is provided by behavioral equivalence and preorder relations, which allow to compare different descriptions of a given system. More precisely, if we note S (Specification) the most abstract description of the system and I (Implementation) the most detailed one, it is possible to check whether I is in fact an implementation of S in the following manner: from S and I , generate two Labeled Transition Systems (LTS for short) S 1 and S 2 . Let R be an appropriate equivalence relation or preorder relation on LTS. Then, I implements S if and only if S ...","1998-09-01"
507,144409,"Learning Classification Trees","Wray Buntine;","Algorithms for learning classification trees have had successes in artificial intelligence and statistics over many years. This paper outlines how a tree learning algorithm can be derived using Bayesian statistics. This introduces Bayesian techniques for splitting, smoothing, and tree averaging. The splitting rule is similar to Quinlan's information gain, while smoothing and averaging replace pruning. Comparative experiments with reimplementations of a minimum encoding approach, Quinlan's C4 (Quinlan et al., 1987) and Breiman et al.'s CART (Breiman et al., 1984) show the full Bayesian algorithm can produce more accurate predictions than versions of these other approaches, though pay a computational price. Date: November 10, 1991 Keywords: Classification trees, Bayesian statistics, CART, ID3 Publication: This paper is a final draft submitted for publication to the Statistics and Computing journal; a version with some minor changes appeared in Volume 2, 1992, pages 63--73. Contents ...","1995-09-17"
508,144751,"An Automata-Theoretic Approach to Presburger Arithmetic Constraints (Extended Abstract)","null",") Pierre Wolper and Bernard Boigelot ? Universit'e de Li`ege, Institut Montefiore, B28, 4000 Li`ege Sart Tilman, Belgium. Email : fpw,boigelotg@montefiore.ulg.ac.be Appears in: Proc. of Static Analysis Symposium, Glasgow, Lecture Notes in Computer Science, Springer-Verlag, September, 1995. Abstract. This paper introduces a finite-automata based representation of Presburger arithmetic definable sets of integer vectors. The representation consists of concurrent automata operating on the binary encodings of the elements of the represented sets. This representation has several advantages. First, being automata-based it is operational in nature and hence leads directly to algorithms, for instance all usual operations on sets of integer vectors translate naturally to operations on automata. Second, the use of concurrent automata makes it compact. Third, it is insensitive to the representation size of integers. Our representation can be used whenever arithmetic constraints are needed. To il...","1999-03-05"
509,145069,"Practical Issues in Graphical Constraints","Michael Gleicher;","Use of constraint-based techniques in interactive graphics applications poses a variety of unique challenges to system implementors. This paper begins by describing how interface concerns create demands on interactive, constraint-based, graphical applications. We will discuss why such applications must be able to handle systems of non-linear constraints, and survey some of the techniques available to solve them. Employing these numerical algorithms in the contexts of interactive systems provides a set of challenges, including dynamically setting up the equations to be solved and achieving adequate performance and scalability. This paper will explore these issues and describe the methods we have used in our efforts to address them. 1 Introduction The ability to represent and maintain relationships among objects can be an extremely useful tool in a graphical application. Since the earliest interactive graphical applications[25], the use of such constraint techniques has been demonstrat...","1993-05-10"
510,145363,"Relating Linear and Branching Model Checking","M. Y. Vardi; O. Kupferman;","The difference in the complexity of branching and linear model checking has been viewed as an argument in favor of the branching paradigm. In particular, the computational advantage of CTL model checking over LTL model checking makes CTL a popular choice, leading to efficient model-checking tools for this logic. Can we use these tools in order to verify linear properties? In this paper we relate branching and linear model checking. With each LTL formula /, we associate a CTL formula /A that is obtained from / by preceding each temporal operator by the universal path quantifier A. We first describe a number of attempts to utilize the tight syntactic relation between / and /A in order to use CTL model-checking tools in the process of checking the formula /. Neither attempt, however, suggests a method that is guaranteed to perform better than usual LTL model checkers. We then claim that, in practice, LTL model checkers perform nicely on formulas with equivalences of CTL. In fact, they oft...","1998-02-22"
511,145483,"Test Generation for Intelligent Networks Using Model Checking","Loe Feijs; Sjouke Mauw;","We study the use of model checking techniques for the generation of test sequences. Given a formal model of the system to be tested, one can formulate test purposes. A model checker then derives test sequences that fulfill these test purposes. The method is demonstrated by applying it to a specification of an Intelligent Network with two features.","1999-01-28"
512,145612,"Efficient OBDD-Based Boolean Manipulation in CAD Beyond Current Limits","Christoph Meinel; Fachbereich Iv; Jochen Bern; Mathematik Informatik; Trierer Forschungsberichte;","We present the concept of TBDD's which considerably enlarges the class of Boolean functions that can be efficiently manipulated in terms of small size OBDD's. This is done by applying the concept of domain transformations, which is well-known in many areas of mathematics, physics, and technical sciences, to the context of BDD-- based Boolean function manipulation in CAD: Instead of working with the OBDDrepresentation of a function f , TBDD's allow to work with an OBDD-representation of a suited cube transformed version of f . Besides of giving some theoretical insights into the new concept, we investigate in some detail cube transformations which are based on complete types. We ffl show that such TBDD--representations can be derived similarly as OBDD-- representations, ffl give evidence of the practical importance of such TBDD's by presenting very small-size TBDD-representations of the hidden weighted bit functions HWBn which were proved to have only very large OBDD-representations,...","1998-12-15"
513,145703,"Weak Alternating Automata and Tree Automata Emptiness","Moshe Y. Vardi; Orna Kupferman;","Automata on infinite words and trees are used for specification and verification of nonterminating programs. The verification and the satisfiability problems of specifications can be reduced to the nonemptiness problem of such automata. In a weak automaton, the state space is partitioned into partially ordered sets, and the automaton can proceed from a certain set only to smaller sets. Reasoning about weak automata is easier than reasoning about automata with no restricted structure. In particular, the nonemptiness problem for weak alternating automata over a singleton alphabet can be solved in linear time. Known translations of alternating automata to weak alternating automata involve determinization, and therefore involve a double exponential blow-up. In this paper we describe simple and efficient translations, which circumvent the need for determinization, of parity and Rabin alternating word automata to weak alternating word automata. Beyond the independent interest of such transla...","1998-03-01"
514,145724,"Rewrite Systems for Integer Arithmetic","H. R. Walters H. Zantema;","We present three term rewrite systems for integer arithmetic with addition, multiplication, and, in two cases, subtraction. All systems are ground confluent and terminating; termination is proved by semantic labelling and recursive path order. The first system represents numbers by successor and predecessor. In the second, which defines non-negative integers only, digits are represented as unary operators. In the third, digits are represented as constants. The first and the second system are complete; the second and the third system have logarithmic space and time complexity, and are parameterized for an arbitrary radix (binary, decimal, or other radices). Choosing the largest machine representable single precision integer as radix, results in unbounded arithmetic with machine efficiency. Key Words & Phrases: integers, term rewriting, specification languages, formal semantics, confluence, termination. 1991 CR Categories: D.1.1 [Programming Techniques]: Applicative (Functional) Progra...","1994-10-12"
515,145880,"Feature Subset Selection as Search with Probabilistic Estimates","Ron Kohavi;","Irrelevant features and weakly relevant features may reduce the comprehensibility and accuracy of concepts induced by supervised learning algorithms. We formulate the search for a feature subset as an abstract search problem with probabilistic estimates. Searching a space using an evaluation function that is a random variable requires trading off accuracy of estimates for increased state exploration. We show how recent feature subset selection algorithms in the machine learning literature fit into this search problem as simple hill climbing approaches, and conduct a small experiment using a best-first search technique. 1 Introduction Practical algorithms in supervised machine learning degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output. An important question in the field of machine learning, statistics, and pattern recognition, is how to select a good subset set of features. From a theoretical standpoint,...","1997-08-25"
516,145920,"The Complexity of the Optimal Variable Ordering Problems of A Shared Binary Decision Diagram","null","A binary decision diagram (BDD) is a directed acyclic graph for representing a Boolean function. BDD's are widely used in various areas which require Boolean function manipulation, since BDD's can represent efficiently many of practical Boolean functions and have other desirable properties. However the complexity of constructing BDD's has hardly been researched theoretically. In this report, we prove that the optimal variable ordering problem of a shared BDD is NP-complete, and touch on the approximation hardness of this problem and related problems of BDD's. ANY OTHER IDENTIFYING INFORMATION OF THIS REPORT This report is a refined version of the paper which will appear in ISAAC'93. DISTRIBUTION STATEMENT First Issue 50 copies SUPPLEMENTARY NOTES REPORT DATE December, 1993 TOTAL NO. OF PAGES 18 WRITTEN LANGUAGE English NO. OF REFERENCES 9 DEPARTMENT OF INFORMATION SCIENCE Faculty of Science, University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo, 113 Japan The Complexity of the Op...","1997-12-02"
517,146328,"Multi-criteria Reinforcement Learning","Konkoly Thege M. Ut;","We consider multi-criteria sequential decision making problems where the vector-valued evaluations are compared by a fixed total ordering of the vectors. Conditions for the optimality of stationary policies and the Bellman optimality equation are given for a special, but important class of problems, when the evaluation of policies can be computed componentwise. The analysis requires special care as the topology introduced by pointwise convergence and the order-topology introduced by the preference order are in general incompatible several. Reinforcement learning algorithms are then proposed and analyzed. Preliminary computer experiments confirm the validity of the derived algorithms. These type of multi-criteria problems are most useful when there are several optimal solutions to a problem and one wants to choose the one among these which is optimal according to another fixed criterion. Possible application in robotics and repeated games are outlined. 1 Introduction Scalar-valued rein...","1998-10-12"
518,146468,"Automatic Test Generation for the Analysis of a Real-Time System: Case Study","Duncan Clarke; Insup Lee; Young Si Kim;","We present a framework for testing timing constraints of real-time systems. Our tests are automatically derived from specifications of minimum and maximum allowable delays between input/output events in the execution of a system. Our test derivation scheme uses a graphical specification formalism for timing constraints, and the real-time process algebra Algebra of Communicating Shared Resources (ACSR) for representing tests and process models. The use of ACSR to describe test sequences has two main advantages. First, tests can be applied to an ACSR model of the software system within the ACSR semantic framework for model validation purposes. Second, ACSR has concise notation and a precise semantics that will facilitate the translation of real-time tests into a software test language for software validation purposes. The major benefit of our approach is that it can be used to validate a design specification which has too many states for exhaustive state space exploration based analysis....","1997-01-28"
519,146952,"Pattern Recognition Via Linear Programming: Theory And Application To Medical Diagnosis","O. L. Mangasariany; R. Setiono; W. H. Wolberg;",". A decision problem associated with a fundamental nonconvex model for linearly inseparable pattern sets is shown to be NP-complete. Another nonconvex model that employs an 1Gamma norm instead of the 2-norm, can be solved in polynomial time by solving 2n linear programs, where n is the (usually small) dimensionality of the pattern space. An effective LP-based finite algorithm is proposed for solving the latter model. The algorithm is employed to obtain a nonconvex piecewise-linear function for separating points representing measurements made on fine needle aspirates taken from benign and malignant human breasts. A computer program trained on 369 samples has correctly diagnosed each of 45 new samples encountered and is currently in use at the University of Wisconsin Hospitals. 1. Introduction. The fundamental problem we wish to address is that of distinguishing between elements of two distinct pattern sets. Mathematically we can formulate the problem as follows. Given two disjoint fin...","1999-02-10"
520,147460,"Minimization of Timed Transition Systems","C. Courcoubetis; D. Dill; H. Wong-toi; N. Halbwachs; R. Alur;","this paper, we show how to apply state-minimization techniques to verification algorithms for real-time systems. We use timed automata as a representation of real-time systems [11, 2]. A timed automaton provides a way of annotating a state-transition graph of the system with timing constraints. It operates with a finite-state control and a finite number of fictitious time-measuring elements called","1996-08-06"
521,147542,"Theory and Applications of Agnostic PAC-Learning with Small Decision Trees","Peter Auer; Robert C. Holte; Technische Universitaet Graz; Wolfgang Maass;","We exhibit a theoretically founded algorithm T2 for agnostic PAC-learning of decision trees of at most 2 levels, whose computation time is almost linear in the size of the training set. We evaluate the performance of this learning algorithm T2 on 15 common ""real-world"" datasets, and show that for most of these datasets T2 provides simple decision trees with little or no loss in predictive power (compared with C4.5). In fact, for datasets with continuous attributes its error rate tends to be lower than that of C4.5. To the best of our knowledge this is the first time that a PAC-learning algorithm is shown to be applicable to ""real-world"" classification problems. Since one can prove that T2 is an agnostic PAClearning algorithm, T2 is guaranteed to produce close to optimal 2-level decision trees from sufficiently large training sets for any (!) distribution of data. In this regard T2 differs strongly from all other learning algorithms that are considered in applied machine learning, for w...","1998-11-06"
522,147583,"A Data Parallel Algorithm for Boolean Function Manipulation","M. Rebaudengo; M. Sonza Reorda; S. Gai;","* This paper describes a data-parallel algorithm for boolean function manipulation. The algorithm adopts Binary Decision Diagrams (BDDs), which are the state-of-the-art approach for representing and handling boolean functions. The algorithm is well suited for SIMD architectures and is based on distributing BDD nodes to the available Processing Elements and traversing BDDs in a breadth-first manner. An improved version of the same algorithm is also presented, which does not use virtual processors. A prototypical package has been implemented and its behavior has been studied with two different applications. In both cases the results show that the approach exploits well the parallel hardware by effectively distributing the load; thanks to the limited CPU time required and to the great amount of memory available, it can solve problems that can not be faced with by conventional architectures. 1. Introduction Efficient techniques for boolean function manipulation are a key point in many a...","1996-01-18"
523,148287,"Sentinel Scheduling for VLIW and Superscalar Processors","B. Ramakrishna; Rau Michael; S. Schlansker; Scott A. Mahlke; Wen-mei W. Hwu; William Y. Chen;","Speculative execution is an important source of parallelism for VLIW and superscalar processors. A serious challenge with compiler-controlled speculative execution is to accurately detect and report all program execution errors at the time of occurrence. In this paper, a set of architectural features and compile-time scheduling support referred to as sentinel scheduling is introduced. Sentinel scheduling provides an effective framework for compiler-controlled speculative execution that accurately detects and reports all exceptions. Sentinel scheduling also supports speculative execution of store instructions by providing a store buffer which allows probationary entries. Experimental results show that sentinel scheduling is highly effective for a wide range of VLIW and superscalar processors. 1 Introduction Instruction level parallelism (ILP) within basic blocks is extremely limited. An effective VLIW or superscalar machine must schedule instructions across basic block boundaries to ac...","1992-08-12"
524,148879,"Yacc: Yet Another Compiler-Compiler","Stephen C. Johnson;","Computer program input generally has some structure; in fact, every computer program that does input can be thought of as defining an ""input language"" which it accepts. An input language may be as complex as a programming language, or as simple as a sequence of numbers. Unfortunately, usual input facilities are limited, difficult to use, and often are lax about checking their inputs for validity. Yacc provides a general tool for describing the input to a computer program. The Yacc user specifies the structures of his input, together with code to be invoked as each such structure is recognized. Yacc turns such a specification into a subroutine that handles the input process; frequently, it is convenient and appropriate to have most of the flow of control in the user's application handled by this subroutine. The input subroutine produced by Yacc calls a user-supplied routine to return the next basic input item. Thus, the user can specify his input in terms of individual input characters,...","1996-07-31"
525,149505,"Verification of the Futurebus+ Cache Coherence Protocol","D. Long; E. Clarke; H. Hiraishi; K. Mcmillan; L. Ness; O. Grumberg; S. Jha;","We used a hardware description language to construct a formal model of the cache coherence protocol described in the draft IEEE Futurebus+ standard. By applying temporal logic model checking techniques, we found several errors in the standard. The result of our project is a concise, comprehensible and unambiguous model of the protocol that should be useful both to the Futurebus+ Working Group members, who are responsible for the protocol, and to actual designers of Futurebus+ boards. 1 Introduction This paper describes the formalization and verification of the cache coherence protocol described in the draft IEEE Futurebus+ standard (IEEE Standard 896.1--1991) [8]. We constructed a precise model of the protocol in a hardware description language and then used temporal logic model checking to show that the model satisfied a formal specification of cache coherence. In the process of formalizing and verifying the protocol, we discovered a number of errors and ambiguities. We believe tha...","1996-09-16"
526,149673,"Back to the Future: Towards a Theory of Timed Regular Languages","Rajeev Alur; Thomas A. Henzinger;",": Timed automata are finite-state machines constrained by timing requirements so that they accept timed words --- words in which every symbol is labeled with a real-valued time. These automata were designed to lead to a theory of finite-state real-time properties with applications to the automatic verification of real-time systems. However, both deterministic and nondeterministic versions suffer from drawbacks: several key problems, such as language inclusion, are undecidable for nondeterministic timed automata, whereas deterministic timed automata lack considerable expressive power when compared to decidable real-time logics. This is why we introduce two-way timed automata --- timed automata that can move back and forth while reading a timed word. Two-wayness in its unrestricted form leads, like nondeterminism, to the undecidability of language inclusion. However, if we restrict the number of times an input symbol may be revisited, then two-wayness is both harmless and desirable. We s...","1997-06-17"
527,149820,"The Anchored Version of the Temporal Framework","Amir Pnueli; Zohar Manna;",". In this survey paper we present some of the recent developments in the temporal formal system for the specification, verification and development of reactive programs. While the general methodology remains very much the one presented in some earlier works on the subject, such as [MP83c, MP83a, Pnu86], there have been several technical improvements and gained insights in understanding the computational model, the logic itself, the proof system and its presentation, and connections with alternative formalisms, such as finite automata. In this paper we explicate some of these improvements and extensions. The main difference between this and preceding versions is that here we consider a notion of validity for temporal formulae, which is anchored at the initial state of the computation. The paper discusses some of the consequences of this decision. Key words: Temporal Logic, Reactive Systems, Concurrent Programs, Specification, Verification, Proof System, Classification of Prtoperties, Sa...","1995-09-08"
528,150450,"UTSE: Construction of Optimum Timetables for University Courses - A CLP Based Approach","Harikleia Frangouli; Panagiotis Stamatopoulos; Vassilis Harmandas;","The construction of timetables for universities or schools is an extremely complex problem, whose manual solution requires much effort. The set of all possible solutions, that is the search space of the problem, is very large, at least in the realworld examples. An acceptable solution is one that satisfies all the problem constraints. The problem becomes even more difficult if someone wants to generate an optimum timetable according to some heuristic criteria. Various attempts have been made so far on the automatic solving of the timetabling problem by a computer. In this paper, a method is proposed for the construction of optimum timetables for university courses. A specific system is presented which has been used for the timetabling procedure of the Department of Informatics of the University of Athens. The software platform of the implementation is an instance of the Constraint Logic Programming class of languages, the ECL i PS e system. ECL i PS e is proved to be an appropr...","1997-07-23"
529,150502,"The MONK's Problems A Performance Comparison of Different Learning Algorithms","And J. Zhang; B. Cestnik; D. Fisher; E. Bloedorn; H. Vafaie; I. Bratko; I. Kononenko; J. Bala; J. Cheng; J. Kreuziger; J. Wnek; K. De Jong; K. Kaufman; P. Pachowicz; R. Hamann; S. B. Thrun; S. Dzeroski; S. E. Fahlman; S. Keller; T. Mitchell; W. Van De Welde; W. Wenzel; Y. Reich;","This report summarizes a comparison of different learning techniques which was performed at the 2nd European Summer School on Machine Learning, held in Belgium during summer 1991. A variety of symbolic and non-symbolic learning techniques -- namely AQ17-DCI, AQ17-HCI, AQ17FCLS, AQ14-NT, AQ15-GA, Assistant Professional, mFOIL, ID5R, IDL, ID5R-hat, TDIDT, ID3, AQR, CN2, CLASS WEB, ECOBWEB, PRISM, Backpropagation, and Cascade Correlation -- are compared on three classification problems, the MONK's problems. The MONK's problems are derived from a domain in which each training example is represented by six discrete-valued attributes. Each problem involves learning a binary function defined over this domain, from a sample of training examples of this function. Experiments were performed with and without noise in the training examples. One significant characteristic of this comparison is that it was performed by a collection of researchers, each of whom was an advocate of the technique they t...","1998-09-29"
530,150689,"An Action-Based Framework for Verifying Logical and Behavioural Properties of Concurrent Systems","A. Fantechi; R. De Nicola; S. Gnesi;","A system is described which supports proving both behavioural and logical properties of concurrent systems; these are specified by means of a process algebra and its associated logic. The logic is an action based version of the branching time logic CTL, which we call ACTL. It is interpreted over transition labelled structures while CTL is interpreted over state labelled ones. The core of the system are two existing tools, AUTO and EMC. The first builds the labelled transition system corresponding to a term of a process algebra and permits proof of equivalence and simplification of terms, while the second checks the validity of CTL logical formulae. The integration is realized by means of two translation functions from the action based branching time logic ACTL to CTL and from transition-labelled to state-labelled structures. The correctness of the integration is guaranteed by the proof that the two translation functions when coupled preserve satisfiability of logical formulae. 1. Intro...","1997-01-29"
531,151235,"A Laboratory for a Digital Design Course Using FPGAs","Niklaus Wirth; Stefan Ludwig; Stephan Gehring;",". In our digital design laboratory we have replaced the traditional wired circuit modules by workstations equipped with an extension board containing a single FPGA. This hardware is supplemented with a set of software tools consisting of a compiler for the circuit specification language Lola, a graphical layout editor for design entry, and a checker to verify conformity of a layout with its specification in Lola. The new laboratory has been used with considerable success in digital design courses for computer science students. Not only is this solution much cheaper than collections of modules to be wired, but it also allows for more substantial and challenging exercises. 1 Introduction In order to demonstrate that what had been learnt in the classroom can actually be materialized into useful, correctly operating circuits, digital circuit design courses are accompanied by exercises in the laboratory. There, students select building elements from an available collection and assemble cir...","1996-10-21"
532,152632,"A Formal Specification Model for Hardware/Software Codesign","Alberto Sangiovanni-vincentelli; Attila Jurecska; Harry Hsieh; Luciano Lavagno; Massimiliano Chiodo; Paolo Giusto;","Embedded controllers for reactive real-time applications are implemented as mixed software hardware systems. In this paper we present a model for specification, partitioning, and implementation of such systems. The model, called Codesign Finite State Machines (CFSMs), is based on FSMs and is particularly suited to a specific class of systems with relatively low algorithmic complexity. Pre-existing formal specification languages can be used by the designer to specify the intended behavior of the system and mapped into our model. CFSMs use a non-zero unbounded reaction delay model and hence can be indifferently implemented either in hardware or in software. The implementation only restricts the range of variation of some previously undefined delays, thus preserving formal properties of the specification across implementation refinements. The communication primitive, event broadcasting, is low-level enough to be implemented efficiently and yet general enough to allow higher-level mechanism...","1996-04-17"
533,152694,"Learning Boolean Concepts in the Presence of Many Irrelevant Features","Hussein Almuallim; Thomas G. Dietterich;","In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias in Boolean domains. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires Theta( 1 ffl ln 1 ffi + 1 ffl [2 p + p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. For implementing the MIN-FEATURES bias, the paper presents five algorithms that identify a subset of features sufficient to construct a hypothesis consistent with the training examples. FOCUS-1 is a straightforward algorithm that returns a minimal and sufficient subset of features in quasi-polynomial time. FOCUS-2 does the same task as FOCUS-1 but is empirically shown to be substantially faster than FOCUS-1. Finally, the Simple-Greedy, Mutual-Information-G...","1994-09-13"
534,153638,"Selecting a Classification Method by Cross-Validation","Cullen Schaffer;","If we lack relevant problem-specific knowledge, cross-validation methods may be used to select a classification method empirically. We examine this idea here to show in what senses cross-validation does and does not solve the selection problem. As illustrated empirically, cross-validation may lead to higher average performance than application of any single classification strategy and it also cuts the risk of poor performance. On the other hand, cross-validation is no more or less a form of bias than simpler strategies and applying it appropriately ultimately depends in the same way on prior knowledge. In fact, cross-validation may be seen as a way of applying partial information about the applicability of alternative classification strategies. Keywords: Cross-validation, classification, decision trees, neural networks. 1 Introduction Machine learning researchers and statisticians have produced a host of approaches to the problem of classification including methods for inducing rul...","1994-07-05"
535,154735,"Analyzing a Heuristic Strategy for Constraint-Satisfaction and Scheduling","Mark D. Johnston; Steven Minton;","This paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. On the n-queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. The technique has also been used for scheduling the Hubble Space telescope. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be most effective. 1 Introduction One of the most promising general approaches for solving combinatorial search problems is to generate an initial...","1998-10-09"
536,155019,"Symbolic Verification with Periodic Sets","Bernard Boigelot;",". Symbolic approaches attack the state explosion problem by introducing implicit representations that allow the simultaneous manipulation of large sets of states. The most commonly used representation in this context is the Binary Decision Diagram (BDD). This paper takes the point of view that other structures than BDD's can be useful for representing sets of values, and that combining implicit and explicit representations can be fruitful. It introduces a representation of complex periodic sets of integer values, shows how this representation can be manipulated, and describes its application to the state-space exploration of protocols. Preliminary experimental results indicate that the method can dramatically reduce the resources required for state-space exploration. 1 Introduction Verification by state-space exploration is an old technique [Wes78] whose many advantages have long been outweighed by its main drawback: the state explosion problem. However, in recent years, this central ...","1999-03-05"
537,155080,"An introduction to Kolmogorov Complexity and its Applications: Preface to the First Edition","null","This document has been prepared using the L a T E X system. We thank Donald Knuth for T E X, Leslie Lamport for L a T E X, and Jan van der Steen at CWI for online help. Some figures were prepared by John Tromp using the xpic program. The London Mathematical Society kindly gave permission to reproduce a long extract by A.M. Turing. The Indian Statistical Institute, through the editor of Sankhy¯a, kindly gave permission to quote A.N. Kolmogorov. We gratefully acknowledge the financial support by NSF Grant DCR8606366, ONR Grant N00014-85-k-0445, ARO Grant DAAL03-86-K0171, the Natural Sciences and Engineering Research Council of Canada through operating grants OGP-0036747, OGP-046506, and International Scientific Exchange Awards ISE0046203, ISE0125663, and NWO Grant NF 62-376. The book was conceived in late Spring 1986 in the Valley of the Moon in Sonoma County, California. The actual writing lasted on and off from autumn 1987 until summer 1993. One of us [PV] gives very special thanks to his lovely wife Pauline for insisting from the outset on the significance of this enterprise. The Aiken Computation Laboratory of Harvard University, Cambridge, Massachusetts, USA; the Computer Science Department of York University, Ontario, Canada; the Computer Science Department of the University xii of Waterloo, Ontario, Canada; and CWI, Amsterdam, the Netherlands provided the working environments in which this book could be written. Preface to the Second Edition","1997-09-02"
538,155235,"Symbolic Model Checking without BDDs","Alessandro Cimatti; Armin Biere; Edmund Clarke; Yunshan Zhu;","Symbolic Model Checking [3, 14] has proven to be a powerful technique for the verification of reactive systems. BDDs [2] have traditionally been used as a symbolic representation of the system. In this paper we show how boolean decision procedures, like Stalmarck's Method [16] or the Davis & Putnam Procedure [7], can replace BDDs. This new technique avoids the space blow up of BDDs, generates counterexamples much faster, and sometimes speeds up the verification. In addition, it produces counterexamples of minimal length. We introduce a bounded model checking procedure for LTL which reduces model checking to propositional satisfiability. We show that bounded LTL model checking can be done without a tableau construction. We have implemented a model checker BMC, based on bounded model checking, and preliminary results are presented. 1 Introduction Model checking [4] is a powerful technique for verifying reactive systems. Able to find subtle errors in real commercial designs, it is g...","1999-01-14"
539,155792,"Logics and Models of Real Time: A Survey","Rajeev Alur; Thomas A. Henzinger;",". We survey logic-based and automata-based languages and techniques for the specification and verification of real-time systems. In particular, we discuss three syntactic extensions of temporal logic: time-bounded operators, freeze quantification, and time variables. We also discuss the extension of finite-state machines with clocks and the extension of transition systems with time bounds on the transitions. All of the resulting notations can be interpreted over a variety of different models of time and computation, including linear and branching time, interleaving and true concurrency, discrete and continuous time. For each choice of syntax and semantics, we summarize the results that are known about expressive power, algorithmic finite-state verification, and deductive verification. 1 Introduction The number of formalisms that purportedly facilitate the modeling, specifying, and proving of timing properties for reactive systems has exploded over the past few years. The authors, who ...","1997-06-17"
540,156791,"Data-Structures for the Verification of Timed Automata","Alain Kerbrat; Amir Pnueli; Anne Rasse; Eugene Asarin; Marius Bozga; Oded Maler;",". In this paper we suggest numerical decision diagrams, a bdd- based data-structure for representing certain subsets of the Euclidean space, namely those encountered in verification of timed automata. Unlike other representation schemes, ndd's are canonical and provide for all the necessary operations needed in the verification and synthesis of timed automata. We report some preliminary experimental results. 1 Introduction Consider a transition system A = (Q; ffi), where Q is the set of states and ffi : Q 7! 2 Q is a transition function, mapping each state q 2 Q into the set of q-successors ffi(q) ` Q. The problem of calculating or characterizing all the states reachable from a subset F ` Q of the state-space is one of the central problems in verification. The basic algorithm to calculate this set of states is the following: F 0 := F for i = 0; 1; : : : ; repeat F i+1 := F i [ ffi(F i ) until F i+1 = F i where ffi(F i ) = S q2F i ffi(q). Symbolic methods [BCM + 93], [M...","1997-04-30"
541,157215,"Finding Optimal Solutions to Rubik's Cube Using Pattern Databases","Richard E. Korf;","We have found the first optimal solutions to random instances of Rubik's Cube. The median optimal solution length appears to be 18 moves. The algorithm used is iterative-deepening-A* (IDA*), with a lowerbound heuristic function based on large memory-based lookup tables, or ""pattern databases"" (Culberson and Schaeffer 1996). These tables store the exact number of moves required to solve various subgoals of the problem, in this case subsets of the individual movable cubies. We characterize the effectiveness of an admissible heuristic function by its expected value, and hypothesize that the overall performance of the program obeys a relation in which the product of the time and space used equals the size of the state space. Thus, the speed of the program increases linearly with the amount of memory available. As computer memories become larger and cheaper, we believe that this approach will become increasingly cost-effective. Introduction 1 Rubik's Cube, invented in the late 1970s by Ern...","1997-05-30"
542,157742,"Model Checking of Hierarchical State Machines","Mihalis Yannakakis; Rajeev Alur;","Model checking is emerging as a practical tool for detecting logical errors in early stages of system design. We investigate the model checking of hierarchical (nested) systems, i.e. finite state machines whose states themselves can be other machines. This nesting ability is common in various software design methodologies and is available in several commercial modeling tools. The straightforward way to analyze a hierarchical machine is to flatten it (thus, incurring an exponential blow up) and apply a model checking tool on the resulting ordinary FSM. We show that this flattening can be avoided. We develop algorithms for verifying linear time requirements whose complexity is polynomial in the size of the hierarchical machine. We address also the verification of branching time requirements and provide efficient algorithms and matching lower bounds. 1 Introduction Finite state machines (FSMs) are widely used in the modeling of systems in various areas. Descriptions using FSMs are useful...","1998-07-22"
543,157769,"Test Pattern Generation Using Boolean Satisfiability","Tracy Larrabee;","This article describes the Boolean satisfiability method for generating test patterns for single stuck-at faults in combinational circuits. This new method generates test patterns in two steps: First, it constructs a formula expressing the Boolean difference between the unfaulted and faulted circuits. Second, it applies a Boolean satisfiability algorithm to the resulting formula. This approach differs from previous methods now in use, which search the circuit structure directly instead of constructing a formula from it. The new method is general and effective: it allows for the addition of heuristics used by structural search methods, and it has produced excellent results on popular test pattern generation benchmarks. 1 Introduction To produce reliable computer systems, defect-free components must be available. Automatic test pattern generation (ATPG) systems distinguish defective components from defect-free components by generating input sets that cause the outputs of a component und...","1996-03-19"
544,158402,"Useful Feature Subsets and Rough Set Reducts","Brian Frasca; Ron Kohavi;","In supervised classification learning, one attempts to induce a classifier that correctly predicts the label of novel instances. We demonstrate that by choosing a useful subset of features for the indiscernibility relation, an induction algorithm based on simple decision table can have high prediction accuracy on artificial and real-world datasets. We show that useful feature subsets are not necessarily maximal independent sets (relative reducts) with respect to the label, and that, in practical situations, using a subset of the relative core features may lead to superior performance. 1 Introduction In supervised classification learning, one is given a training set containing labelled instances (examples) . Each labelled instance contains a list of feature values (attribute values) and a discrete label value. The induction task is to build a classifier that will correctly predict the label of novel instances. Common classifiers are decision trees, neural networks, and nearest-neighbor...","1997-08-25"
545,159038,"Global Tree Optimization: A Non-greedy Decision Tree Algorithm","Kristin P. Bennett;","A non-greedy approach for constructing globally optimal multivariate decision trees with fixed structure is proposed. Previous greedy tree construction algorithms are locally optimal in that they optimize some splitting criterion at each decision node, typically one node at a time. In contrast, global tree optimization explicitly considers all decisions in the tree concurrently. An iterative linear programming algorithm is used to minimize the classification error of the entire tree. Global tree optimization can be used both to construct decision trees initially and to update existing decision trees. Encouraging computational experience is reported. 1 Introduction Global Tree Optimization (GTO) is a new approach for constructing decision trees that classify two or more sets of n-dimensional points. The essential difference between this work and prior decision tree algorithms (e.g. CART [5] and ID3 [10]) is that GTO is non-greedy. For greedy algorithms, the ""best"" decision at each node...","1995-03-07"
546,161139,"Bit-Level Abstraction in the Verification of Pipelined Microprocessors by Correspondence Checking","null","We present a way to abstract functional units in symbolic simulation of actual circuits, thus achieving the effect of uninterpreted functions at the bit-level. Additionally, we propose an efficient encoding technique that can be used to represent uninterpreted symbols with BDDs, while allowing these symbols to be propagated by simulation with a conventional bit-level symbolic simulator. Our abstraction and encoding techniques result in an automatic symmetry reduction and allow the control and forwarding logic of the actual circuit to be used unmodified. The abstraction method builds on the behavioral Efficient Memory Model [18] [19] and its capability to dynamically introduce consistent initial state, which is identical for two simulation sequences. We apply the abstraction and encoding ideas on the verification of pipelined microprocessors by correspondence checking, where a pipelined microproc...","1999-03-11"
547,162183,"Using Logic Programming And Coroutining For Electronic Cad","And Andreas; Neumann We Prolog; Ulrich Bieker;","this paper we describe the use of Prolog for a very high level of abstraction. A very elegant simulator, based on a hardware description language and a suitable Prolog circuit representation based on trees is presented. The simulator is able to simulate a processor together with a given microprogram. We also present a concept to generate microcode for a given hardware structure which can be used to test the processor. The part of the MSS system concerning this paper is shown in figure 1.1.","1998-06-17"
548,162240,"Control Path Oriented Verification of Sequential Generic Circuits with Control and Data Path","K. Schneider; R. Kumar; Th. Kropf;","Usually, digital circuits are split up into control and data path as there are specific synthesis methods for controllers and operation units. However, all known approaches to hardware verification which make use of this fact, model the operation unit also as a finite-state machine. This leads to enormous space requirements which limit the applicability of these approaches. In order to avoid this, abstraction mechanisms can be used to map boolean tuples onto more complex data types. However, approaches to the verification of generic n-bit circuits have considered so far only circuits with simple controllers, such that the verification of only combinational circuits or special cases of sequential circuits is possible. In this paper, we present a new approach to hardware verification which allows the verification of generic circuits with non-trivial controllers. 1 Introduction Over the last few years, a lot of formal approaches to hardware verification have been developed, e.g. equival...","1997-12-19"
549,162688,"Hierarchical Constraint Logic Programming","Alan Borning; Molly Wilson;","Constraint Logic Programming (CLP) is a general scheme for extending logic programming to include constraints. It is parameterized by D, the domain of the constraints. However, CLP(D) languages, as well as most other constraint systems, only allow the programmer to specify constraints that must hold. In many applications, such as interactive graphics, planning, document formatting, and decision support, one needs to express preferences as well as strict requirements. If we wish to make full use of the constraint paradigm, we need ways to represent these defaults and preferences declaratively, as constraints, rather than encoding them in the procedural parts of the language. We describe a scheme for extending CLP(D) to include both required and preferential constraints. An arbitrary number of strengths of preference are allowed. We present a theory of such constraint hierarchies, and an extension, Hierarchical Constraint Logic Programming, of the CLP scheme to include constraint hierarc...","1993-05-24"
550,163118,"A Software-Hardware Cosynthesis Approach to Digital System Simulation","Jeremy Levitt; Kunle Olukotun; Rachid Helaihel; Ricardo Ramirez;","This paper presents a software-hardware based simulation approach to digital system simulation. Our approach provides better performance than a software only approach and also works with high-level system models. The approach uses an optimizing simulation compiler to transform a hardware description language system model into a high performance simulator. The target architecture for the simulation compiler is a tightly coupled processor and field-programmable gate array (FPGA). The components of the simulation compiler are a high performance compiled-code software simulator, an automatic partitioner that partitions the system model between the processor and FPGA, and a scheduler that maximizes concurrent execution within the FPGA and between the FPGA and processor. We describe these components and show how they can be used to improve the performance of synchronous digital system simulation by up to a factor of two when compared to a high performance all software simulator. 1.0 Introduc...","1996-09-11"
551,163353,"Self-Similarity in World Wide Web Traffic: Evidence and Possible Causes","Azer Bestavros; Mark E. Crovella;","Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we show evidence that the subset of network traffic that is due to World Wide Web transfers can show characteristics that are consistent with self-similarity, and we present a hypothesized explanation for that self-similarity. Using a set of traces of actual user executions of NCSA Mosaic, we examine the dependence structure of WWW traffic. First, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user ""think time,"" and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from client traces and from data independently collected at WWW servers.","1998-07-14"
552,163604,"Intelligence Without Representation","Rodney A. Brooks;","Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environ...","1998-05-28"
553,163663,"Model Checking of Probabilistic and Nondeterministic Systems","Luca De Alfaro;",". The temporal logics pCTL and pCTL* have been proposed as tools for the formal specification and verification of probabilistic systems: as they can express quantitative bounds on the probability of system evolutions, they can be used to specify system properties such as reliability and performance. In this paper, we present model-checking algorithms for extensions of pCTL and pCTL* to systems in which the probabilistic behavior coexists with nondeterminism, and show that these algorithms have polynomial-time complexity in the size of the system. This provides a practical tool for reasoning on the reliability and performance of parallel systems. 1 Introduction Temporal logic has been successfully used to specify the behavior of concurrent and reactive systems. These systems are usually modeled as nondeterministic processes: at any moment in time, more than one future evolution may be possible, but a probabilistic characterization of their likelihood is normally not attempted. While ma...","1996-01-31"
554,164141,"Automatic Synthesis of Device Drivers for Hardware/Software Co-design","Elizabeth A. Walkup; Elizabeth Walkup; Gaetano Borriello;","Automatically synthesizing device drivers, the hardware and software needed to interface a device to a processor, is an important element of hardware/software co-design. Driver software consists of the sequences of instructions needed for the processor to control its interactions with the device. Driver hardware consists of the digital logic necessary to physically connect the devices and generate signal events while meeting timing constraints. We describe an approach that begins with device specifications in the form of timing and state diagrams and determines which signals can be controlled directly from software and which require indirect control through intervening hardware. Minimizing this hardware requires solving a simultaneous scheduling and partitioning problem whose goal is to limit the number of wires whose events are not directly generated by the processor software. We show that even the simplest version of this problem is NP-hard and discuss a heuristic solution that shoul...","1994-11-10"
555,164171,"Symbolic Trajectory Evaluation","Carl-johan H. Seger; Scott Hazelhurst;","ion The main problem with model checking is the state explosion problem -- the state space grows exponentially with system size. Two methods have some popularity in attacking this problem: compositional methods and abstraction. While they cannot solve the problem in general, they do offer significant improvements in performance. The direct method of verifying that a circuit has a property f is to show the model M satisfies f . The idea behind abstraction is that instead of verifying property f of model M , we verify property f A of model MA and the answer we get helps us answer the original problem. The system MA is an abstraction of the system M . One possibility is to build an abstraction MA that is equivalent (e.g. bisimilar [48]) to M . This sometimes leads to performance advantages if the state space of MA is smaller than M . This type of abstraction would more likely be used in model comparison (e.g. as in [38]). Typically, the behaviour of an abstraction is not equivalent...","1996-12-24"
556,164217,"Rethinking the Taxonomy of Fault Detection Techniques","Michal Young; Richard N. Taylor;","The conventional classification of software fault detection techniques as static or dynamic analysis is inadequate as a basis for identifying useful relationships between techniques. A more useful distinction is between techniques that sample the space of possible executions, and techniques that fold the space. The new distinction provides better insight into the ways different techniques can interact, and is a basis for considering hybrid fault detection techniques including combinations of testing and formal verification. Keywords: Fault detection, hybrid analysis techniques, static analysis, dynamic analysis. An earlier version of this paper appeared in Proceedings of the 11th International Conference on Software Engineering, Pittsburgh, May 1989. Address correspondence to the first author at Department of Computer Sciences, Purdue University, West Lafayette, IN 47907. Email: young@cs.purdue.edu. Phone: (317) 494-6023. This work was supported in part by the National Science Foundat...","1996-04-05"
557,164433,"Learning With Many Irrelevant Features","Hussein Almuallim; Thomas G. Dietterich;","In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires Theta( 1 ffl ln 1 ffi + 1 ffl [2 p + p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that--- contrary to expectations---these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURE...","1996-07-09"
558,164592,"Efficient Timing Analysis Algorithms for Timed State Space Exploration","Chris J. Myers; Wendy Belluomini;","This paper presents new timing analysis algorithms for efficient state space exploration during timed circuit synthesis. Timed circuits are a class of asynchronous circuits that incorporate explicit timing information in the specification which is used throughout the synthesis procedure to optimize the design. Much of the computational complexity in the synthesis of timed circuits currently is in finding the reachable timed state space. We introduce new algorithms which utilize geometric regions to represent the timed state space and partial orders to minimize the number of regions necessary. These algorithms operate on specifications sufficiently general to describe practical circuits. 1. Introduction There has been a renewed interest in asynchronous circuits in recent years due to their advantages over synchronous circuits in performance and power consumption and as a way to eliminate problems related to clock skew [3, 5, 9, 16, 17]. However many of these advantages are often reduce...","1998-08-03"
559,164633,"Heterogeneous Analysis and Verification for Distributed Systems","Bernhard Steffen; Lehrstuhl Fur Informatik; Tiziana Margaria;","In this paper we present an environment for the development of special purpose heterogeneous analysis and verification tools, which is unique in 1) constituting a framework for the development of application specific heterogeneous tools and 2) providing facilities for the automation of the synthesis process. Based on a specification language that uniformly combines taxonomic component specifications, interface conditions, and ordering constraints, our method adds a global view to conventional single component retrieval. Following a user session, we illustrate the interactive synthesis process, which supports the inclusion of a satisfactory new software component into the repository by proposing an appropriately precomputed default taxonomic classification. This guarantees convenient retrieval for later reuse. 1 Motivation To master the complexity and variety of system-level distributed system designs flexible environments are required, which efficiently support automation, `design in ...","1998-01-09"
560,164643,"Composite Model Checking: Verification with Type-Specific Symbolic Representations","Christopher League; Richard Gerber; Tevfik Bultan;","In recent years, there has been a surge of progress in automated verification methods based on state exploration. In areas like hardware design, these technologies are rapidly augmenting key phases of testing and validation. To date, one of the most successful of these methods has been symbolic model checking, in which large finite-state machines are encoded into compact data structures such as binary decision diagrams (BDDs) -- and are then checked for safety and liveness properties. However, these techniques have not realized the same success on software systems. One limitation is their inability to deal with infinite-state programs -- even those with a single unbounded integer. A second problem is that of finding efficient representations for various variable types. We recently proposed a model checker for integer-based systems that uses arithmetic constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems...","1999-01-14"
561,164788,"Verification of Hardware Descriptions by Retargetable Code Generation","Lothar Nowak; Nixdorf Computer Ag; Peter Marwedel;","This paper 1 proposes a new method for hardware verification. The basic idea is the application of a retargetable compiler as verification tool. A retargetable compiler is able to compile programs into the machine code of a specified hardware (target). If the program is the complete behavioural specification of the target, the compiler can be used to verify that a properly programmed structure implements the behaviour. Methods, algorithms and applications of an existing retargetable compiler are described. 1 Introduction The correctness of digital systems is relatively neglected by current design systems. It will become even more important, because the systems will become more complex and there will be more critical applications. Simulation, in general, cannot be used for correctness proofs, since the complexity of most systems makes exhaustive simulation impossible. In the past, synthesis and verification have been proposed as means for the design of correct systems. Synthesis has ...","1998-06-17"
562,164935,"Exploiting Structural Similarities in a BDD-based Verification Method","C. A. J. Van Eijk; G. L. J. M. Janssen;",". A major challenge in the area of hardware verification is to devise methods that can handle circuits of practical size. This paper intends to show how the applicability of combinational circuit verification tools based on binary decision diagrams (BDDs) can be greatly improved. The introduction of dynamic variable ordering techniques already makes these tools more robust; a designer no longer needs to worry about a good initial variable order. In addition, we present a novel approach combining BDDs with a technique that exploits structural similarities of the circuits under comparison. We explain how these similarities can be detected and put to effective use in the verification process. Benchmark results show that the proposed method significantly extends the range of circuits that can be verified using BDDs. 1 Introduction The times when researchers in the CAD field could sit in their ivory tower thinking up neat solutions for theoretical problems belong to the past. Nowadays, ind...","1998-05-19"
563,165221,"Temporal Analysis and Object-Oriented Real-Time Software Development: a Case Study with ROOM/ObjecTime","Daniel Gaudreau; Paul Freedman;","New generation methodologies and CASE tools are making possible increasing ""automation"" by addressing software development in terms of executable models. In this paper, we describe one such methodology, ROOM, along with its CASE tool, ObjecTime, for which systems are modelled in terms of hierarchically organised communicating objects whose behavior is defined in terms of finite state machines. In particular, we present the results of a Rate Monotonic Analysis inspired temporal analysis which makes evident some of the runtime limitations associated with ROOM semantics and the way in which the ROOM model is cast as an executable for the intended embedded platform. 1. Introduction In order to respond to the increasingly sophisticated demands of operating authorities in terms of safety, reliability, and performance [1], computerized control is rapidly gaining importance in mass transit e.g. [2], [8]. In addition, the relatively short (2 year) design-andbuild contracts make the need for re...","1997-02-12"
564,165617,"The synchronous dataflow programming language LUSTRE","D. Pilaud; N. Halbwachs; P. Caspi; P. Raymond;","This paper describes the language Lustre, which is a dataflow synchronous language, designed for programming reactive systems --- such as automatic control and monitoring systems --- as well as for describing hardware. The dataflow aspect of Lustre makes it very close to usual description tools in these domains (block-diagrams, networks of operators, dynamical samples-systems, etc: : : ), and its synchronous interpretation makes it well suited for handling time in programs. Moreover, this synchronous interpretation allows it to be compiled into an efficient sequential program. Finally, the Lustre formalism is very similar to temporal logics. This allows the language to be used for both writing programs and expressing program properties, which results in an original program verification methodology. 1 Introduction Reactive systems Reactive systems have been defined as computing systems which continuously interact with a given physical environment, when this environment is unable to sy...","1994-05-30"
565,166209,"Computing Simulations on Finite and Infinite Graphs","Monika R. Henzinger; Peter W. Kopke; Thomas A. Henzinger;",". We present algorithms for computing similarity relations of labeled graphs. Similarity relations have applications for the refinement and verification of reactive systems. For finite graphs, we present an O(mn) algorithm for computing the similarity relation of a graph with n vertices and m edges (assuming m n). For effectively presented infinite graphs, we present a symbolic similarity-checking procedure that terminates if a finite similarity relation exists. We show that 2D rectangular automata, which model discrete reactive systems with continuous environments, define effectively presented infinite graphs with finite similarity relations. It follows that the refinement problem and the 8CTL model-checking problem are decidable for 2D rectangular automata. 1 Introduction A labeled graph G = (V; E;A; hhDeltaii) consist of a (possibly infinite) set V of vertices, a set E ` V 2 of edges, a set A of labels, and a function hhDeltaii : V ! A that maps each vertex v to a label hh...","1996-07-06"
566,167226,"Model Checking for Extended Timed Temporal Logics","Ahmed Bouajjani; Sergio Yovine; Yassine Lakhnech;","We introduce the real-time temporal logic BTATL p which is obtained by extending CTL with both past operators and timed automata constraints. Considering together past operators and automata constraints allows expressing timing requirements in a simple and natural way. Model-checking for full BTATL p is undecidable. Fortunately, there exist significant (both linear-time and branchingtime) fragments for which the verification problem is decidable. In particular, we identify a sublogic of BTATL p , which is more expressive than TCTL, and for which model-checking can be effectively done. The practical interest of this logic is illustrated through the example of the Philips audio protocol. 1 Introduction One of the major approaches for specifying and verifying (reactive) systems consists in using !-automata and/or temporal logics as specification formalisms with model checking algorithms for automatic verification. This framework has been extended to the case of real-time systems by ad...","1996-10-08"
567,167231,"Learning Bayesian Networks Using Feature Selection","Gregory M. Provan; Moninder Singh;","This paper introduces a novel enhancement for learning Bayesian networks with a bias for small, high-predictive-accuracy networks. The new approach selects a subset of features which maximizes predictive accuracy prior to the network learning phase. We examine explicitly the effects of two aspects of the algorithm, feature selection and node ordering. Our approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all attributes. 1 INTRODUCTION Bayesian networks are being increasingly recognized as an important representation for probabilistic reasoning. For many domains, the need to specify the probability distributions for a Bayesian network is considerable, and learning these probabilities from data using an algorithm like K2 [8] 1 could alleviate such specification difficulties. We describe an extension to the Bayesian network learning approaches introduced in K2. Rather than ...","1994-11-05"
568,167247,"Learning Non-Linearly Separable Boolean Functions With Linear Threshold Unit Trees and Madaline-Style Networks","Mehran Sahami;","This paper investigates an algorithm for the construction of decisions trees comprised of linear threshold units and also presents a novel algorithm for the learning of nonlinearly separable boolean functions using Madalinestyle networks which are isomorphic to decision trees. The construction of such networks is discussed, and their performance in learning is compared with standard BackPropagation on a sample problem in which many irrelevant attributes are introduced. Littlestone's Winnow algorithm is also explored within this architecture as a means of learning in the presence of many irrelevant attributes. The learning ability of this Madaline-style architecture on non-optimal (larger than necessary) networks is also explored. Introduction We initially examine a non-incremental algorithm that learns binary classification tasks by producing decision trees of linear threshold units (LTU trees). This decision tree bears some similarity to the decision trees produced by ID3 (Quinlan 19...","1995-05-12"
569,167515,"The Concurrency Factory - Practical Tools for Specification, Simulation, Verification, and Implementation of Concurrent Systems","And S. Zhang; Implementation Concurrent; J. N. Gada; O. Sokolsky; S. A. Smolka; Systems Cleaveland;",". The Concurrency Factory is an integrated toolset for specification, simulation, verification, and implementation of concurrent systems such as communication protocols and process control systems. Two themes central to the project are the following: the use of process algebra , e.g., CCS, ACP, CSP, as the underlying formal model of computation, and the provision of practical support for process algebra. By ""practical"" we mean that the Factory should be usable by protocol engineers and software developers who are not necessarily familiar with formal verification, and it should be usable on problems of real-life scale, such as those found in the telecommunications industry. The main features of the Concurrency Factory are graphical (VTView) and textual (VPL) user interfaces; a suite of analysis routines for automatic verification including a bisimulation and model checker; a graphical simu- lator for VTView specifications; and a graphical compiler that transforms VTView and VPL speci...","1994-10-18"
570,167686,"Misclassification Minimization","O. L. Mangasarian;","The problem of minimizing the number of misclassified points by a plane, attempting to separate two point sets with intersecting convex hulls in n-dimensional real space, is formulated as a linear program with equilibrium constraints (LPEC). This general LPEC can be converted to an exact penalty problem with a quadratic objective and linear constraints. A Frank-Wolfetype algorithm is proposed for the penalty problem that terminates at a stationary point or a global solution. Novel aspects of the approach include: (i) A linear complementarity formulation of the step function that ""counts"" misclassifications, (ii) Exact penalty formulation without boundedness, nondegeneracy or constraint qualification assumptions, (iii) An exact solution extraction from the sequence of minimizers of the penalty function for a finite value of the penalty parameter for the general LPEC and an explicitly exact solution for the LPEC with uncoupled constraints, and (iv) A parametric quadratic programming form...","1999-02-10"
571,167774,"Formal Verification in a Commercial Setting","R. P. Kurshan;","This tutorial addresses the following questions: ffl why do formal verification? ffl who is doing it today? ffl what are they doing? ffl how are they doing it? ffl what about the future? Introduction Formal methods long have been touted as a means to produce ""provably correct implementations"". It is only recently, however, with rather more modest claims, that one formal method: modelchecking, has been embraced by industry. In stark contrast with its two-decade development, only the last two years have laid witness to its commercial viability. Nonetheless, in this very short time, this technology has blossomed from scattered pilot projects at a very few commercial sites, into implementations in at least five commercially offered Design Automation tools. This acceleration of activity has even caught the attention of the investment community. Happy graduate students of this technology are basking in an unexpected competition for their talents in an otherwise lack-luster job market. W...","1997-05-29"
572,169083,"Representation and Symbolic Manipulation of Linearly Inductive Boolean Functions","Aarti Gupta; Allan L. Fisher;","We consider a class of practically useful Boolean functions, called Linearly Inductive Functions (LIFs), and present a canonical representation as well as algorithms for their automatic symbolic manipulation. LIFs can be used to capture structural induction in parameterized circuit descriptions, whereby our LIF representation provides a fixed-sized representation for all size instances of a circuit. Furthermore, since LIFs can naturally capture the temporal induction inherent in sequential system descriptions, our representation also provides a canonical form for sequential functions. This allows for a wide range of applications of symbolic LIF manipulation in the verification and synthesis of digital systems. We also present practical results from a preliminary implementation of a general purpose LIF package. 1 Introduction Symbolic manipulation of Boolean functions has found numerous applications in the area of VLSI design automation [5]. These applications are greatly facilitated ...","1995-08-04"
573,169124,"Myrinet - A Gigabit-per-Second Local-Area Network","Alan E. Kulawik; Charles L. Seitz; Danny Cohen; Jakov N. Seizovic; Nanette J. Boden; Robert E. Felderman; Wen-king Su;",". Myrinet is a new type of local-area network (LAN) based on the technology used for packet communication and switching within ""massivelyparallel processors"" (MPPs). Think of Myrinet as an MPP message-passing network that can span campus dimensions, rather than as a wide-area telecommunications network that is operating in close quarters. The technical steps toward making Myrinet a reality included the development of (1) robust, 25m communication channels with flow control, packet framing, and error control; (2) self-initializing, low-latency, cut-through switches; (3) host interfaces that can map the network, select routes, and translate from network addresses to routes, as well as handle packet traffic; and (4) streamlined host software that allows direct communication between user processes and the network. Background. In order to understand how Myrinet differs from conventional LANs such as Ethernet and FDDI, it is helpful to start with Myrinet's genealogy. Myrinet is rooted in t...","1995-04-19"
574,169223,"SATO: an Efficient Propositional Prover","Hantao Zhang;","r class of SAT instances. For instance, in our study of quasigroup problems, one rule seems better than the others: choose one literal in one of the shortest positive clauses (a positive clause is a clause where all the literals are positive). On the other hand, a proved effective splitting rule is to choose a variable x such that the value f 2 (x) f 2 (:x) is maximal, where f 2 (L) is one plus the number of occurrences of literal L in binary clauses [2, 5]. We tried to combine the above two rules into one as follows: Let 0 ! a 1 and n be the number of shortest non-Horn clauses in the current set. At first, we collect all the variable names appearing in the first da ne shortest positive clauses. Then we choose x in this pool","1998-11-17"
575,169288,"Fourier's Elimination: Which to Choose?","Jean-louis Imbert;","Variable elimination is of major interest for Constraint Logic Programming Languages [JaLa86], and Constraint Query Languages [KKR90], where we would like to eliminate auxiliary variables introduced during the execution of a program. This elimination is always suitable for final results. It can also increase the efficiency of the intermediary processes. We focus on linear inequalities of the form ax b, where a denotes a n-real vector, x an n-vector of variables, b a real number, and the juxtaposition ax denotes the inner product. In this paper, we will focus exclusively on methods related to Fourier's elimination [Fourie]. Our aim is to make visible the links between the different contributions of S.N. Cernikov [Cern63], D.A. Kolher [Kohl67], R.J. Duffin [Duff74], JL.J. Imbert [Imbe90], and J.Jaffar, M.J. Maher, P.J. Stuckey and R.H.C. Yap [JMSY92]. This study, which has never been done before, is of great interest for languages such as CHIP, CLP(!) and Prolog III. We show that the t...","1993-05-10"
576,169748,"Model Checking Coloured Petri Nets Exploiting Strongly Connected Components","Allan Cheng; Kjeld H. Mortensen;",". In this paper we present a CTL-like logic which is interpreted over the state spaces of Coloured Petri Nets. The logic has been designed to express properties of both state and transition information. This is possible because the state spaces are labelled transition systems. We compare the expressiveness of our logic with CTL's. Then, we present a model checking algorithm which for efficiency reasons utilises strongly connected components and formula reduction rules. We present empirical results for non-trivial examples and compare the performance of our algorithm with that of Clarke, Emerson, and Sistla. 1 Introduction Coloured Petri Nets (CP-nets or CPN) are convenient for specifying complex concurrent systems. Until now properties of CP-nets have mainly been specified directly in terms of the state spaces of CP-nets [4,6]. Temporal logics such as CTL are also useful for expressing properties of concurrent systems (see, e.g., [1]). We show how we can define a CTL like logic, ASK-C...","1997-04-16"
577,170066,"Multivariate versus Univariate Decision Trees","Carla E. Brodley; Paul E. Utgoff;","In this paper we present a new multivariate decision tree algorithm LMDT, which combines linear machines with decision trees. LMDT constructs each test in a decision tree by training a linear machine and then eliminating irrelevant and noisy variables in a controlled manner. To examine LMDT's ability to find good generalizations we present results for a variety of domains. We compare LMDT empirically to a univariate decision tree algorithm and observe that when multivariate tests are the appropriate bias for a given data set, LMDT finds small accurate trees. 1 Introduction One commonly used approach for learning from examples is to induce a univariate decision tree (Hunt, Marin & Stone, 1966; Breiman, Friedman, Olshen & Stone, 1984; Quinlan, 1986). Each test in a univariate tree is based on one of the input variables and therefore, is restricted to representing a split through the instance space that is orthogonal to the variable's axis. Such a bias may be inappropriate for problems...","1993-09-15"
578,170200,"A Comparison of Presburger Engines for EFSM Reachability","James H. Kukula; Rajeev K. Ranjan; Thomas R. Shiple;",". Implicit state enumeration for extended finite state machines relies on a decision procedure for Presburger arithmetic. We compare the performance of two Presburger packages, the automata-based Shasta package and the polyhedrabased Omega package. While the raw speed of each of these two packages can be superior to the other by a factor of 50 or more, we found the asymptotic performance of Shasta to be equal or superior to that of Omega for the experiments we performed. 1 Introduction Peano arithmetic, the theory of arithmetic with multiplication and addition, is undecidable. However, decision procedures do exist for the subset of arithmetic, known as Presburger arithmetic, that excludes multiplication [13]. Presburger formulas are built up from natural number constants, natural number variables, addition, equality, inequality, and the first order logical connectives. An example of such a formula is 1 9x(y = 2x + 1): Even though the best known procedure for deciding Presburger arit...","1998-07-09"
579,170453,"Tracking Design Changes with Formal Verification","Paul Curzon;",". Designs are often modified for use in new circumstances. If formal proof is to be an acceptable verification methodology for industry, it must be capable of tracking design changes quickly. We describe our experiences formally verifying an implementation of an ATM network component, and on our subsequent verification of modified designs. Three of the designs verified are in use in a working network. They were designed and implemented with no consideration for formal methods. This case study gives an indication of the difficulties in formally verifying a real design and of subsequently tracking design changes. 1 Introduction Designs are often modified as requirements change. Such modifications often take a fraction of the original design time to complete. Even if a design can initially be validated in an acceptable time scale, formal verification is unlikely to be accepted if a similar amount of time is required to validate subsequent modified designs. It has been suggested that this...","1994-11-03"
580,170577,"A New Approach to Effective Circuit Clustering","Andrew B. Kahng; Lars Hagen;","The complexity of next-generation VLSI systems will exceed the capabilities of top-down layout synthesis algorithms, particularly in netlist partitioning and module placement. Bottom-up clustering is needed to ""condense"" the netlist so that the problem size becomes tractable to existing optimization methods. In this paper, we establish the DS quality measure, the first general metric for evaluation of clustering algorithms. The DS metric in turn motivates our RWST algorithm, a new self-tuning clustering method based on random walks in the circuit netlist. RWST efficiently captures a globally good circuit clustering. When incorporated within a two-phase iterative Fiduccia-Mattheyses partitioning strategy, the RW-ST clustering method improves bisection width by an average of 17% over previous matching-based methods. 1 Introduction Top-down approaches are widely used to cope with increasing problem complexity in layout synthesis. Recursive calls to a partitioning algorithm generate a ci...","1992-10-17"
581,170700,"An Exact Probability Metric for Decision Tree Splitting","J. Kent Martin;","ID3's information gain heuristic is well-known to be biased towards multi-valued attributes. This bias is only partially compensated by the gain ratio used in C4.5. Several alternatives have been proposed, notably orthogonality and Beta. Gain ratio and orthogonality are strongly correlated, and all of the metrics share a common bias towards splits with one or more small expected values, under circumstances where the split likely ocurred by chance. Both classical and Bayesian statistics lead to the multiple hypergeometric distribution as the posterior probability of the null hypothesis. Both gain and the chi-squared significance test are shown to arise in asymptotic approximations to the hypergeometric, revealing similar criteria for admissibility and showing the nature of their biases. Previous failures to find admissible stopping rules are traced to coupling these biased approximations with one another or with arbitrary thresholds; problems which are overcome by the hypergeometric. Em...","1995-12-29"
582,171529,"Symbolic Equivalence Checking","A. Kerbrat; J. C. Fern; L. Mounier;",". We describe the implementation, within ALDEBARAN of an algorithmic method allowing the generation of a minimal labeled transition system from an abstract model ; this minimality is relative to an equivalence relation. The method relies on a symbolic representation of the state space. We compute the minimal labeled transition system using the Binary Decision Diagram structures to represent the set of equivalence classes. Some experiments are presented, using a model obtained from LOTOS specifications. 1 Introduction Program analysis is a part of process design whose purpose is to verify statically dynamic properties of the run time behaviour of a program. In this general framework, we are interested in the verification of behavioural properties on a concurrent program specified in the Lotos language [ISO87]. For this purpose, a possible approach is to translate the program and the properties to be verified into suitable abstract models, and to check the equivalence of these models un...","1998-09-01"
583,171813,"The Power of QDDs","And Pierre Wolper; Bernard Boigelot; Bernard Willems; Patrice Godefroid;",". Queue-content Decision Diagrams (QDDs) are finite-automaton based data structures for representing (possibly infinite) sets of contents of a finite collection of unbounded FIFO queues. Their intended use is to serve as a symbolic representation of the possible queue contents that can occur in the state space of a protocol modeled by finite-state machines communicating through unbounded queues. This is done with the help of a loop-first search, a state-space exploration technique that attempts whenever possible to compute symbolically the effect of repeatedly executing a loop any number of times, making it possible to analyze protocols with infinite state spaces though without the guarantee of termination. This paper first solves a key problem concerning the use of QDDs in this context: it precisely characterizes when, and shows how, the operations required by a loop-first search can be applied to QDDs. Then, it addresses the problem of exploiting QDDs and loop-first searches to broad...","1999-03-05"
584,172836,"An International Survey of Industrial Applications of Formal Methods Volume 2 Case Studies","Dan Craigen; Ora Canada; Susan Gerhart;","Formal methods are mathematically-based techniques, often supported by reasoning tools, that can offer a rigorous and effective way to model, design and analyze computer systems. The purpose of this study is to evaluate international industrial experience in using formal methods. The cases selected are, we believe, representative of industrial-grade projects and span a variety of application domains. The study had three main objectives: . To better inform deliberations within industry and government on standards and regulations; . To provide an authoritative record on the practical experience of formal methods to date; and . To suggest areas where future research and technology development are needed. This is the second volume of a two volume final report on an international survey of industrial applications of formal methods. In this volume, we provide the details of the twelve case studies. For each of the case studies, we present a case description, summarize the information obtaine...","1994-12-09"
585,173238,"Tree-Based Mapping of Algorithms to Predefined Structures","Peter Marwedel;","Due to the need for fast design cycles and low production cost, programmable targets like DSP processors are becoming increasingly popular. Design planning, detailed design as well as updating such designs requires mapping existing algorithms onto these targets. Instead of writing target-specific mappers, we propose using retargetable mappers. The technique reported in this paper is based on pattern matching. Binary code is generated as a result of this matching process. This paper describes the techniques of our mapper MSSV and identifies areas for improvements. As a result, it shows that efficient handling of alternative mappings is crucial for an acceptable performance. 1 Introduction For many years, research on high-level design tools was focused on high-level synthesis. High-level synthesis starts with a behavioral description and generates a structure with the same behavior. In most of the cases, the generated structure inplements just the given behavior. The limitation of that...","1998-06-17"
586,173886,"Semantics of Java Byte Code","Peter Bertelsen C;","Semantic Functions In this section we define a set of abstract semantic functions. The precise semantics of these functions is not considered any further in this specification of the Java Virtual Machine semantics. The following total semantic functions are used for truncating values of type Int to `smaller' Java types: i2z; i2b; i2c; i2s : Int ! Int The above functions truncate their Int operands to values suitable for a Java variable of type boolean, byte, char, and short, respectively. Note that these `small integer' types are not used in the operand stack and local variables of the Java Virtual Machine; all values of the smaller types are represented as Int values, except when they are stored in arrays (cf. rule 20 ff). These total semantic functions are used for converting between and truncating values of (abstract) numeric types: i2l : Int ! Long i2f : Int ! Float i2d : Int ! Double l2i : Long ! Int l2f : Long ! Float l2d : Long ! Double f2i : Float ! Int f2l : Float ! Lon...","1997-04-06"
587,175303,"An Automata-Theoretic Approach to Branching-Time Model Checking (Extended Abstract)","null","Translating linear temporal logic formulas to automata has proven to be an effective approach for implementing linear-time model-checking, and for obtaining many extensions and improvements to this verification method. On the other hand, for branching temporal logic, automata-theoretic techniques have long been thought to introduce an exponential penalty, making them essentially useless for model-checking. Recently, Bernholtz and Grumberg have shown that this exponential penalty can be avoided, though they did not match the linear complexity of non-automata-theoretic algorithms...","1996-07-18"
588,175470,"The TPTP Problem Library","Christian Suttner; Geoff Sutcliffe; Theodor Yemenis; Tu Munchen;","This report provides a detailed description of the TPTP library of problems for automated theorem provers. This library is available via Internet, and is intended to form a common basis for the development of and experimentation with automated theorem provers. To support this goal, this report provides ffl the motivations for building the library; ffl a discussion of troublesome issues, and how they have been resolved; ffl a description of the library structure including overview information; ffl information on each problem contained in the library; ffl descriptions of supplementary utility programs; ffl guidelines for obtaining and using the library. Contents 1 Introduction 2 1.1 State of the Art : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2 1.2 What is Required? : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3 2 Inside the TPTP 5 2.1 The TPTP Domain Structu...","1994-03-19"
589,175563,"A Constraint Oriented Proof Methodology based on Modal Transition Systems","Bernhard Steffen; Carsten Weise; Kim G. Larsen;","In this paper, we present a constraint-oriented state-based proof methodology for concurrent software systems which exploits compositionality and abstraction for the reduction of the verification problem under investigation. Formal basis for this methodology are Modal Transition Systems allowing loose state-based specifications, which can be refined by successively adding constraints. Key concepts of our method are projective views, separation of proof obligations, Skolemization and abstraction. The method is even applicable to real time systems. 1 Introduction The use of formal methods and in particular formal verification of concurrent systems, interactive or fully automatic, is still limited to very specific problem classes. For state-based methods this is mainly due to the state explosion problem: the state graph of a concurrent systems grows exponentially with the number of its parallel components, leading to an unmanageable size for most practically relevant systems. Consequentl...","1995-01-18"
590,175790,"Retargetable Assembly Code Generation by Bootstrapping","Rainer Leupers; Wolfgang Schenk;","In a hardware/software codesign environment compilers are needed that map software components of a partitioned system behavior description onto a programmable processor. Since the processor structure is not static, but can repeatedly change during the design process, the compiler should be retargetable to avoid manual compiler adaption for any alternative architecture. A restriction of existing retargetable compilers is that they only generate microcode for the target architecture instead of machine-level code. In this paper we introduce a bootstrapping technique allowing us to translate high-level language (HLL) programs into real machine-level code using a retargetable microcode compiler. The retargetability is preserved, permitting to compare different architectural alternatives in a codesign framework within relatively little time. As an application of the new code generation technique we consider hardware/software codesign of heterogeneous information processing systems. i Conte...","1998-06-17"
591,176581,"Modelling the Work Flow of a Nuclear Waste Management Program","Kjeld H. Mortensen; Valerio Pinci;",". In this paper we describe a modelling project to improve a nuclear waste management program in charge of the creation of a new system for the permanent disposal of nuclear waste. SADT (Structured Analysis and Design Technique) is used in order to provide a work-flow description of the functions to be performed by the waste management program. This description is then translated into a number of Coloured Petri Nets (CPN or CP-nets) corresponding to different program functions where additional behavioural inscriptions provide basis for simulation. Each of these CP-nets is simulated to produce timed event charts that are useful for understanding the behaviour of the program functions under different scenarios. Then all the CPN models are linked together to form a single stand-alone application that is useful for validating the interaction and cooperation between the different program functions. A technique for linking executable CPN models is developed for supporting large modelling pro...","1997-04-16"
592,176773,"Recognition Algorithms for the Loom Classifier","David Brill; Robert M. Macgregor;","Most of today's terminological representation systems implement hybrid reasoning architectures wherein a concept classifier is employed to reason about concept definitions, and a separate recognizer is invoked to compute instantiation relations between concepts and instances. Whereas most of the existing recognizer algorithms designed to maximally exploit the reasoning supplied by the concept classifier, Loom has experimented with recognition strategies that place less emphasis on the classifier, and rely more on the abilities of Loom's backward chaining query facility. This paper presents the results of experiments that test the performance of the Loom algorithms. These results suggest that, at least for some applications, the Loom approach to recognition is likely to outperform the classical approach. They also indicate that for some applications, much better performance can be achieved by eliminating the recognizer entirely, in favor of a purely backward chaining architecture. We conclude that no single recognition algorithm or strategy is best for all applications, and that an architecture that offers a choice of inference modes is likely to be more useful than one that offers only a single style of reasoning.","1994-03-25"
593,177173,"Some Progress in the Symbolic Verification of Timed Automata","Amir Pnueli; Marius Bozga; Oded Maler;",". In this paper we discuss the practical difficulty of analyzing the behavior of timed automata and report some results obtained using an experimental bdd-based extension of kronos. We have treated examples originating from timing analysis of asynchronous boolean networks and CMOS circuits with delay uncertainties and the results outperform those obtained by previous implementations of timed automata verification tools. 1 Introduction The computational burden associated with the verification of discrete systems consists in representing and calculating the set of reachable states of a transition system, usually described as a product of small interacting systems. Timed systems were introduced in order to provide a more detailed level of modeling in which it is possible to refine a statement such as ""a is followed by b"" into ""a is followed by b within t time units"". Timed formalisms for describing systems (timed automata [AD94], [D89], timed Petri nets [BD91], timed transition systems ...","1997-04-30"
594,178958,"Indexing for Data Models with Constraints and Classes","Darren Vengroff; Jeffrey Vitter; Paris Kanellakis; Sridhar Ramaswamy;","We examine I/O-efficient data structures that provide indexing support for new data models. The database languages of these models include concepts from constraint programming (e.g., relational tuples are generalized to conjunctions of constraints) and from object-oriented programming (e.g., objects are organized in class hierarchies). Let n be the size of the database, c the number of classes, B the page size on secondary storage, and t the size of the output of a query. (1) Indexing by one attribute in many constraint data models is equivalent to external dynamic interval management, which is a special case of external dynamic 2-dimensional range searching. We present a semi-dynamic data structure for this problem that has worst-case space O(n=B) pages, query I/O time O(log B n + t=B) and O(log B n + (log B n) 2 =B) amortized insert I/O time. Note that, for the static version of this problem, this is the first worst-case optimal solution. (2) Indexing by one attribute and by clas...","1994-05-23"
595,179072,"Specifying and Verifying Distributed Intelligent Systems","Michael Fisher; Michael Wooldridge;",". This paper describes first steps towards the formal specification and verification of Distributed Artificial Intelligence (DAI) systems, through the use of temporal belief logics. The paper first describes Concurrent MetateM, a programming language for DAI, and then develops a logic that may be used to reason about Concurrent MetateM systems. The utility of this logic for specifying and verifying Concurrent MetateM systems is demonstrated through a number of examples. The paper concludes with a brief discussion of the wider implications of the work, and in particular on the use of similar logics for reasoning about DAI systems in general. 1 Introduction In the past decade, the discipline of DAI has moved from being a somewhat obscure relation of mainstream AI to being a major research area in its own right. DAI techniques have been applied to domains as diverse as archaeology and economics, as well as more mundane problems such as distributed sensing and manufacturing control [7]. ...","1994-06-29"
596,180149,"Implementing a Model Checker for LEGO","Shenwei Yu; Zhaohui Luo;",". Interactive theorem proving gives a general approach for modelling and verification of both hardware and software systems but requires significant human efforts to deal with many tedious proofs. To be used in practical, we need some automatic tools such as model checkers to deal with those tedious proofs. In this paper, we formalise a verification system of both CCS and an imperative language in LEGO which can be used to verify both finite and infinite problems. Then a model checker, LegoMC, is implemented to generate the LEGO proof terms of finite models automatically. Therefore people can use LEGO to verify a general problem and throw some finite sub-problems to be verified by LegoMC. On the other hand, this integration extends the power of model checking to verify more complicated and infinite models as well. 1 Introduction Interactive theorem proving gives a general approach for modelling and verification of both hardware and software systems but requires significant human effor...","1997-06-11"
597,180603,"Normalised Rewriting and Normalised Completion","null","We introduce normalised rewriting, a new rewrite relation. It generalises former notions of rewriting modulo E, dropping some conditions on E. For example, E can now be the theory of identity, idempotency, the theory of Abelian groups, the theory of commutative rings. We give a new completion algorithm for normalised rewriting. It contains as an instance the usual AC completion algorithm, but also the wellknown Buchberger's algorithm for computing standard bases of polynomial ideals. We investigate the particular case of completion of ground equations, In this case we prove by a uniform method that completion modulo E terminates, for some interesting E. As a consequence, we obtain the decidability of the word problem for some classes of equational theories. We give implementation results which shows the efficiency of normalised completion with respect to completion modulo AC. 1 Introduction Equational axioms are very common in most sciences, including computer science. Equations can ...","1994-06-20"
598,180886,"Incremental Formalization: a Key to Industrial Success","Bernhard Steffen; Lehrstuhl Fur Programmiersysteme; Tiziana Margaria; Volker Braun;","Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 9.2 Formal Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 10 Conclusions, Experience and Perspectives 15 List of Figures 1 The META-Frame Global Architecture . . . . . . . . . . . . . . . . . . . . 2 2 The Service Logic Editor with SIB Palette . . . . . . . . . . . . . . . . . . 4 3 The Service Definition Process . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Symbolic Execution via Red Line Tracing . . . . . . . . . . . . . . . . . . . 10 5 A SIB Hypertext Descriptor and the Browser's History . . . . . . . . . . . 11 6 The Model Checker Finds a Billing Error . . . . . . . . . . . . . . . . . . . 13 7 The Corresponding Error View . . . . . . . . . . . . . . . . . . . . . . . . 14 8 The Corrected Error View . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 9 The Correct Granny's Freephone Service . . . . . . . . . . . . . . . . . . . 1 Motivation In his invited tal...","1998-01-09"
599,181463,"Timing Analysis in COSPAN","Rajeev Alur; Robert P. Kurshan;",". We describe how to model and verify real-time systems using the formal verification tool Cospan. The verifier supports automatatheoretic verification of coordinating processes with timing constraints. We discuss different heuristics, and our experiences with the tool for certain benchmark problems appearing in the verification literature. 1 Introduction Model checking is a method of automatically verifying concurrent systems in which a finite-state model of a system is compared with a correctness requirement. This method has been shown to be very effective in detecting errors in high-level designs, and has been implemented in various tools. We consider the tool Cospan that is based on the theory of !-automata (!-automata are finite automata accepting infinite sequences, see [Tho90] for a survey, and [VW86, Kur94] for applications to verification). The system to be verified is modeled as a collection of coordinating processes described in the language S/R [Kur94]. The semantics of su...","1996-01-02"
600,182371,"Learning Problem-Oriented Decision Structures from Decision Rules: The AQDT-2 System","Ibrahim F. Imam; Ryszard S. Michalski;","A decision structure is an acyclic graph that specifies an order of tests to be applied to an object (or a situation) to arrive at a decision about that object. and serves as a simple and powerful tool for organizing a decision process. This paper proposes a methodology for learning decision structures that are oriented toward specific decision making situations. The methodology consists of two phases: 1---determining and storing declarative rules describing the decision process, 2---deriving on-line a decision structure from the rules. The first step is performed by an expert or by an AQbased inductive learning program that learns decision rules from examples of decisions (AQ15 or AQ17). The second step transforms the decision rules to a decision structure that is most suitable for the given decision making situation. The system, AQDT-2, implementing the second step, has been applied to a problem in construction engineering. In the experiments, AQDT-2 outperformed all other programs a...","1999-01-26"
601,182548,"Parallel Programming in Split-C","Andrea Dusseau; Arvind Krishnamurthy; David E. Culler; Katherine Yelick; Seth Copen Goldstein; Steven Lumetta; Thorsten Von Eicken;","We introduce the Split-C language, a parallel extension of C intended for high performance programming on distributed memory multiprocessors, and demonstrate the use of the language in optimizing parallel programs. Split-C provides a global address space with a clear concept of locality and unusual assignment operators. These are used as tools to reduce the frequency of remote access and perform required remote accesses efficiently. The language allows a mixture of shared memory, message passing, and data parallel programming styles while providing efficient access to the underlying machine. We demonstrate the basic language concepts using regular and irregular parallel programs, including an electromagnetics problem on a general graph. Performance results are given for various stages of program optimization on a prototype implementation of Split-C on the CM-5. 1 Overview Split-C is a parallel extension of the C programming language that supports efficient access to a global address s...","1995-04-21"
602,182742,"Computing Quantitative Characteristics of Finite-State Real-Time Systems","E. Clarke; H. Hiraishi; M. Minea; S. Campos; W. Marrero;","Current methods for verifying real-time systems are essentially decision procedures that establish whether the system model satisfies a given specification. We present a general method for computing quantitative information about finite-state real-time systems. We have developed algorithms that compute exact bounds on the delay between two specified events and on the number of occurrences of an event in a given interval. This technique allows us to determine performance measures such as schedulability, response time, and system load. Our algorithms produce more detailed information than traditional methods. This information leads to a better understanding of system behavior, in addition to determining its correctness. We also show that our technique can be extended to a more general representation of real-time systems, namely, timed transition graphs. The algorithms presented in this paper have been incorporated into the SMV model checker and used to verify a model of an aircraft cont...","1997-10-24"
603,182869,"Discretization of Timed Automata","Aleks Gollu; Anuj Puri; Pravin Varaiya;","We construct two discretizations of dense time automata which generate the same untimed language as the dense time automata. 1 Introduction A timed automaton (TA) is an automaton together with a finite set of clocks that constrain the occurrence of transitions. The timed language of the TA consists of pairs of sequences f(oe i ; ø i )g, where ø i is the timestamp of the event oe i . If ø i can take any real value, one obtains the dense time language LCT . If ø i can take discrete values 0; Delta; 2Delta; :::, one obtains the discrete time language LDT . The untimed languages LCU and LDU are obtained by dropping the timestamps. We prove for two different discretizations that LDU = LCU , extending the result of [2]. TA's form the first step in a descriptive hierarchy between (untimed) automata and hybrid systems. 2 Timed Automata Recall the definition of a TA [1]. It is an automaton together with a set C = f1 Delta Delta Delta Kg of timers (clocks) with state space R K + . Tim...","1995-07-28"
604,184297,"Compositional Minimal Semantics for the Stochastic Process Algebra TIPP","Markus Siegle; Michael Rettelbach;","The problem of deriving the Markov chain underlying a stochastic process algebra term is addressed. Transition rate matrices are used as a convenient method for uniquely describing Markov chains. For a modified version of the stochastic process algebra TIPP, we propose a set of new semantic rules which specify the way in which process terms are translated into their corresponding matrices. For each operator of the language, a semantic rule describes how the (one ore more) operand matrices have to be combined in order to form the matrix corresponding to the overall term. These semantic rules guarantee certain highly advantageous properties of the resulting matrices, the two most important of which are (i) the absence of non-reachable states and (ii) minimality with respect to Markov chain lumpability. Thus avoiding redundancy, our new approach is a contribution to the struggle against state space explosion. 1 Introduction TIPP is a formal language for specifying Markovian models of beh...","1997-06-25"
605,184608,"The Power of Decision Tables","Ron Kohavi;",". We evaluate the power of decision tables as a hypothesis space for supervised learning algorithms. Decision tables are one of the simplest hypothesis spaces possible, and usually they are easy to understand. Experimental results show that on artificial and real-world domains containing only discrete features, IDTM, an algorithm inducing decision tables, can sometimes outperform state-of-the-art algorithms such as C4.5. Surprisingly, performance is quite good on some datasets with continuous features, indicating that many datasets used in machine learning either do not require these features, or that these features have few values. We also describe an incremental method for performing cross validation that is applicable to incremental learning algorithms including IDTM. Using incremental cross-validation, it is possible to cross-validate a given dataset and IDTM in time that is linear in the number of instances, the number of features, and the number of label values. The time for incre...","1997-08-25"
606,184966,"Propagating Constraints in Recursive Deductive Databases","David B. Kemp; Isaac Balbin; Kotagiri Ramamohanarao; Krishnamurthy Meenakshi;","In traditional database systems, as in deductive databases that do not contain recursive rules, the efficient retrieval of tuples satisfying a constraint such as ""retrieve all people who earn more than 30,000 dollars"" is crucial to the performance of the database system. We investigate an algorithm which permits the ""early use"" of constraints to guide a bottom-up computation of recursive rules. The algorithm is an adaptation of the well-known fold/unfold transformations and is designed to work in conjunction with the magic set approach. A contribution of the algorithm is that it generalises the ability of magic-sets to transform a program, traditionally based on argument bindings within a rule or query, to the case of arguments in a rule or query which are constrained, but not necessarily bound. In addition, it permits the implementation of a bi-directional sip (sideways information passing strategy). 1 Introduction The field of Deductive Databases [8], is concerned with developing...","1994-08-02"
607,185191,"Consistency Checking of SCR-Style Requirements Specifications","Bruce Labaw; Constance Heitmeyer; Daniel Kiskis;","This paper describes a class of formal analysis called consistency checking that mechanically checks requirements specifications, expressed in the SCR tabular notation, for application-independent properties. Properties include domain coverage, type correctness, and determinism. As background, the SCR notation for specifying requirements is reviewed. A formal requirements model describing the meaning of the SCR notation is summarized, and consistency checks derived from the formal model are described. The results of experiments to evaluate the utility of automated consistency checking are presented. Where consistency checking of requirements fits in the software development process is discussed. 1 Introduction A recent study of industrial application of formal methods concludes that formal methods, including those for specifying and analyzing requirements, are ""beginning to be used seriously and successfully by industry: : : to develop systems of significant scale and importance"" [5]...","1995-02-21"
608,185918,"Ptolemy: A Framework for Simulating and Prototyping Heterogeneous Systems","David G. Messerschmitt; Edward A. Lee; Joseph Buck; Soonhoi Ha;","Ptolemy is an environment for simulation and prototyping of heterogeneous systems. It uses modern object-oriented software technology (C++) to model each subsystem in a natural and efficient manner, and to integrate these subsystems into a whole. Ptolemy encompasses practically all aspects of designing signal processing and communications systems, ranging from algorithms and communication strategies, simulation, hardware and software design, parallel computing, and generating real-time prototypes. To accommodate this breadth, Ptolemy must support a plethora of widely-differing design styles. The core of Ptolemy is a set of object-oriented class definitions that makes few assumptions about the system to be modeled; rather, standard interfaces are provided for generic objects and more specialized, application-specific objects are derived from these. A basic abstraction in Ptolemy is the Domain, which realizes a computational model appropriate for a particular type of subsystem. Current e...","1998-01-07"
609,185961,"Constraint-Based Query Optimization for Spatial Databases","Kim Marriott; Martin Odersky; Richard Helm;","We present a method for converting a system of multivariate Boolean constraints into a sequence of univariate range queries of the type supported by current spatial databases. The method relies on the transformation of a Boolean constraint system into triangular form. We extend previous results in this area by considering negative as well as positive constraints. We also present a method to approximate triangular Boolean constraints by bounding box constraints. 1 Introduction In spatial database systems, there is a gap between the high-level query language required by applications and users, and the simpler query language supported by the underlying spatial data-structure. Typically, applications such as geographic information systems [5, 8, 10], visual language parsers [7], VLSI design rule checkers [14], require a query language in which queries and integrity constraints may be expressed over a number of variables subject to Boolean constraints (that is, constraints over sets). In ...","1999-03-23"
610,186752,"Globus: A Metacomputing Infrastructure Toolkit","Carl Kesselman; Ian Foster;","Emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. While the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing environment poses new challenges for developers of system software, parallel tools, and applications. In this article, we introduce Globus, a system that we are developing to address these challenges. The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and network. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to construct various higher-level metacomp...","1998-05-12"
611,187249,"Concurrent Constraint Logic Programming on Massively Parallel SIMD Computers","Bo-ming Tong; Ho-fung Leung;","With the advent of cost-effective massively parallel computers, researchers conjecture that the future constraint logic programming system is composed of a massively parallel constraint solver as the back-end with a concurrent inference engine as the front-end [Coh90]. This paper represents an attempt to build a constraint logic programming system on a massively parallel SIMD computer. A concurrent constraint logic programming language called Firebird is presented in this paper. Firebird can handle finite domain constraints and supports both concurrency and data-parallelism. As a result, it is suitable for both multiprocessors and SIMD computers. Concurrency arises from the stream and-parallelism of committed-choice logic programming languages. In a nondeterministic derivation step, one of the domain variables is selected to create a choice point. All possible alternatives are attempted in parallel. Dataparallelism is exploited in the resulting or-parallel execution. Firebird is curren...","1993-07-20"
612,187254,"A Logic for Reasoning about Time and Reliability","Bengt Jonsson; Hans Hansson;","We present a logic for stating properties such as, ""after a request for service there is at least a 98% probability that the service will be carried out within 2 seconds"". The logic extends the temporal logic CTL by Emerson, Clarke and Sistla with time and probabilities. Formulas are interpreted over discrete time Markov chains. We give algorithms for checking that a given Markov chain satisfies a formula in the logic. The algorithms require a polynomial number of arithmetic operations, in size of both the formula and This research report is a revised and extended version of a paper that has appeared under the title ""A Framework for Reasoning about Time and Reliability"" in the Proceeding of the 10 th IEEE Real-time Systems Symposium, Santa Monica CA, December 1989. This work was partially supported by the Swedish Board for Technical Development (STU) as part of Esprit BRA Project SPEC, and by the Swedish Telecommunication Administration. the Markov chain. A simple example is inc...","1995-01-15"
613,187439,"Universal Plans for Reactive Robots in Unpredictable Environments","null","In: Proc 10th IJCAI, 1987, 1039ff. To date, reactive robot behavior has been achieved only through manual programming. This paper describes a new kind of plan, called a ""universal plan"", which can be synthesized automatically, yet generates appropriate behavior in unpredictable environments. In classical planning work, problems were posed with unique initial and final world states; in my approach a problem specifies only a goal condition. The planner is thus unable to commit to any specific future course of events but must specify appropriate reactions for anticipated situations. An alternative conception is that one universal plan compactly represents every classical plan. Which part of the universal plan is executed depends entirely on how the environment behaves at execution time. Universal plans are constructed from state-space operator schemas by a nonlinear planner. They explicitly identify predicates requiring monitoring at each moment of execution, and provide for sabotage, se...","1998-09-29"
614,187838,"State-space Planning by Integer Optimization","Henry Kautz; Joachim P. Walser;","This paper describes ILP-PLAN, a framework for solving AI planning problems represented as integer linear programs. ILP-PLAN extends the planning as satisfiability framework to handle plans with resources, action costs, and complex objective functions. We show that challenging planning problems can be effectively solved using both traditional branchand -bound IP solvers and efficient new integer local search algorithms. ILP-PLAN can find better quality solutions for a set of hard benchmark logistics planning problems than had been found by any earlier system. 1 Introduction In recent years the AI community witnessed the unexpected success of satisfiability testing as a method for solving state-space planning problems (Weld 1999). Kautz and Selman (1996) demonstrated that in certain computationally challenging domains, the approach of axiomatizing problems in propositional logic and solving them with general randomized SAT algorithms (SATPLAN) was competitive with or superior to the ...","1999-03-30"
615,187991,"Overfitting Avoidance as Bias","Cullen Schaffer;","Strategies for increasing predictive accuracy through selective pruning have been widely adopted by researchers in decision tree induction. It is easy to get the impression from research reports that there are statistical reasons for believing that these overfitting avoidance strategies do increase accuracy and that, as a research community, we are making progress toward developing powerful, general methods for guarding against overfitting in inducing decision trees. In fact, any overfitting avoidance strategy amounts to a form of bias and, as such, may degrade performance instead of improving it. If pruning methods have often proven successful in empirical tests, this is due, not to the methods, but to the choice of test problems. As examples in this article illustrate, overfitting avoidance strategies are not better or worse, but only more or less appropriate to specific application domains. We are not---and cannot be---making progress toward methods both powerful and general. The ...","1994-07-05"
616,188218,"Cooperation of Synthesis, Retargetable Code Generation and Test Generation in the MSS","Peter Marwedel; Wolfgang Schenk;","This paper demonstrates how the different tools in the MIMOLA hardware design system MSS are used during a typical design process. Typical design processes are partly automatic and partly manual. They include high-level synthesis, manual postoptimization, retargetable code generation, testability evaluation and simulation. The paper demonstrates how consistent tools can help to solve a variety of related design tasks. There is no other system with an equivalent set of consistent tools. A key contribution of this paper is to show how current high-level synthesis systems can be extended by retargetable code generators which map algorithms to predefined structures. This extension is necessary in order to support manual design modifications. 1 Introduction The MIMOLA hardware design system MSS is a set of tools for the design of digital hardware structures. Main emphasis is on programmable structures. The MSS contains tools for high-level synthesis [10, 9], for retargetable code generatio...","1998-06-17"
617,188568,"Equivalence Checking of Combinational Circuits using Boolean Expression Diagrams","Henrik Hulgaard; Henrik Reif Andersen; Poul Frederick Williams;","The combinational logic-level equivalence problem is to determine whether two given combinational circuits implement the same Boolean function. This problem arises in a number of CAD applications, for example when checking the correctness of incremental design changes (performed either manually or by a design automation tool). This paper introduces a data structure called Boolean Expression Diagrams (BEDs) and two algorithms for transforming a BED into a Reduced Ordered Binary Decision Diagram (OBDD). BEDs are capable of representing any Boolean circuit in linear space and can exploit structural similarities between the two circuits that are compared. These properties make BEDs suitable for verifying the equivalence of combinational circuits. BEDs can be seen as an intermediate representation between circuits (which are compact) and OBDDs (which are canonical). Based on a large number of combinational circuits, we demonstrate that BEDs either outperform or achieve results comparable to...","1999-02-08"
618,188852,"Verus: A Tool for Quantitative Analysis of Finite-State Real-Time Systems","E. Clarke; M. Minea; S. Campos; W. Marrero;","Symbolic model checking is a technique for verifying finite-state concurrent systems that has been extended to handle real-time systems. Models with up to 10 30 states can often be verified in minutes. In this paper, we present a new tool to analyze real-time systems, based on this technique. We have designed a language, called Verus, for the description of real-time systems. Such a description is compiled into a state-transition graph and represented symbolically using binary decision diagrams. We have developed new algorithms for exploring the state space and computing quantitative information about the system. In addition to determining the exact bounds on the delay between two specified events, we compute the number of occurrences of an event in all such intervals. This technique allows us to determine performance measures such as schedulability, response time, and system load. Our algorithms produce more detailed information than traditional methods. This information leads to a ...","1997-10-24"
619,189245,"An SE-tree based Characterization of the Induction Problem","Ron Rymon;","Many induction programs use decision trees both as the basis for their search, and as a representation of their classifier solution. In this paper we propose a new structure, called SE-tree, as a more general alternative. 1 INTRODUCTION Many learning algorithms use decision trees as an underlying framework for search and as a representation of their classifier solutions (e.g. ID3 [Quinlan, 86], CART [Breiman et al., 84]). This framework, however, is known to mix search bias (introduced when the algorithm decides on the order in which attributes are to be used in splitting) with hypotheses-space bias. To avoid being trapped by this bias, several researchers have suggested averaging over multiple trees (e.g. [Buntine, 91]). In this paper, still within a recursive partitioning framework, we propose using an alternative data structure called SE-tree [Rymon, 92]. On one hand, since the new framework shares many of the features of decision tree-based algorithms, we should be able to adopt m...","1994-07-31"
620,189392,"Structuring and Automating Hardware Proofs in a Higher-Order Theorem-Proving Environment","Klaus Schneider; Ramayya Kumar; Thomas Kropf;",". In this article we present a structured approach to formal hardware verification by modelling circuits at the register-transfer level using a restricted form of higher-order logic. This restricted form of higher-order logic is sufficient for obtaining succinct descriptions of hierarchically designed register-transfer circuits. By exploiting the structure of the underlying hardware proofs and limiting the form of descriptions used, we have attained nearly complete automation in proving the equivalences of the specifications and implementations. A hardware-specific tool called MEPHISTO converts the original goal into a set of simpler subgoals, which are then automatically solved by a general-purpose, first-order prover called FAUST. Furthermore, the complete verification framework is being integrated within a commercial VLSI CAD framework. Keywords: hardware verification, higher-order logic 1 Introduction The past decade has witnessed the spiralling of interest within the academic com...","1998-09-08"
621,189479,"An Algebraic Approach to Rule Analysis in Expert Database Systems","Elena Baralis; Jennifer Widom;","Expert database systems extend the functionality of conventional database systems by providing a facility for creating and automatically executing Condition-Action rules. While Condition-Action rules in database systems are very powerful, they also can be very difficult to program, due to the unstructured and unpredictable nature of rule processing. We provide methods for static analysis of Condition-Action rules; our methods determine whether a given rule set is guaranteed to terminate, and whether rule execution is confluent (has a guaranteed unique final state). Our methods are based on previous methods for analyzing rules in active database systems. We improve considerably on the previous methods by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not. Our improved analysis is based on a ""propagation"" algorithm, which uses a formal approach based on an extended relatio...","1996-06-26"
622,189845,"Partial Model Checking (Extended Abstract)","null",") Henrik Reif Andersen Department of Computer Science Technical University of Denmark Building 344, DK-2800 Lyngby, Denmark. Abstract A major obstacle in applying finite-state model checking to the verification of large systems is the combinatorial explosion of the state space arising when many loosely coupled parallel processes are considered. The problem also known as the state-explosion problem has been attacked from various sides. This paper presents a new approach based on partial model checking: Parts of the concurrent system are gradually removed while transforming the specification accordingly. When the intermediate specifications constructed in this manner can be kept small, the stateexplosion problem is avoided. Experimental results with a prototype implemented in Standard ML, shows that for Milner's Scheduler --- an often used benchmark --- this approach improves on the published results on Binary Decision Diagrams and is comparable to results obtained using generalized...","1997-02-19"
623,191079,"Applying the SCR Requirements Method to a Simple Autopilot","Constance Heitmeyer; Ramesh Bharadwaj;","Although formal methods for developing computer systems have been available for more than a decade, few have had significant impact in practice. A major barrier to their use is that developers find formal methods difficult to understand and apply. One exception is a formal method called SCR for specifying computer system requirements which, due to its easy-to-use tabular notation and demonstrated scalability, has achieved some success in industry. To demonstrate and evaluate the SCR method and tools, we recently used SCR to specify the requirements of a simplified mode control panel for the Boeing 737 autopilot. This paper presents the SCR requirements specification of the autopilot, outlines the process we used to create the SCR specification from a prose description, and discusses the problems and questions that arose in developing the specification. Formalizing and analyzing the requirements specification in SCR uncovered a number of problems with the original prose description, suc...","1997-07-22"
624,192210,"From Concurrent Logic Programming to Concurrent Constraint Programming","Catuscia Palamidessi; Frank S. De Boer;","The endeavor to extend logic programming to a language suitable for concurrent systems has stimulated in the last decade an intensive research, resulting in a large variety of proposals. A common feature of the various approaches is the attempt to define mechanisms for concurrency within the logical paradigm, the driving ideal being the balance between expressiveness and declarative reading. In this survey we present the motivations, the principal lines along which the field has developed, the various paradigms which have been proposed, and the main approaches to the semantic foundations. 1 Introduction Among the various reasons which have contributed to the popularity of logic programming, one is the opinion that it is an inherently parallel language, therefore suitable for parallel and distributed architectures. The pure language can already be regarded as a model for parallel computation: in the so-called process interpretation (van Emden and de Lucena 1982; Shapiro 1983), the goal...","1996-02-18"
625,192588,"Better Verification Through Symmetry","C. Norris; Ip David; L. Dill;","A fundamental difficulty in automatic formal verification of finite-state systems is the state explosion problem -- even relatively simple systems can produce very large state spaces, causing great difficulties for methods that rely on explicit state enumeration. We address the problem by exploiting structural symmetries in the description of the system to be verified. We make symmetries easy to detect by introducing a new data type scalarset, a finite and unordered set, to our description language. The operations on scalarsets are restricted so that states are guaranteed to have the same future behaviors, up to permutation of the elements of the scalarsets. Using the symmetries implied by scalarsets, a verifier can automatically generate a reduced state space, on the fly. We provide a proof of the soundness of the new symmetry-based verification algorithm based on a definition of the formal semantics of a simple description language with scalarsets. The algorithm has been implemented ...","1996-11-25"
626,192769,"On Bias Plus Variance","David H. Wolpert;",": This paper presents several additive ""corrections"" to the conventional quadratic loss biasplus -variance formula. One of these corrections is appropriate when both the target is not fixed (as in Bayesian analysis) and also training sets are averaged over (as in the conventional bias-plusvariance formula). Another additive correction casts conventional fixed-training-set Bayesian analysis directly in terms of bias-plus-variance. Another correction is appropriate for measuring full generalization error over a test set rather than (as with conventional bias-plus-variance) error at a single point. Yet another correction can help explain the recent counter-intuitive bias-variance decomposition of Friedman for zero-one loss. After presenting these corrections this paper then discusses some other loss-function-specific aspects of supervised learning. In particular, there is a discussion of the fact that if the loss function is a metric (e.g., zero-one loss), then there is bound on the chang...","1996-12-28"
627,193029,"A Logically Complete Reasoning Maintenance System Based on a Logical Constraint Solver","J. C. Madre; O. Coudert;","This paper presents a logically complete assumption based truth maintenance system (ATMS) that is part of a complex blast furnace computer aided piloting system [ 5 ] . This system is built on an efficient and logically complete propositional constraint solver that has been successfully used for industrial applications in computer aided design. 1 Introduction A reasoning maintenance system (RMS) is a critical part of a reasoning system, since it is responsible for assuring that the inferences made by that system are valid. The reasoning system provides the RMS with information about each inference it performs, and in return the RMS provides the reasoning system with information about the whole set of inferences. Several implementations of reasoning maintenance systems have been proposed in the past, remarkable ones being Doyle's truth maintenance system (TMS) [ 6 ] , and De Kleer's assumption-based truth maintenance system (ATMS) [ 7 ] . Both of them suffer from some limitations. The...","1995-08-05"
628,193435,"Using Structured Modelling for Efficient Performance Prediction of Parallel Systems","Markus Siegle;",": A method for analyzing parallel systems with the help of structured Markovian models is presented. The overall model is built from interdependent submodels, and symmetries are exploited automatically by grouping similar submodels in classes. During model analysis, this leads to a state space reduction, based on the concept of exact lumpability. 1. Introduction The high complexity of today's parallel computers --- due to the large number of concurrently active and mutually dependent hardware and software components --- makes them difficult to understand for human users. Developing efficient software for this class of machines is therefore very time-consuming and expensive. The resulting performance often fails to meet expectations, therefore requiring major re-implementation. Event-based performance models can be used to alleviate this problem. They help humans to understand the dynamic behaviour of parallel systems by abstracting the overwhelming number of details of the real world...","1993-10-07"
629,193692,"Efficient Encoding Schemes for Symbolic Analysis of Petri Nets","Enric Pastor; Jordi Cortadella;","Petri nets are a graph-based formalism appropriate to model concurrent systems such as asynchronouscircuits or network protocols. Symbolic techniques based on Binary Decision Diagrams (BDDs) have emerged as one of the strategies to overcome the state explosion problem in the analysis of systems modeled byPetri nets. The existing techniques for state encoding use a variableper -place strategy that leads to encoding schemes with very low density. This drawback has been partially mitigated by using Zero-Suppressed BDDs, that provide a typical reduction of BDD sizes by a factor of two. This work presents novel encoding schemes for Petri nets. By using algebraic techniques to analyze the topology of the net, sets of places ""structurally related"" can be derived and encoded by only using a logarithmic number of boolean variables. Such approach allows to drastically decrease the number of variables for state encoding and reduce memory and CPU requirements significantly. 1 Introduction Petri ...","1997-12-04"
630,193723,"Report on the Program AMoRE","A. Miller; A. Potthoff; E. Valkema; O. Matz; W. Thomas;","this document. Preface","1970-01-01"
631,195019,"Generating Oracles from Your Favorite Temporal Specifications","L. K. Dillon; Y. S. Ramakrishna;","The paper describes a generic tableau algorithm, which is the basis for a general customizable method for producing oracles from temporal logic specifications. A generic argument gives semantic rules with which to build the semantic tableau for a specification. Parameterizing the tableau algorithm by semantic rules permits it to easily accommodate a variety of temporal operators and provides a clean mechanism for fine-tuning the algorithm to produce efficient oracles. The paper develops conditions that ensure a set of rules results in a correct tableau procedure. It gives sample rules for a variety of linear-time temporal operators and shows how rules are tailored to reduce the size of an oracle. To illustrate the versatility of this method, its application to a high level interval logic is discussed. Keywords: formal specification and validation methods, specification-based test oracles, trace checking, tableau methods, temporal logic, interval logic, concurrent systems 1 Introductio...","1995-09-01"
632,195148,"Adding Epistemic Operators to Concept Languages","Werner Nutt;","We investigate the use of epistemic operators in the framework of concept languages (also called terminological languages). The results of this work have a twofold significance. From the point of view of epistemic logics, our contribution is to have identified an effective procedure for the problem of answering epistemic queries posed to a knowledge base expressed in the concept language ALC. From the point of view of concept languages, the most relevant aspect of our work is that we have reconstructed in logic several common features of existing knowledge representation systems. Epistemic operators provide a highly expressive query language; allow for the treatment of several database features, such as closed world reasoning and integrity constraints; and finally can give a formal characterization of some procedural mechanisms, such as trigger rules, usually considered in frame-based systems. 1 Introduction A substantial amount of research has been carried out in the last decade wit...","1996-03-05"
633,195674,"OC1: Randomized induction of oblique decision trees","Simon Kasif; Sreerama Murthy; Steven Salzberg;","This paper introduces OC1, a new algorithm for generating multivariate decision trees. Multivariate trees classify examples by testing linear combinations of the features at each non-leaf node of the tree. Each test is equivalent to a hyperplane at an oblique orientation to the axes. Because of the computational intractability of finding an optimal orientation for these hyperplanes, heuristic methods must be used to produce good trees. This paper explores a new method that combines deterministic and randomized procedures to search for a good tree. Experiments on several different real-world data sets demonstrate that the method consistently finds much smaller trees than comparable methods using univariate tests. In addition, the accuracy of the trees found with our method matches or exceeds the best results of other machine learning methods. 1 Introduction Decision trees (DTs) have been used quite extensively in the machine learning literature for a wide range of classification probl...","1994-11-04"
634,196299,"Abstraction and Approximate Decision Theoretic Planning","Craig Boutilier; Richard Dearden;","ion and Approximate Decision Theoretic Planning Richard Dearden and Craig Boutilier y Department of Computer Science University of British Columbia Vancouver, British Columbia CANADA, V6T 1Z4 email: dearden,cebly@cs.ubc.ca Abstract Markov decision processes (MDPs) have recently been proposed as useful conceptual models for understanding decision-theoretic planning. However, the utility of the associated computational methods remains open to question: most algorithms for computing optimal policies require explicit enumeration of the state space of the planning problem. We propose an abstraction technique for MDPs that allows approximately optimal solutions to be computed quickly. Abstractions are generated automatically, using an intensional representation of the planning problem (probabilistic strips rules) to determine the most relevant problem features and optimally solving a reduced problem based on these relevant features. The key features of our method are: abstractions can ...","1999-06-23"
635,196593,"Towards a Declarative Framework for Hardware-Software Codesign","Teddy Wu; Wayne Luk;","We present an experimental framework for mapping declarative programs, written in a language known as Ruby, into various combinations of hardware and software. Strategies for parametrised partitioning into hardware and software can be captured concisely in this framework, and their validity can be checked using algebraic reasoning. The method has been used to guide the development of prototype compilers capable of producing, from a Ruby expression, a variety of implementations involving fieldprogrammable gate arrays (FPGAs) and microprocessors. The viability of this approach is illustrated using a number of examples for two reconfigurable systems, one containing an array of Algotronix devices and a PC host, and the other containing a transputer and a Xilinx device. 1 Introduction Although it has been known for many years that, from a functional point of view, there is little distinction between hardware and software, in current practice they are mostly developed using very different m...","1999-03-19"
636,200890,"Decidable Reasoning in Terminological Knowledge Representation Systems","Martin Buchheit;","Terminological Knowledge Representation Systems (TKRS) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRS. The new features studied, all of practical interest, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability-, subsumption- and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems and can be easily turned into a pr...","1998-08-12"
637,201700,"Exhibiting Knowledge in Planning Problems to Minimize State Encoding Length","Malte Helmert; Stefan Edelkamp;","In this paper we present a general-purposed algorithm for transforming a planning problem specified in Strips into a concise state description for single state or symbolic exploration. The process of finding a state description consists of four phases. In the first phase we symbolically analyze the domain specification to determine constant and one-way predicates, i.e. predicates that remain unchanged by all operators or toggle in only one direction, respectively. In the second phase we symbolically merge predicates which lead to a drastic reduction of state encoding size, while in the third phase we constrain the domains of the predicates to be considered by enumerating the operators of the planning problem. The fourth phase combines the result of the previous phases.","1999-08-10"
638,204508,"Guidelines for Automated Implementation of Executable Object Oriented Models for Real-Time Embedded Control Systems","M. Saksena; P. Freedman; P. Rodziewicz;","In this paper we present our experiences in applying real-time scheduling theory to embedded control systems designed using ROOM (Real-time Object Oriented Modeling) methodology. ROOM has originated from the telecommunications community, and has been successfully applied to many commercial systems through the supporting case tool ObjecTime. It is particularly suitable for modeling reactive real-time behavior. Furthermore, it provides many other advantages through the use of object-orientation, and the use of executable models from which code may be generated quickly and efficiently. Since many real-time embedded control systems have significant reactive, event-driven, behavior, it is attractive to use ROOM methodology to develop such systems. However, the ROOM methodology does not provide tools to specify and analyze the temporal behavior as is required for the hard real-time components of embedded systems, and for which the real-time scheduling theory provides an analytical basis. In ...","1998-11-09"
639,205541,"Using a Formal Description Technique to Model Aspects of a Global Air Traffic Telecommunications Network","J. H. Andrews; J. J. Joyce; N. A. Day;","Aspects of a draft version of the Aeronautical Telecommunications Network (ATN) Standards and Recommended Practices (SARPs) under development by ISO-compliant committees of the International Civil Aviation Organization (ICAO) have been mathematically modelled using a formal description technique. The ATN SARPs are a specification for a global telecommunications network for air traffic control systems. A version of Harel's statecharts formalism embedded within a machine readable typed predicate logic has been used as a formal description technique to construct this mathematical model. Our model has been `typechecked' to partially validate the internal consistency of the specification. The work described in this paper has already uncovered some problems in the draft SARPs, and will provide a basis for follow-on efforts to apply formal analysis methods such as model-checking and symbolic execution to aspects of the ATN SARPs. The success of this approach suggests that typed predicate lo...","1998-01-28"
640,206141,"Combining Model Checking and Deduction for I/O-Automata","And Tobias Nipkow; Tu Munchen;","We propose a combination of model checking and interactive theorem proving where the theorem prover is used to represent finite and infinite state systems, reason about them compositionally and reduce them to small finite systems by verified abstractions. As an example we verify a version of the Alternating Bit Protocol with unbounded lossy and duplicating channels: the channels are abstracted by interactive proof and the resulting finite state system is model checked. 1 Introduction The purpose of this paper is to combine the two major paradigms for the verification of distributed systems: model checking and theorem proving. The advantages of each approach are well known: model checking is automatic but limited to finite state processes, theorem proving requires user interaction but can deal with arbitrary processes. Recently attempts have been made to combine the strength of both methods by using the deductive machinery of theorem provers to reduce ""large"" correctness problems to on...","1995-05-24"
641,206738,"A Partial Approach to Model Checking","Patrice Godefroid; Pierre Wolper;","This paper presents a model-checking method for linear-time temporal logic that can avoid most of the state explosion due to the modelling of concurrency by interleaving. The method relies on the concept of Mazurkiewicz's trace as a semantic basis and uses automatatheoretic techniques, including automata that operate on words of ordinality higher than omega. 1 Introduction Model checking [CES86, LP85, QS81, VW86] is an effective and simple method for verifying that a concurrent program satisfies a temporal logic formula. It works on finite-state programs and proceeds by viewing the program as a structure for interpreting temporal logic and by evaluating the formula on that structure. It is much simpler than temporal deductive proofs and can be easily and effectively implemented. It has been intensively studied for linear-time temporal logic [LP85, VW86, Var89], branching -time temporal logic [CES86, EL85b, EL85a, Bro86] and temporal ¯-calculi [EL86, Var88, Cle90, SW89]. It has been extend...","1995-02-16"
642,206892,"Bisimulation and Model Checking","Kathi Fisler; Moshe Y. Vardi;","State space minimization techniques are crucial for combating state explosion. A variety of verification tools use bisimulation minimization to check equivalence between systems, to minimize components before composition, or to reduce a state space prior to model checking. This paper explores the third use in the context of verifying invariant properties. We consider three bisimulation minimization algorithms. From each, we produce an on-the-fly model checker for invariant properties and compare this model checker to a conventional one based on backwards reachability. Our comparisons, both theoretical and experimental, lead us to conclude that bisimulation minimization does not appear to be viable in the context of invariance verification, because performing the minimization requires as many, if not more, computational resources as model checking the unminimized system through backwards reachability. Keywords: Bisimulation minimization, model checking, invariant properties, on-the-fly...","1999-07-16"
643,208243,"Decomposing the Proof of Correctness of Pipelined Microprocessors","Mandayam Srivas; Ravi Hosabettu;",". We present a systematic approach to decompose and incrementally build the proof of correctness of pipelined microprocessors. The central idea is to construct the abstraction function using completion functions, one per unfinished instruction, each of which specifies the effect (on the observables) of completing the instruction. In addition to avoiding term-size and case explosion problem that limits the pure flushing approach, our method helps localize errors, and also handles stages with iterative loops. The technique is illustrated on a pipelined and a superscalar pipelined implementations of a subset of the DLX architecture. It has also been applied to a processor with out-of-order execution. 1 Introduction Many modern microprocessors employ radical optimizations such as superscalar pipelining, speculative execution and out-of-order execution to enhance their throughput. These optimizations make microprocessor verification difficult in practice. Most approaches to mechanical ver...","1999-08-23"
644,208724,"Learning One More Thing","Sebastian Thrun; Tom M. Mitchell;","Most research on machine learning has focused on scenarios in which a learner faces a single, isolated learning task. The lifelong learning framework assumes that the learner encounters a multitude of related learning tasks over its lifetime, providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach, in which knowledge is transferred via a learned model of the invariances of the domain. Results on learning to recognize objects from color images demonstrate superior generalization capabilities if invariances are learned and used to bias subsequent learning. 1 Introduction Supervised learning is concerned with learning an unknown target function from a finite collection of input-output examples of that function. Formally, the framework of supervised learning can be characterized as follows. Let F denote the set of all target functions. For example, in a robot arm ...","1999-08-26"
645,210287,"Construction of abstract state graphs with PVS","Hassen Saidi; Susanne Graf;",": We describe in this paper a method based on abstract interpretation which, from a theoretical point of view, is similar to the splitting methods proposed in [DGG93, Dam96] but the weaker abstract transition relation we use, allows us to construct automatically abstract state graphs paying a reasonable price. We consider a particular set of abstract states: the set of the monomials on a set of state predicates ' 1 ; :::; ' ` . The successor of an abstract state m for a transition ø of the program is the least monomial satisfied by all successors via ø of concrete states satisfying m. This successor m 0 can be determined exactly if for each predicate ' i it can be determined if ' i or :' i is a postcondition of m for ø . In order to do this, we use the Pvs theorem prover [SOR93] and our Pvs-interface defined in [GS96]. If the tactic used for the proof of the verification conditions is not powerful enough, only an upper approximation of the abstract successor m is constructed. This a...","1999-02-26"
646,210342,"Efficient Representation and Computation of Tableaux Proofs","Klaus Schneider; Ramayya Kumar; Thomas Kropf;","The current first-order automatic prover FAUST, embedded in HOL, is based on a sequent calculus which is quite slow and memory intensive. In this paper, an improved version of FAUST using a modified form of tableau calculus called Tableau Graph Calculus is presented which overcomes the well-known inefficiencies of the traditional tableau calculus to a large extent. This calculus works on a compact representation of analytic tableaux called tableau graphs which are obtained by a preprocessing step which covers most of the rule applications of usual tableau calculus. This representation retains the clarity of the input formula and furthermore, its size is linear with respect to the length of the input formula. As a result of this preprocessing, our calculus has only one single rule which is repeatedly applied to obtain a proof. Many optimizations for the rule applications to effectively prune the search space are presented as well and are currently being implemented in a new version of F...","1997-12-19"
647,210594,"Computing Minimum and Maximum Reachability Times in Probabilistic Systems","Luca De Alfaro;","A Markov decision process is a generalization of a Markov chain in which both probabilistic and nondeterministic choice coexist. Given a Markov decision process with costs associated with the transitions and a set of target states, the stochastic shortest path problem consists in computing the minimum expected cost of a control strategy that guarantees to reach the target. In this paper, we consider the classes of stochastic shortest path problems in which the costs are all non-negative, or all non-positive. Previously, these two classes of problems could be solved only under the assumption that the policies that minimize or maximize the expected cost also lead to the target with probability 1. This assumption does not necessarily hold for Markov decision processes that arise as model for distributed probabilistic systems. We present efficient methods for solving these two classes of problems without relying on additional assumptions. The methods are based on algorithms to transform th...","1999-06-04"
648,210810,"Microcode Generation for Flexible Parallel Target Architectures","Peter Marwedel; Rainer Leupers; Wolfgang Schenk;",": Advanced architectural features of microprocessors like instruction level parallelism and pipelined functional hardware units require code generation techniques beyond the scope of traditional compilers. Additionally, recent design styles in the area of digital signal processing pose a strong demand for retargetable compilation. This paper presents an approach to code generation based on netlist descriptions of the target processor. The basic features of the MSSQ microcode compiler are outlined, and novel techniques for handling complex hardware modules and multi-cycle operations are presented. 1 Keyword Codes: B.1.4 Keywords: Control Structures and Microprogramming, Microprogram Design Aids 1 Introduction Besides instruction pipelining, two important means for increasing the throughput of microprocessors have been identified by hardware designers: instruction level parallelism and data pipelining. Instruction level parallelism comprises several functional units working independent...","1999-06-30"
649,212034,"Genetic Programming","John R. Koza;","Introduction Genetic programming is a domain-independent problem-solving approach in which computer programs are evolved to solve, or approximately solve, problems. Genetic programming is based on the Darwinian principle of reproduction and survival of the fittest and analogs of naturally occurring genetic operations such as crossover (sexual recombination) and mutation. John Holland's pioneering Adaptation in Natural and Artificial Systems (1975) described how an analog of the evolutionary process can be applied to solving mathematical problems and engineering optimization problems using what is now called the genetic algorithm (GA). The genetic algorithm attempts to find a good (or best) solution to the problem by genetically breeding a population of individuals over a series of generations. In the genetic algorithm, each individual in the population represents a candidate solut","1999-07-30"
650,212842,"Runtime Assurance Based On Formal Specifications","I. Lee; M. Kim; M. Viswanathan; O. Sokolsky; S. Kannan;","We describe the Monitoring and Checking (MaC) framework which assures the correctness of the current execution at run-time. Monitoring is performed based on a formal specification of system requirements. MaC bridges the gap between formal specification and verification, which ensures the correctness of a design rather than an implementation, and testing, which partially validates an implementation. An important aspect of the framework is a clear separation between implementation-dependent description of monitored objects and high-level requirements specification. Another salient feature is automatic instrumentation of executable code. The paper presents an overview of the framework and two languages to specify monitoring scripts and requirements, and briefly explain our on-going prototype implementation. 1 Introduction Much research in the past two decades concentrated on methods for analysis and validation of distributed and real-time systems. Important results have been achieved, in...","1999-04-26"
651,213792,"Partial-Order Methods for Model Checking: From Linear Time to Branching Time","Bernard Willems; Pierre Wolper;","Partial-order methods make it possible to check properties of a concurrent system by state-space exploration without considering all interleavings of independent concurrent events. They have been applied to linear-time model checking, but so far only limited results are known about their applicability to branching-time model checking. In this paper, we introduce a general technique for lifting partial-order methods from linear-time to branching-time logics. This technique is shown to be applicable both to reductions that are applied to the structure representing the program before running the model checking procedure, as well as to reductions that can be obtained whenmodel checking is done in an automata-theoretic framework. The latter are extended to branching-time logics by using the model-checking framework based on alternating automata introduced by Bernholtz, Vardi and Wolper. 1. Introduction Model checking [6, 17, 20, 26] is by now a widespread technique for verifying concurrent...","1996-07-30"
652,214041,"Building A Federated Relational Database System: An Approach Using A Knowledge-Based System","An Approach; System Miguel Blanco; Using A Knowledge-based;","Due to the emerging interest in integrating different application environments, there have been many recent proposals for federated systems. In this paper, a federated system that permits the integration of heterogeneous relational databases using a terminological knowledge representation system is presented. In particular, two of the system's components: the translator and the integrator are explained in depth. The translator permits one to obtain a terminology from a relational schema, either semiautomatically, by expressing database properties, or manually, by using a set of predefined operations. In turn, the integrator generates a federated terminology by integrating several terminologies using the semantics expressed as correspondences between the data elements of different terminologies. Unlike many other approaches, the use of a terminological system permits us to obtain a semantically richer federated terminology and, at the same time, define a wider and more consistent integr...","1998-05-23"
653,214148,"Least Fixpoint Approximations for Reachability Analysis","Fabio Somenzi; In-ho Moon; James Kukula; Tom Shiple;","The knowledge of the reachable states of a sequential circuit can dramatically speed up optimization and model checking. However, since exact reachability analysis may be intractable, approximate techniques are often preferable. Cho et al. presented the Machine-By-Machine (MBM) and Frame-By-Frame (FBF) methods to perform approximate FSM traversal. FBF produces tighter upper bounds than MBM; however, it usually takes much more time and it may have convergence problems. In this paper, we show that there exists a class of methods---Least Fixpoint Approximations--- that compute the same results as RFBF (Reached FBF, one of the FBF methods). We show that one member of this class, which we call Least fixpoint MBM (LMBM), is as efficient as MBM, but provably more accurate. Therefore, the trade-off that existed between MBM and RFBF has been eliminated. LMBM can compute RFBFquality approximations for all the large ISCAS-89 benchmark circuits in a total of less than 9000 seconds. 1 Introduction...","1999-08-13"
654,214254,"Closed Terminologies in Description Logics","Robert A. Weida;","We introduce a predictive concept recognition methodology for description logics based on a new closed terminology assumption. During knowledge engineering, our system adopts the standard open terminology assumption as it automatically classifies concept descriptions into a taxonomy via subsumption inferences. However, for applications like configuration, the terminology becomes fixed during problem solving. Then, closed terminology reasoning is more appropriate. In our interactive configuration application, a user incrementally specifies an individual computer system in collaboration with a configuration engine. Choices can be made in any order and at any level of abstraction. We distinguish between abstract and concrete concepts to formally define when an individual's description may be considered finished. We also take advantage of the closed terminology assumption, together with the terminology's subsumptionbased organization, to efficiently track the types of systems and component...","1998-02-02"
655,215202,"A Decision Procedure for a Class of Set Constraints","And Joxan Jaffar; Nevin Heintze;","A set constraint is of the form exp 1 ' exp 2 where exp 1 and exp 2 are set expressions constructed using variables, function symbols, projection symbols, and the set union, intersection and complement symbols. While the satisfiability problem for such constraints is open, restricted classes have been useful in program analysis. The main result herein is a decision procedure for definite set constraints which are of the restricted form a ' exp where a contains only constants, variables and function symbols, and exp is a positive set expression (that is, it does not contain the complement symbol). A conjunction of such constraints, whenever satisfiable, has a least model and the algorithm will output an explicit representation of this model. 1 1 Introduction We consider a formalism for elementary set algebra which is useful for describing properties of programs whose underlying domain of computation is a Herbrand universe. The domain of discourse for this formalism is the powerset of...","1995-06-13"
656,217271,"Tearing Based Automatic Abstraction for CTL Model Checking","Abelardo Pardo; Fabio Somenzi; Gary Hachtel; Jae-young Jang; Woohyuk Lee;","ion for CTL Model Checking Woohyuk Lee Abelardo Pardo Jae-Young Jang Gary Hachtel Fabio Somenzi University of Colorado ECEN Campus Box 425 Boulder, CO, 80309 Abstract In this paper we present the tearing paradigm as a way to automatically abstract behavior to obtain upper and lower bound approximations of a reactive system. We present algorithms that exploit the bounds to perform conservative ECTL and ACTL model checking. We also give an algorithm for false negative (or false positive) resolution for verification based on a theory of a lattice of approximations. We show that there exists a bipartition of the lattice set based on positive versus negative verification results. Our resolution methods are based on determining a pseudo-optimal shortest path from a given, possibly coarse but tractable approximation, to a nearest point on the contour separating one set of the bipartition from the other. 1 Introduction We describe an approach for approximation-based formal verification o...","1996-10-04"
657,217381,"Integration in Real PCF","Abbas Edalat; Martn Hotzel Escardo;","Real PCF is an extension of the programming language PCF with a data type for real numbers. Although a Real PCF definable real number cannot be computed in finitely many steps, it is possible to compute an arbitrarily small rational interval containing the real number in a sufficiently large number of steps. Based on a domain-theoretic approach to integration, we show how to define integration in Real PCF. We propose two approaches to integration in Real PCF. One consists in adding integration as primitive. The other consists in adding a primitive for maximization of functions and then recursively defining integration from maximization. In both cases we have an adequacy theorem for the corresponding extension of Real PCF. Moreover, based on previous work on Real PCF definability, we show that Real PCF extended with the maximization operator is universal, which implies that it is also fully abstract. 1. Introduction Traditionally, in computing science one represents real numbers by floa...","1996-03-28"
658,218891,"Unification in the Union of Disjoint Equational Theories: Combining Decision Procedures","Equational Theories; Franz Baader; Klaus U. Schulz; Lehr- Und Forschungsgebiet Theoretische;","Most of the work on the combination of unification algorithms for the union of disjoint equational theories has been restricted to algorithms which compute finite complete sets of unifiers. Thus the developed combination methods usually cannot be used to combine decision procedures, i.e., algorithms which just decide solvability of unification problems without computing unifiers. In this paper we describe a combination algorithm for decision procedures which works for arbitrary equational theories, provided that solvability of so-called unification problems with constant restrictions---a slight generalization of unification problems with constants---is decidable for these theories. As a consequence of this new method, we can for example show that general A-unifiability, i.e., solvability of A-unification problems with free function symbols, is decidable. Here A stands for the equational theory of one associative function symbol. Our method can also be used to combine algorithms which ...","1995-10-11"
659,219063,"A Case Study in Model Checking Software Systems","Jeannette M. Wing; Mandana Vaziri-farahani;","Model checking is a proven successful technology for verifying hardware. It works, however, on only finite state machines, and most software systems have infinitely many states. Our approach to applying model checking to software hinges on identifying appropriate abstractions that exploit the nature of both the system, S, and the property, OE, to be verified. We check OE on an abstracted, but finite, model of S. Following this approach we verified three cache coherence protocols used in distributed file systems. These protocols have to satisfy this property: ""If a client believes that a cached file is valid then the authorized server believes that the client's copy is valid."" In our finite model of the system, we need only represent the ""beliefs"" that a client and a server have about a cached file; we can abstract from the caches, the files' contents, and even the files themselves. Moreover, by successive application of the generalization rule from predicate logic, we need only conside...","1996-05-30"
660,219179,"The Java Language Specification","Bill Joy; Frank Yellin; Guy Steele; Harlow England; James Gosling; Kathy Walrath; Ken Arnold; Mary Campione; Rosanna Lee; Technical Advisor; Tim Lindholm;","Method Declarations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 9.4.1 Inheritance and Overriding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 9.4.2 Overloading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 9.4.3 Examples of Abstract Method Declarations . . . . . . . . . . . . . . . 190 9.4.3.1 Example: Overriding . . . . . . . . . . . . . . . . . . . . . . . . 190 9.4.3.2 Example: Overloading . . . . . . . . . . . . . . . . . . . . . . . 191 10 Arrays. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 10.1 Array Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 10.2 Array Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii 10.3 Array Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 10.4 Array Access...","1996-09-11"
661,219258,"Decidability of Model Checking for Infinite-State Concurrent Systems","Javier Esparza;","We study the decidability of the model checking problem for linear and branching time logics, and two models of concurrent computation, namely Petri nets and Basic Parallel Processes. 1 Introduction Most techniques for the verification of concurrent systems proceed by an exhaustive traversal of the state space. Therefore, they are inherently incapable of considering systems with infinitely many states. Recently, some new methods have been developed in order to at least palliate this problem. Using them, several verification problems for some restricted infinite-state models have been shown to be decidable. These results can be classified into those showing the decidability of equivalence relations [8, 9, 24, 26], and those showing the decidability of model checking for different modal and temporal logics. In this paper, we contribute to this second group. The model checking problem has been studied so far for three infinite-state models: context-free processes, pushdown processes, and...","1970-01-01"
662,219404,"Applications of Multi-Terminal Binary Decision Diagrams","E. Clarke; M. Fujita; X. Zhao;","Functions that map boolean vectors into the integers are important for the design and verification of arithmetic circuits. MTBDDs and BMDs have been proposed for representing this class of functions. We discuss the relationship between these methods and describe a generalization called hybrid decision diagrams which is often much more concise. The Walsh transform and Reed-Muller transform have numerous applications in computeraided design, but the usefulness of these techniques in practice has been limited by the size of the boolean functions that can be transformed. Currently available techniques limit the functions to less than 20 variables. In this paper, we show how to compute concise representations of the Walsh transform and Reed-Muller transform for functions with several hundred variables. We show how to implement arithemetic operations efficiently for hybrid decision diagrams. In practice, this is one of the main limitations of BMDs since performing arithmetic operations on fu...","1995-06-23"
663,219735,"Combination Techniques and Decision Problems for Disunification","Franz Baader; Klaus U. Schulz;","Previous work on combination techniques considered the question of how to combine unification algorithms for disjoint equational theories E 1 ; : : : ; E n in order to obtain a unification algorithm for the union E 1 [ : : : [ E n of the theories. Here we want to show that variants of this method may be used to decide solvability and ground solvability of disunification problems in E 1 [ : : : [E n . Our first result says that solvability of disunification problems in the free algebra of the combined theory E 1 [ : : : [E n is decidable if solvability of disunification problems with linear constant restrictions in the free algebras of the theories E i (i = 1; : : : ; n) is decidable. In order to decide ground solvability (i.e., solvability in the initial algebra) of disunification problems in E 1 [ : : : [ E n we have to consider a new kind of subproblem for the particular theories E i , namely solvability (in the free algebra) of disunification problems with linear constant restricti...","1995-10-11"
664,220337,"Symbolic Model Checking for Sequential Circuit Verification","D. E. Long; D. L. Dill; E. M. Clarke; J. R. Burch; K. L. Mcmillan;","The temporal logic model checking algorithm of Clarke, Emerson, and Sistla [17] is modified to represent state graphs using binary decision diagrams (BDDs) [7] and partitioned transition relations [10, 11]. Because this representation captures some of the regularity in the state space of circuits with data path logic, we are able to verify circuits with an extremely large number of states. We demonstrate this new technique on a synchronous pipelined design with approximately 5 Theta 10 120 states. Our model checking algorithm handles full CTL with fairness constraints. Consequently, we are able to express a number of important liveness and fairness properties, which would otherwise not be expressible in CTL. We give empirical results on the performance of the algorithm applied to both synchronous and asynchronous circuits with data path logic. 1 Introduction Bugs found late in the design phase of a digital circuit are a major cause of unexpected delays in realizing the circuit in...","1993-12-17"
665,220463,"Will The Robot Do The Right Thing?","Alan K. Mackworth; Ying Zhang;","Constraint Nets have been developed as an algebraic on-line computational model of robotic systems. A robotic system consists of a robot and its environment. A robot consists of a plant and a controller. A constraint net is used to model the dynamics of each component and the complete system. The overall behavior of the system emerges from the coupling of each of its components. The question posed in the title is decomposed into two questions: first, what is the right thing? second, how does one guarantee the robot will do it? We answer these questions by establishing a formal approach to the specification and verification of robotic behaviors. In particular, we develop a real-time temporal logic for the specification of behaviors and a new verification method, based on timed 8-automata, for showing that the constraint net model of a robotic system satisfies the specification of a desired global behavior of the system. Since the constraint net model of the controller can also serve as...","1994-05-19"
666,222245,"Symbolic Model Checking for Probabilistic Processes","Christel Baier; Mark Ryan; Marta Kwiatkowska;","We introduce a symbolic model checking procedure for Probabilistic Computation Tree Logic (PCTL) and its generalization PCTL over labelled Markov chains as models. Model checking for probabilistic logics typically involves solving linear equation systems as means to ascertain the probability of a given (non-probabilistic) formula holding in a state. Our algorithm is based on the idea of representing the matrices used in the linear equation systems in terms of an appropriate generalization of Binary Decision Diagrams (BDDs) called Multi-Terminal Binary Decision Diagrams (MTBDDs) introduced in Clarke et al [8], and combining that with BDD-based model checking. To enable precise probability calculation for path formulas we use the standard construction of the Rabin automaton (via Vardi &Wolper construction of the Buchi automaton followed by Safra's determinization) for an LTL formula and, as suggested by de Alfaro [2], construct the product of thus obtained automaton with the Markov c...","1997-01-16"
667,222394,"Operational Semantics Based Formal Symbolic Simulation","K. G. W. Goossens; Scotland U. K;","This paper describes the development of progressively more powerful and abstract hardware simulators. A small computer hardware design and description language picoella is then introduced, followed by its formal semantics. Using a number of small examples, we will then show the how this formal semantics may be used within a proof system as a sophisticated simulation tool. Examples include some full adders, a general N bit adder, and two parity checkers. Keyword Codes: I.2.3; B.7.2; F.3 Keywords: Deduction and Theorem Proving; Integrated Circuits, Design Aids; Logics and Meaning of Programs 1 Introduction This introduction describes the development of various kinds of hardware simulators. Following this, a small hdl called picoella, is introduced in section 2. Its formal semantics, and a brief account of this semantics' embedding in a proof system are described in section 3. Section 4 illustrates the use of the semantics in the capacity of a symbolic simulator, as described in the rema...","1992-10-27"
668,222571,"Interval Diagram Techniques for Symbolic Model Checking of Petri Nets","Karsten Strehl; Lothar Thiele;","Symbolic model checking tries to reduce the state explosion problem by implicit construction of the state space. The major limiting factor is the size of the symbolic representation mostly stored in huge binary decision diagrams. A new approach to symbolic model checking of Petri nets and related models of computation is presented, outperforming the conventional one and avoiding some of its drawbacks. Our approach is based on a novel, efficient form of representation for multi-valued functions called interval decision diagram (IDD) and the corresponding image computation technique using interval mapping diagrams (IMDs). IDDs and IMDs are introduced, their properties are described, and the feasibility of the new approach is shown with some experimental results. 1 Introduction During the last years, a promising approach named symbolic model checking [2] was applied to many areas of system verification, even in industrial applications. This approach makes use of binary decision diagrams ...","1999-03-14"
669,223091,"User Guide for the PVS Specification and Verification System (Beta Release)","J. M. Rushby; N. Shankar; S. Owre;","this document to refer to either the context or to the associated directory. Note that the directory may contain files other than those produced by or for PVS, but these are not considered to be a part of the context. During a PVS session, there is always a current context in which the activities of PVS take place. For example, typechecking of a specification file is allowed only if the file is a part of the current context. There are commands for changing the current context during a PVS session, so that it is unnecessary to exit PVS just to change contexts. Because contexts are associated with Unix directories there can be at most one PVS context in a directory, but a directory that contains a PVS context may also contain arbitrary additional files. User Interface","1996-02-08"
670,223320,"ARC - A Tool for Efficient Refinement and Equivalence Checking for CSP","Atanas N. Parashkevov; Jay Yantchev;","This paper presents the design and implementation of ARC --- a tool for automated verification of concurrent systems. The tool is based on the untimed CSP language, its semantic models and theory of refinement. We alleviate the combinatorial explosion problem using Ordered Binary Decision Diagrams (OBDDs) for the internal representation of complex data structures --- sets and labeled transition systems (LTS). The semantically complex external choice operator is translated into the corresponding LTS using an optimized algorithm. This and some other implementation improvements allow verifying systems with up to 10 33 states, which is consistent with the capabilities of other OBDD-based approaches. Compared to two existing CSP tools, FDR and MRC, ARC has fewer language restrictions and is more memory efficient. A performance comparison based on the nschedulers and dining philosophers problems suggests that the checking algorithm of ARC is, in most cases, faster than those of the other ...","1970-01-01"
671,223431,"Scheduling Hardware/Software Systems Using Symbolic Techniques","Dirk Ziegenbein; Jurgen Teich; Karsten Strehl; Lothar Thiele; Rolf Ernst;","In this paper, a scheduling method for heterogeneous embedded systems is developed. At first, an internal representation model called FunState is presented which enables the explicit representation of nondeterminism and scheduling using a combination of functions and state machines. The new scheduling method is able to deal with mixed data/control flow specifications and takes into account different mechanisms of non-determinism as occurring in the design of embedded systems. Constraints imposed by other already implemented components are respected. The scheduling approach avoids the explicit enumeration of execution paths by using symbolic techniques and guarantees to find a deadlock-free and bounded schedule if one exists. The generated schedule consists of statically scheduled basic blocks which are dynamically called at run time. 1 Introduction One of the major sources of complexity in the design of embedded systems is related to their heterogeneity. On the one hand, the specific...","1999-09-29"
672,223672,"A Language Independent Scanner Generator","Teodor Rus; Tom Halverson;","This paper discusses a new methodology for scanner generation that supports language independent lexicon specification and automatic generation of stand alone lexical analyzers from specifications. The mechanism that sits at the basis of this methodology is the layering of the lexicon specification on two levels: in the first level, ""universal"" lexical constructs which are used as building blocks by most programming languages are defined, and in the second level, customized lexical constructs of specific programming languages are specified in terms of universal lexical constructs. The universal lexicon is specified by regular expressions over a global alphabet used by most programming languages, such as the character set of a keyboard, and is efficiently implemented by deterministic finite automata. The customized lexicon is conveniently specified by regular expressions of properties of universal lexical constructs and is implemented by nondeterministic automata whose transition functi...","1999-02-17"
673,223844,"Another Look at LTL Model Checking","E. Clarke; K. Hamaguchi; O. Grumberg;","We show how LTL model checking can be reduced to CTL model checking with fairness constraints. Using this reduction, we also describe how to construct a symbolic LTL model checker that appears to be quite efficient in practice. In particular, we show how the SMV model checking system developed by McMillan [16] can be extended to permit LTL specifications. The results that we have obtained are quite surprising. For the examples we considered, the LTL model checker required at most twice as much time and space as the CTL model checker. Although additional examples still need to be tried, it appears that efficient LTL model checking is possible when the specifications are not excessively complicated. This research was sponsored in part by the Avionics Laboratory, Wright Research and Development Center, Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio 45433-6543 under Contract F33615-90-C-1465, ARPA Order No. 7597 and in part by the National Science foundat...","1994-03-24"
674,224357,"Real-Time Symbolic Model Checking for Discrete Time Models","null","The bdd-based symbolic model checking algorithm given in [4, 10] is extended to handle real-time properties using the bounded until operator [9]. We believe that this algorithm, which is based on discrete time, is able to handle many real-time properties that arise in practical problems. One example of such a property is priority inversion. This is a serious problem that can make real-time systems unpredictable in subtle ways. Our work discusses this problem and presents one possible solution. The solution is formalized and verified using the modified algorithm. We also propose another extension to the model checking algorithm. Timed transition graphs are transition graphs in which events may take non-unit time to occur. The time it takes for a transition in a TTG to happen is determined by a time interval. This allows the construction of smaller and more realistic models. A symbolic model checking algorithm is given for formulas using the bounded until operator in TTG models. 1 Intr...","1994-05-25"
675,224696,"Mathematical Programming in Neural Networks","O. L. Mangasarian;","This paper highlights the role of mathematical programming, particularly linear programming, in training neural networks. A neural network description is given in terms of separating planes in the input space that suggests the use of linear programming for determining these planes. A more standard description in terms of a mean square error in the output space is also given, which leads to the use of unconstrained minimization techniques for training a neural network. The linear programming approach is demonstrated by a brief description of a system for breast cancer diagnosis that has been in use for the last four years at a major medical facility. 1 What is a Neural Network? A neural network is a representation of a map between an input space and an output space. A principal aim of such a map is to discriminate between the elements of a finite number of disjoint sets in the input space. Typically one wishes to discriminate between the elements of two disjoint point sets in the n-dim...","1970-01-01"
676,225018,"Probabilistic Predicate Transformers","Annabelle Mciver; Carroll Morgan; Jw S; Jw Sanders; Karen Seidel;","Predicate transformers facilitate reasoning about imperative programs, including those exhibiting demonic non-deterministic choice. Probabilistic predicate transformers extend that facility to programs containing probabilistic choice, so that one can in principle determine not only whether a program is guaranteed to establish a certain result, but also its probability of doing so. We bring together independent work of Claire Jones and Jifeng He, showing how their constructions can be made to correspond; from that link between a predicate-based and a relation-based view of probabilistic execution we are able to propose `probabilistic healthiness conditions', generalising those of Dijkstra for ordinary predicate transformers. The associated calculus seems suitable for exploring further the rigorous derivation of imperative probabilistic programs. Keywords: Probability, predicate transformers, non-determinism, verification, refinement, imperative programming, program derivation, Galois c...","1995-08-11"
677,225173,"Symbolic Model Checking with Partitioned Transition Relations","D. E. Long; E. M. Clarke; J. R. Burch;","We significantly reduce the complexity of BDD-based symbolic verification by using partitioned transition relations to represent state transition graphs. This method can be applied to both synchronous and asynchronous circuits. The times necessary to verify a synchronous pipeline and an asynchronous stack are both bounded by a low polynomial in the size of the circuit. We were able to handle stacks with over 10 50 reachable states and pipelines with over 10 120 reachable states. 1 Introduction Although methods for verifying sequential circuits by searching their state transition graphs have been investigated for many years, it is only recently that such methods have begun to seem practical. Before, the largest circuits that could be verified had about 10 6 states. Now it is easy to check circuits that have many orders of magnitude more states [3, 5, 6, 7]. The reason for the dramatic increase is the use of special data structures such as binary decision diagrams (BDDs) [2] for...","1992-07-10"
678,225251,"Interval Diagram Techniques and Their Applications","Karsten Strehl; Lothar Thiele;","Interval diagram techniques have been applied successfully to symbolic formal verification of process networks, Petri nets, and timed automata, outperforming conventional approaches and avoiding some of their drawbacks. Our approach is based on a novel, efficient form of representation for multi-valued functions called interval decision diagram (IDD) and the corresponding image computation technique using interval mapping diagrams (IMDs). Besides formal verification, interval diagram techniques may be used to perform symbolic scheduling of heterogeneous embedded systems. IDDs and IMDs are presented, their properties are described, and the above applications are sketched. 1 Introduction Many formal verification methods such as symbolic model checking try to reduce the state explosion problem by implicit construction of the state space. The major limiting factor is the size of the symbolic representation mostly stored in huge binary decision diagrams (BDDs). When applying conventional s...","1999-05-20"
679,225404,"Word Level Symbolic Model Checking","E. Clarke; X. Zhao;","The highly-publicized division error in the Pentium has emphasized the importance of formal verification of arithmetic operations. Symbolic model checking techniques based on binary decision diagrams (BDDs) have been successful in verifying control logic. However, lack of proper representation for functions that map boolean vectors into integers has prevented this technique from being used for verifying arithmetic circuits. We have used hybrid decision diagrams to represent the integer functions that occur in the arithmetic circuit verification. For the state variables corresponding to data bits, our representation behaves like a binary moment diagram (BMD) while for the state variables corresponding to control signals, it behaves like a multi-terminal BDD (MTBDD). By using this representation, we are able to handle circuits with both control logic and wide data paths. We have extended the symbolic model checking system SMV so that it can also handle properties involving relationships ...","1995-06-23"
680,225734,"Systematic Software Development Using VDM","Cliff B Jones;","ion and Program Synthesis, volume 75: Math. Studies of Information Processing of Lecture Notes in Computer Science. Springer-Verlag, 1979. [Bjo80a] D. Bjorner, editor. Abstract Software Specifications, volume 86 of Lecture Notes in Computer Science. Springer-Verlag, 1980. [Bjø80b] D. Bjørner. Application of formal models. In Data Bases. INFOTECH Proceedings, October 1980. [Bjø80c] D. Bjørner. Experiments in block-structured goto-modelling: Exits vs. continuations. [Bjo80a], pages 216--247, 1980. [Bjø80d] D. Bjørner. Formal description of programming concepts: a software engineering viewpoint. In MFCS '80, Lecture Notes Vol. 88, pages 1--21. Springer-Verlag, 1980. [Bjø81] D. Bjørner. The VDM principles of software specification and program design. In TC2 Work.Conf. on Formalization of Programming Concepts, pages 44--74, LNCS Vol. 107, 1981. IFIP, Springer-Verlag. [BL84] R. Bahlke and T. Letschert. Ausfuhrbare denotationale semantik. In Proc. 4, pages 3--19. Gl-Fachgesprach Implementie...","1996-05-30"
681,225937,"Serial and Parallel Multicategory Discrimination","Kristin P. Bennett; O. L. Mangasarian;","A parallel algorithm is proposed for a fundamental problem of machine learning, that of multicategory discrimination. The algorithm is based on minimizing an error function associated with a set of highly structured linear inequalities. These inequalities characterize piecewiselinear separation of k sets by the maximum of k affine functions. The error function has a Lipschitz continuous gradient that allows the use of fast serial and parallel unconstrained minimization algorithms. A serial quasi-Newton algorithm is considerably faster than previous linear programming formulations. A parallel gradient distribution algorithm is used to parallelize the error-minimization problem. Preliminary computational results are given for both a DECstation 5000/125 and a Thinking Machines Corporation CM-5 multiprocessor. 1 Introduction We consider a fundamental problem of machine learning and pattern recognition, that of discriminating between k sets. Given k disjoint sets, A i ; i = 1; : : : ; k;...","1970-01-01"
682,226359,"Using Interval Diagram Techniques for the Symbolic Verification of Timed Automata","Karsten Strehl;","In this report, we suggest interval diagram techniques for formal verification of timed automata. Interval diagram techniques are based on interval decision diagrams (IDDs)---representing sets of system configurations of, e.g., timed automata---and interval mapping diagrams (IMDs)---modeling their transition behavior. IDDs are canonical representations of Boolean functions and allow for their efficient manipulation. We present the methods necessary for our approach and compare its results to another, similar verification technique. Contents 1 Introduction 1 2 Timed Automata 3 2.1 The Timed Automaton . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Time Forward Projection . . . . . . . . . . . . . . . . . . . . . . . . . 4 3 Interval Diagram Techniques 6 3.1 Interval Decision Diagrams . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Interval Mapping Diagrams . . . . . . . . . . . . . . . . . . . . . . . 6 4 Formal Verification of Timed Automata 9 4.1 Using Differ...","1999-01-26"
683,226801,"Embedding extensional finite sets in CLP","Agostino Dovier; Gianfranco Rossi;","In this paper we review the definition of {log}, a logic language with sets, from the viewpoint of CLP. We show that starting with a CLP-scheme allows a more uniform treatment of the built-in set operations (namely, =, 2 and their negative counterparts), and allows all the theoretical results of CLP to be immediately exploitable. We prove this by precisely defining the privileged interpretation domain and the axioms of the selected set theory. Then we define a non-deterministic procedure for checking constraint satisfiability based on the reduction of a given constraint to a collection of constraint in a suitable canonical form, which is provable to be sound and complete w.r.t. the given theory. Algorithms for trasforming each one of the set constraints the language provides into their corresponding canonical forms are described in details. It is also shown that the resulting language is powerful enough to allow all the usual operations on sets (such as subset, union etc.)","1970-01-01"
684,226964,"Seven More Myths of Formal Methods","Jonathan P. Bowen; Michael G. Hinchey;","For whatever reason, formal methods remain one of the more contentious techniques in industrial software engineering. Despite some improvement in the uptake of formal methods, it is still the case that the vast majority of potential users of formal methods fail to become actual users. A paper by Hall in 1990 examined a number of `myths' concerning formal methods, assumed by some to be valid. This paper considers a few more beliefs held by many and presents some counter examples. Mathematicians first used the sign [square root of -1], without in the least knowing what it could mean, because it shortened work and led to correct results. People naturally tried to find out why this happened and what [square root of -1], really meant. After two hundred years they succeeded. -- Mathematician's Delight (1943) by W. W. Sawyer","1994-06-01"
685,227057,"Efficient Generation of Counterexamples and Witnesses in Symbolic Model Checking","E. Clarke; K. Mcmillan; O. Grumberg; X. Zhao;","Model checking is an automatic technique for verifying sequential circuit designs and protocols. An efficient search procedure is used to determine whether or not the specification is satisfied. If it is not satisfied, our technique will produce a counterexample execution trace that shows the cause of the problem. Although finding counterexamples is extremely important, there is no description of how to do this in the literature on model checking. We describe an efficient algorithm to produce counterexamples and witnesses for symbolic model checking algorithms. This algorithm is used in the SMV model checker and works quite well in practice. We also discuss how to extend our technique to more complicated specifications. This extension makes it possible to find counterexamples for verification procedures based on showing language containment between various types of !-automata. 1. Introduction Complex state-transition systems occur frequently in the design of sequential circuits and ...","1994-11-22"
686,227285,"Analytica - A Theorem Prover for Mathematica","Edmund Clarke; Xudong Zhao;","Analytica is an automatic theorem prover for theorems in elementary analysis. The prover is written in Mathematica language and runs in the Mathematica environment. The goal of the project is to use a powerful symbolic computation system to prove theorems that are beyond the scope of previous automatic theorem provers. The theorem prover is also able to guarantee the correctness of certain steps that are made by the symbolic computation system and therefore prevent common errors like division by a symbolic expression that could be zero. In this paper we describe the structure of Analytica and explain the main techniques that it uses to construct proofs. We have tried to make the paper as self-contained as possible so that it will be accessible to a wide audience of potential users. We illustrate the power of our theorem prover by several non-trivial examples including the basic properties of the stereographic projection and a series of three lemmas that lead to a proof of Weierstrass's...","1992-12-01"
687,229617,"Verification of Arithmetic Functions with Binary Moment Diagrams","Randal E. Bryant; Yirng-an Chen;","Binary Moment Diagrams (BMDs) provide a canonical representations for linear functions similar to the way Binary Decision Diagrams (BDDs) represent Boolean functions. Within the class of linear functions, we can embed arbitary functions from Boolean variables to real, rational, or integer values. BMDs can thus model the functionality of data path circuits operating over word level data. Many important functions, including integer multiplication, that cannot be represented efficiently at the bit level with BDDs have simple representations at the word level with BMDs. Furthermore, BMDs can represent Boolean functions with around the same complexity as BDDs. We propose a hierarchical approach to verifying arithmetic circuits, here basic building blocks are first shown to implement a word-level specification. The overall circuit functionality is then verified at the word level. Multipliers with word sizes of up to 62 bits have been verified by this technique. Keywords: Formal verification...","1994-06-20"
688,229731,"A Class Of Hierarchical Queueing Networks And Their Analysis","Peter Buchholz;","Queueing networks are an adequate model type for the analysis of complex system behavior. Most of the more realistic models are rather complex and do not fall into the easy solvable class of product form networks. Those models have to be analyzed by numerical solution of the underlying Markov chain and/or approximation techniques including simulation. In this paper a class of hierarchically structured queueing networks is considered and it is shown that the hierarchical model structure is directly reflected in the state space and the generator matrix of the underlying Markov chain. Iterative solution techniques for stationary and transient analysis can be modified to make use of the model structure and allow an efficient numerical analysis of large, up to now not solvable queueing networks. Keywords: Hierarchical queueing networks, Markov chains, numerical analysis, steady state solution, transient solution. 1 Introduction Performance analysis of dynamic systems (computer systems, co...","1994-12-30"
689,230168,"Automated Deduction with Shannon Graphs","Joachim Posegga; Peter H. Schmitt;","Binary Decision Diagrams (BDDs) are a well-known tool for representing Boolean functions. We show how BDDs can be extended to full first-order logic by integrating means for representing quantifiers. The resulting structures are called Shannon graphs. A calculus based on these Shannon graphs is set up, and its soundness and completeness proofs are outlined. A comparison of deduction with first-order BDDs and semantic tableaux shows that both calculi are closely related. From a practical perspective, however, BDDs have advantages over tableaux: they provide a more compact representation, since BDDs can be understood as a linear, graphical representation of a fully expanded tableaux. Furthermore, BDDs represent not only the models of a formula, but also its counter models: this offers a very efficient way to represent lemmata during the proof search. The last part of the paper introduces a compilation-based approach to implementing deduction systems based on Shannon graphs. The idea is t...","1970-01-01"
690,230245,"The Priority Inversion Problem and Real-Time Symbolic Model Checking","S'ergio V. Campos;","Priority inversion is a serious problem that can make real-time systems unpredictable in subtle ways. This makes it more difficult to implement and debug such systems. Our work discusses this problem and presents one possible solution. The solution is formalized and verified using temporal logic model checking techniques. In order to perform the verification, the bdd-based symbolic model checking algorithm given in [4, 11] was extended to handle real-time properties using the bounded until operator [9]. We believe that this algorithm, which is based on discrete time, is able to handle many real-time properties that arise in practical problems. This research was sponsored in part by the National Science Foundation under grant no. CCR-8722633, by the Semiconductor Research Corporation under contract 92-DJ-294, and by The Defense Advanced Research Projects Agency, Information Science and Technology Office, under the title ""Research on Parallel Computing"", ARPA Order No. 7330, issued by DA...","1993-05-20"
691,230259,"The Lorel Query Language for Semistructured Data","Dallan Quass; Janet Wiener; Jason Mchugh; Jennifer Widom; Serge Abiteboul;","We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data","1996-12-12"
692,230897,"Symbolic Model Checking Using Interval Diagram Techniques","Karsten Strehl; Lothar Thiele;","In this report, a representation of multi-valued functions called interval decision diagrams (IDDs) is introduced. It is related to similar representations as binary decision diagrams. Compared to other model checking strategies, IDDs show some important properties that enable us to verify especially Petri nets, process networks, and related models of computation more adequately than with conventional approaches. Therefore, a new form of transition relation representation called interval mapping diagrams (IMDs)---and their less general version predicate action diagrams (PADs)---is explained. A novel approach to symbolic model checking of Petri nets and process networks is presented. Several drawbacks of traditional strategies are avoided using IDDs and IMDs. Especially the resulting transition relation IMD is very compact, allowing for fast image computations. Furthermore, no artificial limitations concerning place capacities or equivalent have to be introduced. Additionally, applicati...","1998-06-29"
693,231699,"A Compositional Approach to Performance Modelling","Jane Hillston;","Performance modelling is concerned with the capture and analysis of the dynamic behaviour of computer and communication systems. The size and complexity of many modern systems result in large, complex models. A compositional approach decomposes the system into subsystems that are smaller and more easily modelled. In this thesis a novel compositional approach to performance modelling is presented. This approach is based on a suitably enhanced process algebra, PEPA (Performance Evaluation Process Algebra). The compositional nature of the language provides benefits for model solution as well as model construction. An operational semantics is provided for PEPA and its use to generate an underlying Markov process for any PEPA model is explained and demonstrated. Model simplification and state space aggregation have been proposed as means to tackle the problems of large performance models. These techniques are presented in terms of notions of equivalence between modelling entities. A framewo...","1999-10-04"
694,231811,"The Use of Static Constructs in A Modal Process Logic","Hans Huttel; Kim G. Larsen;","this paper we want to demonstrate that --- from a practical","1996-11-21"
695,232059,"Rigorous Learning Curve Bounds from Statistical Mechanics","David Haussler; Michael Kearns; P Rba;",". In this paper we introduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics. The advantage of our theory over the well-established VapnikChervonenkis theory is that our bounds can be considerably tighter in many cases, and are also more reflective of the true behavior of learning curves. This behavior can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory. The disadvantages of our theory are that its application requires knowledge of the input distribution, and it is limited so far to finite cardinality function classes. We illustrate our results with many concrete examples of learning curve bounds derived from our theory. Keywords: learning curves, statistical mechanics, phase transitions, VC dimension 1. Introduction According to the Vapnik-Chervonenkis (VC) theory of learning curves (Vapnik, 1982; Vapnik & Chervonenkis, 1971), minimizing e...","1998-10-21"
696,232100,"Resource Allocation in Networks Using Abstraction and Constraint Satisfaction Techniques","And Boi Faltings; Christian Frei;","ion and Constraint Satisfaction Techniques Christian Frei 1 and Boi Faltings 1 Artificial Intelligence Laboratory, Swiss Federal Institute of Technology (EPFL), 1015 Lausanne, Switzerland, fChristian.Frei, Boi.Faltingsg@epfl.ch Abstract. Most work on constraint satisfaction problems (CSP) starts with a standard problem definition and focuses on algorithms for finding solutions. However, formulating a CSP so that it can be solved by such methods is often a difficult problem in itself. In this paper, we consider the problem of routing in networks, an important problem in communication networks. It is as an example of a problem where a CSP formulation would lead to unmanageable solution complexity. We show how an abstraction technique results in tractable formulations and makes the machinery of CSP applicable to this problem. 1 Introduction Communication networks are expected to offer a wide range of services to an increasingly large number of users, with a diverse range of quality o...","1999-10-07"
697,232424,"An Improved Protocol Reachability Analysis Technique","Gerard J. Holzmann;","An automated analysis of all reachable states in a distributed system can be used to trace obscure logical errors that would be very hard to find manually. This type of validation is traditionally performed by the symbolic execution of a finite state machine (FSM) model of the system studied. The application of this method to systems of a practical size, though, is complicated by time and space requirements. If a system is larger, more space is needed to store the state descriptions and more time is needed to compare and analyze these states. This paper shows that if the FSM model is abandoned and replaced by a state vector model significant gains in performance are feasible, for the first time making it possible to perform effective validations of large systems. Software, Practice and Experience, Vol. 18, No. 2, pp. 137-161, 1988. INTRODUCTION It is notoriously difficult to define, or even to understand, the behavior of a system of interacting asynchronous processes. Improbable seq...","1999-09-14"
698,232568,"On Exploiting The Analysis Power Of Petri Nets For The Validation Of Discrete Event Systems","Monika Heiner;","The development of provably error-free concurrent systems is still a challenge of practical system engineering. Modelling and analysis of concurrent systems by means of Petri nets is one of the well-known approaches using formal methods. Among those Petri net analysis techniques suitable for strong verification purposes there is an increasing amount of promising methods avoiding the construction of the complete interleaving state space, and by this way the well-known state explosion problem. This paper claims to demonstrate that the available methods and tools are actually applicable successfully to at least medium-sized systems. For that purpose, the step-wise validation of various system properties (consistency, safety, progress) of the concurrent control software of a reactive system is performed. If possible, different analysis techniques are applied and compared with each other concerning its efforts. keywords: programmable logic controller, hierarchical place/transition nets, ...","1998-04-11"
699,233078,"On Computing Optimal Controllers for Finite State Systems","Enrico Tronci;","From a computational point of view many control problems involve synthesis of controllers for Finite State Systems (FSSs). In this paper we address the problem of automatic synthesis of Optimal Controllers (OCs) for FSSs. Such problem has been widely studied and many algorithms are known for it. However, because of state explosion their use is limited to quite small dynamical system. We devised a symbolic algorithm for automatic synthesis of OCs for FSSs. In this paper we show practical usefulness of our techniques by giving experimental results on the use of our symbolic algorithm to synthesize a C program implementing an OC for a semiconductor manufacturing facility. This entails computing OCs for plants with about 3 10 9 states. To the best of our knowledge no previous algorithm can handle systems of such size. Key Words Optimal Control, Finite State Systems, Discrete Event Systems, Supervisory Control, Hybrid Systems, Manufacturing Systems, Boolean Symbolic Programming, Boolean...","1970-01-01"
700,234295,"Pushing the Limits: New Developments in Single-Agent Search","Andreas Junghanns;","Search is one of the fundamental methods in articial intelligence (AI). It is at the core of many successes of AI that range from beating world champions in non-trivial games to building master schedules for large corporations. However, the applications of today and tomorrow require more than exhaustive, brute-force search, because these application domains have become increasingly complex. Traditional methods fail to break the complexity barrier caused by the combinatorial explosion that characterizes these large, real-world domains. This thesis enhances our understanding of single-agent search methods. A puzzle (Sokoban) is used to explore new search techniques for single-agent search. Sokoban oers new challenges to AI research, because it has a much larger search space than previously studied puzzle domains and exhibits a new, real-world-like search-space property. Deadlock, the possibility to maneuver into an unsolvable position, provides traditional search methods with considera...","1999-10-12"
701,236799,"Automatic Verification of a Hydroelectric Power Plant","null",". We analyze the specification of a hydroelectric power plant by ENEL (the Italian Electric Company). Our goal is to show that for the specification of the plant (its control system in particular) some given properties hold. We were provided with an informal specification of the plant. From such informal specification we wrote a formal specification using the CCS/Meije process algebra formalism. We defined properties using ¯- calculus. Automatic verification was carried out using model checking. This was done by translating our process algebra definitions (the model) and ¯-calculus formulas into BDDs. In this paper we present the informal specification of the plant, its formal specification, some of the properties we verified and experimental results. 1 Introduction Computer controlled systems are more and more widespread. In safety critical applications this situation calls for formal verification of correctness with respect to the given specifications. Because of the cost of modify...","1970-01-01"
702,237435,"Approximate symbolic model checking of continuous-time Markov chains (Extended Abstract)","And Holger Hermanns; Christel Baier; Joost-pieter Katoen;",". This paper presents a symbolic model checking algorithm for continuous-time Markov chains for an extension of the continuous stochastic logic CSL of Aziz et al [1]. The considered logic contains a time-bounded until-operator and a novel operator to express steadystate probabilities. We show that the model checking problem for this logic reduces to a system of linear equations (for unbounded until and the steady state-operator) and a Volterra integral equation system for timebounded until. We propose a symbolic approximate method for solving the integrals using MTDDs (multi-terminal decision diagrams), a generalisation of MTBDDs. These new structures are suitable for numerical integration using quadrature formulas based on equally-spaced abscissas, like trapezoidal, Simpson and Romberg integration schemes. 1 Introduction The mechanised verification of a given (usually) finite-state model against a property expressed in some temporal logic is known as model checking. For probabilistic...","1999-08-13"
703,239234,"Register File Port Requirements of Transport Triggered Architectures","Henk Corporaal; Jan Hoogerbrugge;","Exploitation of large amounts of instruction level parallelism requires a large amount of connectivity between the shared register file and the function units; this connectivity is expensive and increases the cycle time. This paper shows that the new class of transport triggered architectures requires less ports on the shared register file than traditional operation triggered architectures. This is achieved by programming data-transports instead of operations. Experiments with our extended basic block scheduler have shown that the reduction of the required number of register file ports is substantial. The average requirement for scalar applications is 0.50 read and 0.35 write ports per operation instead of 2 read and 1 write ports. Due to this reduction it is possible to execute 2 operations per cycle with a two-ported register file and 3.6 operations per cycle with a six-ported register file. Keywords: Transport triggered architectures, VLIW, ILP, register ports, scalability 1 Introd...","1997-03-19"
704,239492,"Transport-Triggering vs. Operation-Triggering","Henk Corporaal; Jan Hoogerbrugge;",". Transport-triggered architectures are a new class of architectures that provide more scheduling freedom and have unique compiler optimizations. This paper reports experiments that quantify the advantages of transport-triggered architectures with respect to traditional operation-triggered architectures. For the experiments we use an extended basic block scheduler that is currently being developed. The paper gives an description of the scheduling techniques used. The results of our experiments show that the extra scheduling freedom together with the new optimizations are very welcome when resource constraints are limiting the performance. This is the case when resources are scarce or when the application contains a lot of instruction level parallelism. 1 Introduction Improvements in current computer architectures are obtained by higher clock frequencies, exploitationof instruction level parallelism (ILP), and improvements in compiler technology. Higher clock frequencies are a result of...","1997-03-19"
705,239508,"A Unifying Framework for Model Checking Labeled Kripke Structures, Modal Transition Systems, and Interval Transition Systems","Michael Huth;",". We build on the established work on modal transition systems and probabilistic specifications to sketch a framework in which system description, abstraction, and finite-state model checking all have a uniform presentation across various levels of qualitative and quantitative views together with mediating abstraction and concretization maps. We prove safety results for abstractions within and across such views for the entire modal mu-calculus and show that such abstractions allow for some compositional reasoning with respect to a uniform family of process algebras `a la CCS. 1 Introduction and Motivation Process algebras such as Milner's CCS [16] and modular guarded command languages such as McMillan's SMV [15] are important description languages for a wide range of computer systems. The operational meaning of such descriptions is typically captured by a triple M = (S; R; L), where S is a set of states, R the state-transition relation, and L contains atomic state information; the lat...","1999-09-02"
706,241538,"Deciding Properties Of Regular Real Timed Processes","Kim Larsen; Uno Holmer; Wang Yi;",". We discuss the decidability problem associated with verifying properties of processes expressed in the real time process calculus TCCS of [W90]. A regular subcalculus TC of TCCS is considered. Two operational semantics, and associated timed notions of bisimulation, are given: a standard infinite semantics, and a symbolic finite semantics. The consistency between the two semantics is proved. We show that both the equivalences are decidable for regular processes relative to comparisons between real numbers. As an alternative specification formalism, we present a timed modal logic. It turns out that this logic characterises timed bisimulation equivalence in the sense that equivalent processes enjoy exactly the same properties expressed within the logic. Moreover, we prove that the problem of deciding whether a given regular real timed process satisfies a given property of the logic is decidable, relative to first order assertions about real numbers. Two interpretations of th...","1995-09-12"
707,241722,"Efficient Decision Procedures for Model Checking of Linear Time Logic Properties","Fabio Somenzi; Kavita Ravi; Roderick Bloem;",". We propose an algorithm for LTL model checking based on the classification of the automata and on guided symbolic search. Like most current methods for LTL model checking, our algorithm starts with a tableau construction and uses a model checker for CTL with fairness constraints to prove the existence of fair paths. However, we classify the tableaux according to their structure, and use efficient decision procedures for each class. Guided search applies hints to constrain the transition relation during fixpoint computations. Each fixpoint is thus translated into a sequence of fixpoints that are often much easier to compute than the original one. Our preliminary experimental results suggest that the new algorithm for LTL is quite efficient. In fact, for properties that can be expressed in both CTL and LTL, the algorithm is competitive with the CTL model checking algorithm. 1 Introduction Successful application of model checking requires strategies to bridge the gap betwee...","1999-09-20"
708,242528,"The Design of a Cache-Friendly BDD Library","David E. Long;","We describe the architecture for a new BDD library that is designed to be cache-friendly. The library incorporates a novel technique for terminating searches early during find operations together with a regrouping garbage collector. These features lead to a factor of two improvement in speed on typical examples compared to existing libraries.","1999-02-08"
709,243684,"Synthesizing Petri Nets from State-Based Models","Alex Yakovlev; Jordi Cortadella; Luciano Lavagno; Michael Kishinevsky;","This paper presentsa method to synthesize labeled Petri nets from state-based models. Although state-based models (such as Finite State Machines) are a powerful formalism to describe the behavior of sequential systems,they cannot explicitly express the notions of concurrency, causality and conflict. Petri nets can naturally capture these notions. The proposed method in based on deriving an Elementary Transition System (ETS) from a specification model. Previous work has shown that for any ETS there exists a Petri net with minimum transition count (one transition for each label) with a reachability graph isomorphic to the original ETS. This paper presents the first known approach to obtain an ETS from a non-elementary TS and derive a place-irredundant Petri net. Furthermore, by imposing constraints on the synthesis method, different classes of Petri nets can be derived from the same reachability graph (pure, free choice, unique choice). This method has been implemented and efficiently a...","1995-08-03"
710,243761,"Symbolic Model Checking of Process Networks Using Interval Diagram Techniques","Karsten Strehl; Lothar Thiele;","In this paper, an approach to symbolic model checking of process networks is introduced. It is based on interval decision diagrams (IDDs), a representation of multi-valued functions. Compared to other model checking strategies, IDDs show some important properties that enable the verification of process networks more adequately than with conventional approaches. Additionally, applications concerning scheduling will be shown. A new form of transition relation representation called interval mapping diagrams (IMDs)---and their less general version predicate action diagrams (PADs)---is explained together with the corresponding methods. 1 Introduction Process network models---consisting in general of concurrent processes communicating through unidirectional FIFO queues---as that of Kahn [7, 8] are commonly used, e.g., for specification and synthesis of distributed systems. They form the basis for applications such as real-time scheduling and allocation. Many other models of computation, e...","1998-10-26"
711,245545,"A Comparative Study of Methods for Efficient Reachability Analysis","Marko Rauhamaa;","Six recently proposed methods for efficient reachability analysis of distributed systems are studied and compared: symmetry method (Huber et al), parameterising method (Lindqvist), stubborn set method (Valmari), reduction theory (Berthelot, Haddad), symbolic model checking (Burch et al) and incomplete state-space generation (Holzmann). Petri systems are used as the main presentation formalism. The methods are evaluated by how well they are suited for practical reachability graph generation by computer. One can summarise in brief that the stubborn set method, reduction theory and incomplete state-space generation are found very promising and they provide useful aid to practical reachability analysis. The symmetry method yields a significant reduction in memory space but hardly in generation time. Symbolic model checking is a very general method and immense sample state-spaces have been analysed with it but it is likely that for most practical systems the method performs far worse tha...","1996-01-30"
712,245669,"Symbolic Boolean Manipulation with Ordered Binary Decision Diagrams","null","Introduction to VLSI Systems, Addison-Wesley, Reading, MA. [Meinel 1990] Meinel, C. 1990 Modified branching programs and their computational power, Lecture Notes in Computer Science Vol. 370, G. Goos, and J. Hartmanis, eds. SpringerVerlag, Berlin. [Minato et al 1990] Minato, S., Ishiura, N., and Yajima, S. 1990. Shared binary decision diagram with attributed edges for efficient Boolean function manipulation. Proceedings of the 27th ACM/IEEE Design Automation Conference (Orlando, June), ACM, New York, pp. 52--57. [Ochi et al 1991] Ochi, H., Ishiura, N., and Yajima, S. 1991. Breadth-first manipulation of SBDD of function for vector processing. Proceedings of the 28th ACM/IEEE Design Automation Conference, (San Francisco, June) ACM, New York, pp. 413--416. [Reeves and Irwin 1987] Reeves, D. S., and Irwin, M. J. 1987. Fast methods for switch-level verification of MOS circuits. IEEE Transactions on CAD/IC CA","1997-03-31"
713,246559,"Kinematic Synthesis with Configuration Spaces","Cheuk-san (edward Wang; Devika Subramanian;","This paper introduces a new approach to the conceptual design of mechanical systems from qualitative specifications of behaviour. The power of the approach stems from the integration of techniques in qualitative physics and constraint programming. We illustrate the approach with an effective kinematic synthesis method that reasons with qualitative representations of configuration spaces using constraint programming. 1 Introduction The overall goal of our research is to derive computational theories of conceptual or pre-parametric design. As manufacturing technologies change, as new materials are developed, and as new design constraints emerge (e.g., designs with recyclable parts, and designs that assemble and disassemble easily), basic conceptual design procedures for electro-mechanical systems require effective use of computer tools in the early stages of design. Our specific aim is to use methods from qualitative physics and constraint programming to build new computational pro...","1994-07-05"
714,247088,"On the Relation Between BDDs and FDDs","Bernd Becker; Johann Wolfgang Goethe-universitat; Ralph Werchner; Rolf Drechsler;","Data structures for Boolean functions build an essential component of design automation tools, especially in the area of logic synthesis. The state of the art data structure is the ordered binary decision diagram (OBDD), which results from general binary decision diagrams (BDDs), also called branching programs, by ordering restrictions. In the context of EXOR-based logic synthesis another type of decision diagram (DD), called (ordered) functional decision diagram ((O)FDD) becomes increasingly important. We study the relation between (ordered, free) BDDs and FDDs. Both, BDDs and FDDs, result from DDs by defining the represented function in different ways. If the underlying DD is complete, the relation between both types of interpretation can be described by a Boolean transformation . This allows us to relate the FDD-size of f and the BDD-size of (f) also in the case that the corresponding DDs are free or ordered, but not (necessarily) complete. We use this property to derive...","1997-03-05"
715,247211,"Submodule Construction as Equation Solving in CCS","Joachim Parrow;","In top-down design methodologies the following problem arises: given specifications of a system and of some of its submodules, derive a specification for the remaining submodules. We formulate this problem in CCS as an equation (AjX)nL B, where X is unknown, B represents the whole system, A the known submodules, and L the channels over which the submodules interact. We present a procedure for solving such equations by successive transformation of equations into simpler equations in parallel with generation of a solution. The procedure has been implemented as a semiautomatic program, where the user may interact in order to guide the transformations towards particular solutions. As an example we demonstrate the automatic generation of receivers of two versions of the Alternating Bit protocol. 1 Introduction One of the most important and difficult fields in computer science is to develop methods for construction of complex systems. Most design methodologies rely on modularizat...","1996-02-08"
716,247910,"Technique and Tool for Symbolic Representation and Manipulation of Stochastic Transition Systems","Markus Siegle;","We present a new approach to the compact symbolic representation of stochastic transition systems, based on Decision Node BDDs, a novel stochastic extension of BDDs. Parallel composition of components can be performed on the basis of this new data structure. We also discuss symbolic state space reduction by Markovian bisimulation. In many areas of system design and analysis, there is the problem of generating, manipulating and analysing very large state spaces. We focus on stochastic labelled transition systems (SLTS) where each transition is labelled by an action name and an exponential delay. Such SLTSs (which originate, e.g., from Stochastic Process Algebra specifications [3]) can be interpreted as Markov chains and used for performability analysis. We propose a novel approach to SLTS representation and manipulation which is based on symbolic techniques. This is motivated by the fact that, in recent years, the problem of representing and analysing large state spaces has been very ...","1998-07-17"
717,248384,"Experiments in Theorem Proving and Model Checking for Protocol Verification","And Natarajan Shankar; Klaus Havelund;",". Communication protocols pose interesting and difficult challenges for verification technologies. The state spaces of interesting protocols are either infinite or too large for finite-state verification techniques like model checking and state exploration. Theorem proving is also not effective since the formal correctness proofs of these protocols can be long and complicated. We describe a series of protocol verification experiments culminating in a methodology where theorem proving is used to abstract out the sources of unboundedness in the protocol to yield a skeletal protocol that can be verified using model checking. Our experiments focus on the Philips bounded retransmission protocol originally studied by Groote and van de Pol and by Helmink, Sellink, and Vaandrager. First, a scaled-down version of the protocol is analyzed using the MurOE state exploration tool as a debugging aid and then translated into the PVS specification language. The PVS verification of the ge...","1998-01-12"
718,248596,"Model Checking Large Software Specifications","David Notkin; Francesmary Modugno; Jon Reese; Paul Beame; Richard J. Anderson; Steve Burns; William Chan;","In this paper we present our results and experiences of using symbolic model checking to study the specification of an aircraft collision avoidance system. Symbolic model checking has been highly successful when applied to hardware systems. We are interested in the question of whether or not model checking techniques can be applied to large software specifications. To investigate this, we translated a portion of the finite-state specification of TCAS II (Traffic Alert and Collision Avoidance System) into a form accepted by a model checker (SMV). We successfully used the model checker to investigate a number of dynamic properties of the system. We report on our experiences, describing our approach to translating from RSML to SMV and our methods for achieving acceptable performance in model checking, and giving a summary of the properties that we were able to check. We consider the paper as a data point that provides reason for optimism about the potential for successful applic...","1997-04-24"
719,248598,"Algebraic Decision Diagrams and their Applications","A. Frohm; Abelardo Pardo; Bahar Erica; Charles M. Gaona; Enrico Macii; Fabio Somenzi; Gary D. Hachtel; R. Iris;","In this paper we present theory and experiments on the Algebraic Decision Diagrams (ADD's). These diagrams extend BDD's by allowing values from an arbitrary finite domain to be associated with the terminal nodes. We present a treatment founded in boolean algebras and discuss algorithms and results in applications like matrix multiplication and shortest path algorithms. Furthermore, we outline possible applications of ADD's to logic synthesis, formal verification, and testing of digital systems. 1 Introduction Binary Decision Diagrams (BDD's) [1] have significantly changed the landscape of logic synthesis, formal verification, and testing. Combined with symbolic graph algorithms, they have made the solution of many difficult problems possible. In symbolic graph algorithms node and edge sets are stored symbolically, that is, in terms of their characteristic functions. BDD's provide an efficient and canonical form of representation of those functions. Many successful applications of sy...","1996-08-05"
720,248906,"Inverting the Abstraction Mapping: A Methodology for Hardware Verification","David Cyrluk;","ion Mapping: A Methodology for Hardware Verification David Cyrluk ? Dept. of Computer Science, Stanford University, Stanford CA 94305 and Computer Science Laboratory, SRI International, Menlo Park, CA 94025 cyrluk@cs.stanford.edu Abstract. Abstraction mappings have become a standard approach to verifying the correctness of processors. When used in a straightforward manner this approach suffers from generating extremely large intermediate terms that have to be simplified. In an interactive theorem prover the complete expansion of the abstraction mapping is not even possible. Yet, with human guidance it is interactive theorem proving that is applied to examples too large to be handled by automated methods. We present a methodology for verifying the correctness of processors that aims to limit the size of intermediate terms generated in an interactive proof and to manage the complexity of the proof search. The main idea of this methodology is that, instead of expanding t...","1996-08-20"
721,248936,"Improving Efficiency of Symbolic Model Checking for State-Based System Requirements","David Notkin; Paul Beame; Richard J. Anderson; William Chan;","We present various techniques for improving the time and space efficiency of symbolic model checking for system requirements specified as synchronous finite state machines. We used these techniques in our analysis of the system requirements specification of TCAS II, a complex aircraft collision avoidance system. They together reduce the time and space complexities by orders of magnitude, making feasible some analysis that was previously intractable. The TCAS II requirements were written in RSML, a dialect of statecharts. Keywords Formal verification, symbolic model checking, reachability analysis, binary decision diagrams, partitioned transition relation, statecharts, RSML, TCAS II, system requirements specification, abstraction. 1 Introduction Formal verification based on state exploration can be considered an extreme form of simulation: every possible behavior of the system is checked for correctness. Symbolic model checking [?] using binary decision diagrams (BDDs) [?] is an effic...","1998-02-04"
722,249024,"Compact representation of large performability models based on extended BDDs","Markus Siegle;",": We discuss compact symbolic representations of large state spaces, based on binary decision diagrams (BDD). Extensions of BDDs are considered, in order to represent stochastic transition systems for performability analysis. Parallel composition of components can be performed in this context, without leading to state space explosion. Furthermore, we discuss state space reduction by Markovian bisimulation, also based on symbolic techniques. 1 Introduction During system design and analysis, there often arises the problem of generating, manipulating and analysing large labelled transition systems (LTS). Such transition systems can be very difficult to handle in practice, due to memory limitations. In this paper, the focus is on stochastic LTSs (SLTS) for performability analysis, where transitions are associated with exponential delays. For example, SLTSs are generated from stochastic process algebra (SPA) models. Under certain conditions, such SLTSs can be interpreted as Markov chains. ...","1998-07-17"
723,249127,"A Logic-Model Semantics for SCR Software Requirements","Joanne M. Atlee; Michael A. Buckley;",". This paper presents a simple logic-model semantics for Software Cost Reduction (SCR) software requirements. Such a semantics enables modelchecking of native SCR requirements and obviates the need to transform the requirements for analysis. The paper also proposes modal-logic abbreviations for expressing conditioned events in temporal-logic formulae. The Symbolic Model Verifier (SMV) is used to verify that an SCR requirements specification enforces desired global requirements, expressed as formulae in the enhanced logic. The properties of a small system (an automobile cruise control system) are verified, including an invariant property that could not be verified previously. The paper concludes with a discussion of how other requirements notations for conditioned-event-driven systems could be similarly checked. Keywords: Reactive Systems, Software Requirements, Formal Semantics, Model Checking. 1 Introduction Precise notations have been developed to specify unambigu...","1996-02-23"
724,250620,"Multi Terminal Binary Decision Diagrams to Represent and Analyse Continuous Time Markov Chains","And J. Meyer-kayser; And M. Siegle; H. Hermanns;",". Binary Decision Diagrams (BDDs) have gained high attention in the context of design and verification of digital circuits. They have successfully been employed to encode very large state spaces in an efficient, symbolic way. Multi terminal BDDs (MTBDDs) are generalisations of BDDs from Boolean values to values of any finite domain. In this paper, we investigate the applicability of MTBDDs to the symbolic representation of continuous time Markov chains, derived from high-level formalisms, such as queueing networks or process algebras. Based on this data structure, we discuss iterative solution algorithms to compute the steady-state probability vector that work in a completely symbolic way. We highlight a number of lessons learned, using a set of small examples. 1 Introduction State space explosion is a notorious problem in the context of continuous time Markov chains (CTMC) derived from high-level formalisms such as queueing networks, stochastic automata, stochastic Petri n...","1999-10-25"
725,251082,"On the Role of Basic Design Concepts in Behaviour Structuring","Chris A. Vissers; Dick A. C. Quartel; Henry M. Franken; Lus Ferreira Pires; Marten J. Van Sinderen;","This paper presents some basic design concepts for the design of open distributed systems. These concepts should form the basis for the development of effective design methodologies. The paper discusses how design concepts, such as interaction, action and causality relation, can be used for modelling and structuring behaviours of functional entities in a distributed environment. The paper also addresses some consequences of the application of these design concepts such as the choice of language elements and operations to represent behaviour structure, the structuring of the design process, and the definition of design operations for behaviour refinement. Keywords: design concepts, interactions, actions, causality relations, behaviour specification, behaviour structuring techniques, open distributed systems, behaviour refinement, design milestones. 1 Introduction Many research activities on systematic approaches to distributed systems design are being carried out, such as th...","1998-05-07"
726,251381,"Simulated Annealing To Improve Variable Orderings For OBDDs","And Ingo Wegener; Beate Bollig; Martin Lobbing;","The choice of a good variable ordering is crucial in applications of Ordered Binary Decision Diagrams (OBDDs). A simulated annealing approach with a new type of neighborhood is presented and analyzed. Better results as by known simulated annealing algorithms and heuristics are obtained. Some theoretical results underlining the experiments are stated. Keywords: formal verification, formal techniques and methods for verification, symbolic manipulation, OBDDs, variable orderings, simulated annealing. Supported in part by DFG grant We 1066/7--1. 1 1. INTRODUCTION Boolean function manipulation is an important component of many logic synthesis algorithms including logic optimization and logic verification of combinational and sequential circuits. Ordered Binary Decision Diagrams (OBDDs) have proven useful in these applications as an efficient data structure for the representation and manipulation of Boolean functions (Bryant [2]), if good variable orderings are known. The best algor...","1995-06-20"
727,251802,"Verification of IEEE Compliant Subtractive Division Algorithms","James F. Leathrum; Paul S. Miner;",". A parameterized definition of subtractive floating point division algorithms is presented and verified using PVS. The general algorithm is proven to satisfy a formal definition of an IEEE standard for floating point arithmetic. The utility of the general specification is illustrated using a number of different instances of the general algorithm. 1 Introduction As computing systems become more complex, it becomes increasingly difficult to ensure that testing fully exercises the design. This was made abundantly clear by the infamous bug in the floating point unit of the Intel Pentium(tm) microprocessor. The bug consists of five missing entries in a lookup table. Pratt [21] provides a thorough analysis of this error. He provides compelling arguments that a thorough manual analysis of a design may still allow errors to evade detection. This is particularly true if the flaw is in a region of the design that is thought to be unreachable. Machine assisted reasoning is crucial to preve...","1997-01-18"
728,252203,"Verification of Infinite State Systems by Compositional Model Checking","K. L. Mcmillan;",". Compositional model checking methods can be used to reduce the formal verification of a complex system to model checking problems of tractably small size. However, such techniques are difficult to apply to systems that have large data types, such as memory addresses, or large data arrays such as memories or FIFO buffers. They are also limited to the verification of systems with fixed finite resources. In this paper, a method of compositional verification is presented that uses the combination of temporal case splitting and data type reductions to reduce types of unbounded range to small finite types, and arrays of unbounded size to small fixed-size arrays. The method also supports the use of uninterpreted functions in a novel way, that allows model checking to be applied to systems with uninterpreted functions. These techniques are implemented in a proof assistant that also supports compositional reasoning and reductions via symmetry. Application of the method is illustrated...","1999-03-23"
729,252435,"Requirements Specification for Process-Control Systems","Holly Hildreth; Jon D. Reese; Mats P. E. Heimdahl; Nancy G. Leveson;","This paper describes an approach to writing requirements specifications for processcontrol systems, a specification language that supports this approach, and an example application of the approach and the language on an industrial aircraft collision avoidance system (TCAS II). The example specification demonstrates (1) the practicality of writing a formal requirements specification for a complex, process-control system and (2) the feasibility of building a formal model of a system using a specification language that is readable and reviewable by applications experts who are not computer scientists or mathematicians. Some lessons learned in the process of this work, which are applicable both to forward and reverse engineering, are also presented. Index Terms: process control, reactive systems, requirements, blackbox specifications, formal methods, safety analysis, reverse engineering 1 Introduction Embedded software is part of a larger system and has a primary purpose of pro...","1999-09-23"
730,254251,"Symbolic Model Checking with Sets of States Represented as Formulas","Per Bjesse;",". We analyse traditional symbolic model checking [BCL + 94] to isolate the operations that a representation for sets of states need to support and present a different view, where sets of states are represented by propositional logic formulas. High level algorithms for symbolic model checking based on Stalmarck's method are then presented, which results in a model checking procedure closely related to bounded model checking [BCC + ]. Our method lifts the requirement on finding bounds and allows intermediate minimisations of state set representations. We also sketch how to generalise the approach to stronger logics. 1 Introduction Propositional temporal logic theorem proving is PSPACE-complete, and often infeasible for industrial-size systems. Model checking is a reduction of this complex problem to a possibly exponential number of subproblems that hopefully have better characteristics. Different model checking approaches achieve success with different classes of systems; ...","1999-07-29"
731,254456,"State-Space Caching Revisited","Didier Pirottin; Gerard J. Holzmann; Patrice Godefroid;","State-space caching is a verification technique for finite-state concurrent systems. It performs an exhaustive exploration of the state-space of the system being checked while storing only all states of just one execution sequence plus as many other previously visited states as available memory allows. So far, this technique has been of little practical significance: it allows one to reduce memory usage by only two to three times, before an unacceptable blow-up of the run-time overhead sets in. The explosion of the run-time requirements is due to redundant multiple explorations of unstored parts of the state-space. Indeed, almost all states in the state-space of concurrent systems are typically reached several times during the search. In this paper, we present a method to tackle the main cause of this prohibitive state matching: the exploration of all possible interleavings of concurrent executions of the system, which all lead to the same state. Then, we show that, in many ...","1994-05-11"
732,254899,"Tight Integration of Combinational Verification Methods","Jerry R. Burch; Vigyan Singhal;","Combinational verification is an important piece of most equivalence checking tools. In the recent past, many combinational verification algorithms have appeared in the literature. Previous results show that these algorithms are able to exploit circuit similarity to successfully verify large designs. However, none of these strategies seems to work when the two input designs are not equivalent. We present our combinational verification algorithm, with evidence, that is designed to be robust for both the positive and the negative problem instances. We also show that a tight integration of different verification techniques, as opposed to a coarse integration of different algorithm, is more effective at solving hard instances. 1 Introduction In this paper we study the problem of combinational verification. Given two combinational netlists, which have the same set of inputs and outputs, the problem of combinational verification is to determine if for every possible input combination, each...","1998-10-26"
733,255465,"Bilattices and the Semantics of Logic Programming","null","Bilattices, due to M. Ginsberg, are a family of truth value spaces that allow elegantly for missing or conflicting information. The simplest example is Belnap's four-valued logic, based on classical two-valued logic. Among other examples are those based on finite many-valued logics, and on probabilistic valued logic. A fixed point semantics is developed for logic programming, allowing any bilattice as the space of truth values. The mathematics is little more complex than in the classical two-valued setting, but the result provides a natural semantics for distributed logic programs, including those involving confidence factors. The classical two-valued and the Kripke/Kleene three-valued semantics become special cases, since the logics involved are natural sublogics of Belnap's logic, the logic given by the simplest bilattice. 1 Introduction Often useful information is spread over a number of sites (""Does anybody know, did Willie wear a hat when he left this morning?"") that can be speci...","1999-02-13"
734,255505,"Sampling Schemes for Computing OBDD Variable Orderings","Jawahar Jain; Masahiro Fujita; William Adams;",": We suggest some novel variable ordering techniques based upon the notion of sampling. Such techniques can produce highly efiective static variable orders, and can thus be employed in numerous problems where current static variable ordering techniques prove totally inadequate. They can also augmentvarious reordering techniques thereby helping to produce far superior variable orders in a comparable, or lesser, amount of time. Importantly,wehave been able to build BDDs of circuits which could not be represented previously using numerous other reordering packages. 1 Introduction Ordered Binary Decision Diagrams fi3fi have found extensive use in various algorithms for analyzing Boolean functions, and thus play an important role in numerous CAD related applications. Unfortunately, in many applications, very large sized BDDs can be generated which can render a BDD based analysis scheme impractical or ineficient. In particular, OBDD sizes are very sensitive to the order chosen on input variabl...","1998-10-26"
735,256536,"APRIL: A Processor Architecture for Multiprocessing","Anant Agarwal; Benghong Lim; David Kranz; John Kubiatowicz;","Processors in large#scale multiprocessors must be able to tolerate large communication latencies and synchro# nization delays. This paper describes the architecture of a rapid#context#switching processor called APRIL with support for #ne#grain threads and synchroniza# tion. APRIL achieves high single#thread performance and supports virtual dynamic threads. A commercial RISC#based implementation of APRIL and a run#time software system that can switch contexts in about 10 cycles is described. Measurements taken for several par# allel applications on an APRIL simulator show that the overhead for supporting parallel tasks based on futures is reduced by a factor of twoover a corresponding im# plementation on the Encore Multimax. The scalability of a multiprocessor based on APRIL is explored using a performance model. We show that the SPARC#based implementation of APRIL can achieve close to 80# pro# cessor utilization with as few as three resident threads per processor in a large#scale cache...","1999-05-26"
736,256682,"Prod Reference Manual","And Tino Pyssysalo; Jaakko Halme; Kari Hiekkanen; Kimmo Varpaaniemi; Tino Pyssysalo;","PROD is a Pr/T-net reachability analysis tool that supports on-the-fly verification of linear time temporal properties with the aid of the stubborn set method. Branching time temporal properties can be verified, too.","1995-08-04"
737,258875,"Using Model Checking to Generate Tests from Specifications","Paul E. Ammann; Paul E. Black; William Majurski;","We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker generates counterexamples which distinguish the variations from the original specification. The counterexamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case generation is automatic","1999-01-05"
738,259425,"Compilation and Verification of LOTOS Specifications","Centre Hitella; Hubert Garavel; Joseph Sifakis;","This paper presents the main features of the Caesar system, intended for formal","1999-11-11"
739,259812,"Regular State Machines","Jurgen Teich; Karsten Strehl; Lothar Thiele;","In this paper, we introduce a model called regular state machines (RSMs) that characterizes a class of state transition systems with regular transition behavior. It turns out that many process graph models such as synchronous dataflow graphs and Petri nets have a state transition system that may be described and analyzed in the RSM model. In particular, the proposed approach unifies methods known for the above-mentioned subclasses and yields new results concerning boundedness, deadlocks, scheduling, and formal verification. Keywords: state transition system, model of computation, infinite state, scheduling, formal verification, Petri net 1 Introduction In this paper, a model of computation is introduced called regular state machine (RSM) that fills an existing gap for describing systems with regular, repetitive, and infinite state transition behavior e#ciently. In this introduction, we shortly review existing models of computation and give a motivation for the need of a n...","1999-12-07"
740,259823,"An Expressive Real-Time CCS","Colin Fidge;",". This paper describes a new `real-time' process algebra that exhibits the advantages of both the `constraint-oriented' and `marker variable' specification styles. Its definition extends CCS, firstly with a basic notion of absolute time added to actions, and then with relative timing expressions which may refer to time markers. 1 Introduction Although many proposals for real-time process algebras have been made, they generally suffer from overly complex semantics, especially where the concurrency operators are concerned, or rely on counter-intuitive priority-based concepts in order to achieve expressibility. We define a new real-time process algebra which combines straightforward semantics with good expressive power. To do this we have combined the best features of two previously separate streams of real-time research, the `constraint-oriented' [2, 1] and `marker variable' [4] approaches. As a concrete illustration we describe an extended version of Milner's Calculus of Communicating...","1996-07-18"
741,259899,"A Reduction Theory For Coloured Nets","Universite Paris;","This paper presents the generalization for the coloured nets of the most efficient reductions defined by Berthelot for Petri nets. First a methodology of the generalization is given which is independent from the reduction one wants to generalize. Then based on this methodology, we define extensions of the implicit place transformation and the pre and post agglomeration of transitions. For each reduction we prove that the reduced net has exactly the same properties as the original net. Finally we completely reduce an improved modelling of the data base management with multiple copies showing, by this way, its correctness. INTRODUCTION In any theoretical model of computation, a useful method to prove properties of an object (program, protocol, ...) of this model is to reduce this object such that the simplified object has the same properties as the original one. In his thesis, Berthelot [Ber83] has defined ten reductions of Petri nets and has shown how they can be efficiently used in ...","1999-11-12"
742,259947,"Semantically-Sensitive Macroprocessing","Semantically-sensitive Macroprocessing; William Maddox;","Conventional procedure and type definition mechanisms are not sufficiently powerful to express many programming abstractions that can be captured by syntactic transformations. Unfortunately, conventional macroprocessing is oblivious to the semantics of the base language, resulting in scoping anomalies, poor handling of static semantic errors, and an inability to perform transformations dependent on semantic attributes of the manipulated program. We introduce a new mechanism, semantic macros, which permit such transformations a significant level of access to the static semantic properties of the program fragments they manipulate. In this way, new static semantic processing, including compilation of embedded languages with a rich static semantics of their own, can be incorporated into user-defined language extensions. A proof-of-concept language, XL, is described which embodies this mechanism. Sponsored by the Defense Advanced Research Projects Agency (DoD), monitored by Spac...","1997-03-26"
743,260059,"Model Checking Lossy Vector Addition Systems","Ahmed Bouajjani; Richard Mayr;","Lossy VASS (vector addition systems with states) are defined as a subclass of VASS in analogy to lossy FIFO-channel systems. They can be used to model concurrent systems with unreliable communication. We analyze the decidability of model checking problems for lossy systems and several branching-time and linear-time temporal logics. We present an almost complete picture of the decidability of model checking for normal VASS, lossy VASS and lossy VASS with test for zero. 1 Introduction Systems are usually modeled by finite control transition systems with different kinds of variables and data structures like counters, clocks, stacks, fifo-channels, etc. One of the widely used models of concurrent systems is the model of Petri nets which is equivalent to the model of vector addition systems with states (VASS for short). These models can be considered as particular cases of counter machines where tests to zero are forbidden (the addition of inhibitor arcs gives them the full power o...","1999-06-16"
744,262766,"Automated Generation Of Dsp Program Development Tools Using A Machine Description Formalism +","A. Fauth; A. Knoll; Technische Universitt Berlin;","We introduce a retargetable microcode generator for application specific digital signal processors (ASDSPs). The primary goal of our work is to quickly provide system architects with the set of tools necessary for program development (assemblers, instruction set simulators, debuggers and compilers); in particular when the processor architecture is refined simultaneously with the algorithm. After a modification of the architecture, only the machine description written in our language nML must be altered, the tools are then produced automatically. The machine description need not explicitly list every possible instruction in full length. Instead, a derivation tree is described. Through the extensive use of inheritance and sharing of properties, this description can be very compact. Based on the latter, the recognition of critical data paths and the analysis of machine inherent parallelism is solely performed by the tool generator. 1 INTRODUCTION Digital signal processors (DSPs) are sp...","1995-10-03"
745,263752,"Disconnected Operation in the Coda File System","James J. Kistler; M. Satyanarayanan;","would like to enjoy the benefits of a shared data repository, Disconnected operation is a mode of operation that enables but be able to continue critical work when that repository is a client to continue accessing critical data during temporary inaccessible. We call the latter mode of operation failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in disconnected operation, because it represents a temporary supporting portable computers. In this paper, we show that deviation from normal operation as a client of a shared disconnected operation is feasible, efficient and usable by repository. describing its design and implementation in the Coda File System. The central idea behind our work is that caching of In this paper we show that disconnected operation in a file data, now widely used for performance, can also be system is indeed feasible, efficient and usable. The central exploited to improve availability. idea b...","1999-05-15"
746,265502,"Global Code Selection for Directed Acyclic Graphs","Alois Knoll; Andreas Fauth; Carsten Muller; Gunter Hommel;",". We describe a novel technique for code selection based on data-flow graphs, which arise naturally in the domain of digital signal processing. Code selection is the optimized mapping of abstract operations to partial machine instructions. The presented method performs an important task within the retargetable microcode generator CBC, which was designed to cope with the requirements arising in the context of custom digital signal processor (DSP) programming. The algorithm exploits a graph representation in which control-flow is modeled by scopes. 1 Introduction In the domain of medium-throughput digital signal processing, micro-programmable processor cores are frequently chosen for system realization. By adding dedicated hardware (accelerator paths), these cores are tailored to the needs of new applications. Optimized processor modules can be reused, which is a major benefit compared to high-level synthesis [28] where a completely new design is developed for each application. ...","1995-10-03"
747,266096,"Automatic Analysis of Consistency between Implementations and Requirements: A Case Study","John Gannon; Marsha Chechik;","Formal methods like model checking can be used to demonstrate that safety properties of event-based systems are enforced by the system's requirements. Unfortunately, proving these properties provides no guarantee that they will be preserved in an implementation of the system. This paper describes a tool, called Analyzer, which discovers instances of inconsistency and incompleteness in implementations, and a case study of its use. Analyzer uses requirements information to check that all state transitions implemented in the code are exactly those specified in the requirements. If programmers annotate their implementations with comments describing changes of values of requirements variables, our tool checks it against the requirements. Analyzer can also verify that the implementation is a model of user-specified safety properties. The tool can perform interprocedural analysis and verify properties which involve multiple state machines. In our case study, we analyzed the requirements and...","1996-10-27"
748,268967,"Automating Model Checking for Autonomous Systems","Charles Pecheur; Reid Simmons;","While autonomous systems offer great promise in terms of capability and flexibility, their reliability is particularly hard to assess. This paper describes research in the use of model checking to support the development of reliable autonomy software. In particular, it presents tools and techniques that we are developing to facilitate the integration of model checking into the main software development cycle. The basic approach is to translate highlevel models used by autonomy systems into the specification language of the SMV model checker, verify them using SMV, translate diagnostics back to the source language and visualize and explain those diagnostics. This approach has been applied to MPL models for the Livingstone fault diagnosis system and to TDL task descriptions for mobile robot systems. 1 Introduction Autonomous systems rely on intelligent inference capabilities to be able to take appropriate actions even in unforeseen circumstances. They enable a whole range of new applic...","2000-01-24"
749,269945,"Formalization and Analysis of the Separation Minima for the North Atlantic Region: Complete Specification and Analysis Results","Airspace Management Systems; Gerry Pelletier; Hughes International; Jeffrey J. Joyce; Nancy A. Day;","This report describes work to formalize and validate a specification of the separation minima for aircraft in the North Atlantic (NAT) region completed by researchers at the University of British Columbia in collaboration with Hughes International Airspace Management Systems. Our formal representation of these separation minima is given in a mixture of a tabular style of specification and textual predicate logic. We analyzed the tables for completeness, consistency and symmetry. This report includes the full specification and complete analysis results. Contents 1. Introduction 2. Specification Notation and Analysis 3. Analysis Results 4. Summary 5. Acknowledgments 6. References Appendix A: Full Specification Appendix B: Full Analysis results Appendix C: A Brief Introduction to the S Notation 1 Introduction This report describes work to formalize and validate a specification of the separation minima for aircraft in the North Atlantic (NAT) region completed at the University of Briti...","1999-03-12"
750,270114,"Deterministic State Space Planning with BDDs","And Frank Reffel; Stefan Edelkamp;",". This short paper (cf. [5]) proposes a planner that uses BDDs to compactly represent sets of propositionally represented states. Using this representation, accurate reachability analysis and backward chaining can apparently be carried out without necessarily encountering exponential representation explosion. The main objectives are the interest in optimal solutions, the generality and the conciseness of the approach. The algorithms are tested against the AIPS'98 planning competition problems and lead to substantial improvements to existing solutions. The project Mips abbreviates intelligent model checking and planning system and encapsulates algorithms and data structures for model checking and planning with binary decision diagrams (BDDs) [2]. BDDs have the expressive power to efficiently encode and manipulate large sets of bit-strings. Therefore, BDDs are capable of symbolically representing both the states within the planning space and the operators applied. The planning a...","2000-01-18"
751,270369,"Formal Verification of Statecharts with Instantaneous Chain Reactions","Peter Scholz;",". We present a method for symbolic model checking of - Charts, a Statecharts dialect with instantaneous broadcast communication. Due to this communication concept, -Charts satisfy the perfect synchrony hypothesis. The well-known causality conflicts that arise under instantaneous feedback from negative trigger conditions are resolved semantically through oracle signals. We have implemented a prototypical tool that translates -Charts specifications into -calculus formulae. These formulae are checked against temporal specifications using a - calculus verifier. 1 Introduction Statecharts [4] are a visual specification language for reactive systems. They extend conventional state transition diagrams with structuring and communication mechanisms. Since there is also tool support through several providers like r-active or i-Logix (Statemate [7]), Statecharts have become quite successful in industry. However, the semantics of Statecharts as used in Statemate is based on a delayed br...","1999-10-20"
752,270541,"TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems","Alan L. Cox; Hya Dwarkadas; Pete Keleher; Willy Zwaenepoel;","TreadMarks is a distributed shared memory #DSM# system for standard Unix systems suchas SunOS and Ultrix. This paper presents a performance evaluation of TreadMarks running on Ultrix using DECstation-5000#240's that are connected by a 100-Mbps switch-based ATM LAN and a 10-Mbps Ethernet. Our objective is to determine the e#ciency of a user-level DSM implementation on commercially available workstations and operating systems. Weachieved good speedups on the 8-processor ATM network for Jacobi #7.4#, TSP #7.2#, Quicksort #6.3#, and ILINK #5.7#. For a slightly modi#ed version of Water from the SPLASH benchmark suite, weachieved only moderate speedups #4.0# due to the high communication and synchronization rate. Speedups decline on the 10-Mbps Ethernet #5.5 for Jacobi, 6.5 for TSP, 4.2 for Quicksort, 5.1 for ILINK, and 2.1 for Water#, reffecting the bandwidth limitations of the Ethernet. These results support the contention that, with suitable networking technology, DSM is a viabl...","1999-08-09"
753,270992,"Symbolic Methods Applied to Formal Verification and Synthesis in Embedded Systems Design","Eidgenssischetechnische Hochschule Zrich; Karsten Strehl;","In many of today's technical appliances, embedded systems are deployed on an increasing scale. This trend is expected to massively gain in importance, especially for applications in consumer and automotive electronics, telecommunications, medical engineering, and avionics. Embedded systems can be characterized as follows: . They mostly consist of both hardware and software and perform control or communication tasks within an overall system, to which they are connected via networks, sensors, and actors. . In general, they are heterogeneous, often distributed systems with realtime requirements which may work on both transformative and reactive domains. . They typically are not directly visible to the user who interacts unconciously with them. The design of embedded systems poses significant challenges since system complexity grows rapidly. In particular, very elaborate design methods are necessary, and quality requirements are high because embedded systems often realize safety-cri...","2000-02-28"
754,271339,"A Fast Stochastic Error-Descent Algorithm for Supervised Learning and Optimization","Gert Cauwenberghs;","A parallel stochastic algorithm is investigated for error-descent learning and optimization in deterministic networks of arbitrary topology. No explicit information about internal network structure is needed. The method is based on the model-free distributed learning mechanism of Dembo and Kailath. A modified parameter update rule is proposed by which each individual parameter vector perturbation contributes a decrease in error. A substantially faster learning speed is hence allowed. Furthermore, the modified algorithm supports learning time-varying features in dynamical networks. We analyze the convergence and scaling properties of the algorithm, and present simulation results for dynamic trajectory learning in recurrent networks. 1 Background and Motivation We address general optimization tasks that require finding a set of constant parameter values p i that minimize a given error functional E(p). For supervised learning, the error functional consists of some quantitativ...","1993-01-25"
755,272596,"Bit-Alignment for Retargetable Code Generators","Gert Goossens; Hugo De Man; Koen Schoofs;","When building a bit-true retargetable compiler, every signal type must be implemented exactly as specified, even when the word-length of the signal does not match the length of the available hardware. Extra operations must be introduced in the algorithmic description in order to ensure that the remaining bits do not influence the databits and to assure that signal types are correctly converted from one type to another. An algorithm will be presented which generates code to assure bit-trueness, optimised for the available hardware. 1 Introduction In DSP-algorithms every signal has a certain signal type, indicating the number of bits in the signal, the number of bits behind the binary point, and the way of encoding (signed, unsigned, two's complement, etc ..). In most algorithms, signals with a large variety of types are present. The word length of these signals (the number of bits they contain) can be different from the size of the functional unit they are mapped upon. If the word len...","1994-09-09"
756,274275,"Design Models And Data-Path Mapping For Signal Processing Architectures","Departement Elektrotechniek; Fakulteit Toegepaste Wetenschappen; Prof F. Catthoor; Prof H. De Man; Prof J. Delrue; Prof K. De Vlaminck; Prof P. Wambacq;","building block (ABB) library : : : : : : : : : : 26 2.3.2 Execution unit (EXU) model : data-paths and memories 27 2.3.3 Interconnection network : : : : : : : : : : : : : : : : : : 29 2.3.4 Controller model : : : : : : : : : : : : : : : : : : : : : : 29 2.4 Cathedral-2nd low-level mapping script : : : : : : : : : : : : 30 2.4.1 Starting-point : : : : : : : : : : : : : : : : : : : : : : : : 31 2.4.2 Sequence of subtasks : : : : : : : : : : : : : : : : : : : : 32 2.4.3 Existing prototype tools : : : : : : : : : : : : : : : : : : 36 2.4.4 Comparison with other synthesis scripts : : : : : : : : : 36 2.5 Extension towards a complete synthesis script : : : : : : : : : : 38 2.6 Architectural restrictions : : : : : : : : : : : : : : : : : : : : : : 39 iv CONTENTS v 3 The operation/operator library 41 3.1 Functions for operation and operator types : : : : : : : : : : : : 42 3.1.1 Basic types and encoding languages : : : : : : : : : : : : 42 3.1.2 Functions for operations : : : : : : : : : : : :...","1994-01-06"
757,274411,"Efficient Symbolic State-Space Construction for Asynchronous Systems","Gerald L Uttgen Radu Siminiceanu; Gerald Lttgen; Gianfranco Ciardo; Radu Siminiceanu;",". Many state-of-the-art techniques for the veri#cation of today's complex embedded systems rely on the analysis of their reachable state spaces. In this paper, we develop a new algorithm for the symbolic generation of the state space of asynchronous system models, suchasPetri nets. The algorithm is based on previous work that employs Multi-valuedDecision Diagrams #MDDs# for e#ciently storing sets of reachable states. In contrast to related approaches, however, it fully exploits event locality which is a fundamental semantic property of asynchronous systems. Additionally, the algorithm supports intelligent cache management and achieves faster convergence via advanced iteration control. It is implemented in the tool SMART, and run-time results for several examples taken from the Petri net literature show that the algorithm performs about one order of magnitude faster than the best existing state-space generators. Key words. event locality,multi-valued decision diagrams, state-space expl...","1970-01-01"
758,274744,"Scheduling with Register Constraints for DSP Architectures","Francis Depuydt; Gert Goossens; Hugo De Man;","In recent years, architectural synthesis techniques for real-time signal processing applications have been given much attention. Optimization techniques have been developed which aim at minimizing the chip area for a given throughput. Especially for low to medium throughput applications, one can witness a growing use of embedded programmable architectures , referred to as ""ASIPs "". ASIPs offer programmability and component re-use, which allows to reduce the time-to-market. ASIPs typically have an irregular parallel architecture, with distributed register structures . The data path may contain several special-purpose register-files, of which the number of registers is sometimes parameterizable. New retargetable code generation techniques are needed to generate high-quality machine code for ASIPs. During this process, a close interaction between scheduling and register allocation is essential. This paper addresses the problem of scheduling with tight register constraints . A n...","1994-09-08"
759,275211,"Instruction Set Definition and Instruction Selection for ASIPs","De Man; Goossens Dirk; Johan Van; Lanneer Hugo; Praet Gert;","Application Specific Instruction set Processors (ASIPs) are field or mask programmable processors of which the architecture and instruction set are optimised to a specific application domain. ASIPs offer a high degree of flexibility and are therefore increasingly being used in competitive markets like telecommunications. However, adequate CAD techniques for the design and programming of ASIPs are missing hitherto. In this paper, an interactive approach for the definition of optimised microinstruction sets of ASIPs is presented. A second issue is a method for instruction selection when generating code for a predefined ASIP. A combined instruction set and data-path model is generated, onto which the application is mapped. 1 Introduction Application Specific Instruction set Processors (ASIPs) are in between custom architectures and commercial programmable DSP processors. They allow field and mask programmability but are targeted to a certain class of applications as to limit the amount ...","1994-09-09"
760,275844,"Certifying Compilation and Run-time Code Generation","Luke Hornof; Trevor Jim;",". A certifying compiler takes a source language program and produces object code, as well as a ""certificate"" that can be used to verify that the object code satisfies desirable properties, such as type safety and memory safety. Certifying compilation helps to increase both compiler robustness and program safety. Compiler robustness is improved since some compiler errors can be caught by checking the object code against the certificate immediately after compilation. Program safety is improved because the object code and certificate alone are sufficient to establish safety: even if the object code and certificate are produced on an unknown machine by an unknown compiler and sent over an untrusted network, safe execution is guaranteed as long as the code and certificate pass the verifier. Existing work in certifying compilation has addressed statically generated code. In this paper, we extend this to code generated at run time. Our goal is to combine certifying compilation with run-time ...","1970-01-01"
761,276443,"Process-Oriented Planning and Average-Reward Optimality","Craig Boutilier; Martin L. Puterman;","We argue that many AI planning problems should be viewed as process-oriented, where the aim is to produce a policy or behavior strategy with no termination condition in mind, as opposed to goal-oriented. The full power of Markov decision models, adopted recently for AI planning, becomes apparent with process-oriented problems. The question of appropriate optimality criteria becomes more critical in this case; we argue that average-reward optimality is most suitable. While construction of averageoptimal policies involves a number of subtleties and computational difficulties, certain aspects of the problem can be solved using compact action representations such as Bayes nets. In particular, we provide an algorithm that identifies the structure of the Markov process underlying a planning problem -- a crucial element of constructing average optimal policies -- without explicit enumeration of the problem state space. 1 Introduction The traditional AI planning paradigm re...","1970-01-01"
762,276499,"Detecting Data Races in Parallel Program Executions","Barton P. Miller; Robert H. B. Netzer;","Several methods currently exist for detecting data races in an execution of a shared-memory parallel program. Although these methods address an important aspect of parallel program debugging, they do not precisely define the notion of a data race. As a result, is it not possible to precisely state which data races are detected, nor is the meaning of the reported data races always clear. Furthermore, these methods can sometimes generate false data race reports. They can determine whether a data race was exhibited during an execution, but when more than one data race is reported, only limited indication is given as to which ones are real. This paper addresses these two issues. We first present a model for reasoning about data races, and then present a two-phase approach to data race detection that attempts to validate the accuracy of each detected data race. Our model of data races distinguishes among those data races that actually occurred during an execution (actual data races), those...","1970-01-01"
763,278432,"OBDDs in Heuristic Search","And Frank Reffel; Stefan Edelkamp;",". The use of a lower bound estimate in the search has a tremendous impact on the size of the resulting search trees, whereas OBDDs can be used to efficiently describe sets of states based on their binary encoding. This paper combines these two ideas into a new algorithm BDDA . It challenges both the breadth-first search using OBDDs and the traditional A algorithm. The problem with A is that in many application areas the set of states is too huge to be kept in main memory. In contrast, brute-force breadth-first search using OBDDs unnecessarily expands several nodes. Therefore, we exhibit a new trade-off between time and space requirements and tackle the most important problem in heuristic search, the overcoming of space limitations while avoiding a strong penalty in time. We evaluate our approach in the (n 2 Gamma 1)-Puzzle and within Sokoban. 1 Introduction In heuristic search we explore the state space by generating the successor set over and over again. The choice...","1970-01-01"
764,278565,"Automatic Verification of Requirements Implementation","John Gannon; Marsha Chechik;","Requirements of event-based systems can be automatically analyzed to determine if certain safety properties hold. However, we lack comparable methods to verify that implementations maintain the properties guaranteed by the requirements. We have built a tool that compares implementations written in C with their requirements. Requirements describe events which cause state transitions. Implementations are annotated to describe changes in the values of their requirement's variables, and dataflow analysis techniques are used to determine the set of events which cause particular state changes. To show that an implementation is consistent with its requirements, we show that each event causing a change of state in the implementation appears in the requirements, and that all the events specified to cause state changes in the requirements appear in the implementation. The annotation language encourages programmers to describe local program behaviors. These behaviors are collected into sy...","1999-06-01"
765,278853,"Optimal Scheduling and Software Pipelining of Repetitive Signal Flow Graphs with Delay Line Optimization","F. Depuydt; G. Goossens; H. De Man; W. Geurts;","Software pipelining can have an enormous impact on the clock cycle count and hence on the performance of a real-time signal processing design. Because it pays off to invest CPU time in the optimal software pipelining of time-critical parts of a design, an integer programming approach is proposed for simultaneous scheduling and software pipelining. The integer programming techniques in the literature do not support cyclic (repetitive) signal flow graphs, and/or do not allow optimization of the storage cost of delay lines during software pipelining. The new contributions in this paper are the full integration of software pipelining and scheduling, based on a new timing model that supports cyclic signal flow graphs and optimization of delay line storage costs. Experiments with several real-time signal processing applications have shown the practical applicability of the approach. 1 Introduction Parallel architectures are used to increase the throughput of computationally intensive appli...","1994-09-09"
766,279207,"Formalization and Analysis of the Separation Minima for Aircraft in the North Atlantic Region","Gerry Pelletier; Jeffrey J. Joyce; Nancy A. Day;","The formalization and analysis of an air traffic control separation minima serves in this paper as an illustration of an approach that uses formal operational semantics to drive the automated analysis of specifications. This contrasts with the approach of translating one notation into the input format for an analysis tool, or hard-coding the semantics of a particular notation into the implementation of an analysis technique. The semantic functions capture the structure of the specification and can be directly evaluated to map a notation to a rigourous mathematical foundation. This work contributes to a greater appreciation of how the structure of a specification (e.g., the organization of a table), not just the semantics, is an important input to many analysis functions. Building upon a common mathematical foundation, different notations can be combined to support an integrated approach to the analysis of a formal specification. A related issue is the importance of being able to rever...","1970-01-01"
767,280665,"Generation of Hardware Machine Models from Instruction Set Descriptions","A. Fauth; A. Knoll; M. Freericks;","This paper describes how a modular machine description, which specifies the functionality and the binary representation of an instruction set, can be transformed into a hardware model. This model is built from few generic hardware entities (registers, memories, arithmetic/logic operators, selectors and connections) and may eventually serve as an input to high-level hardware synthesis tools. The transformation steps on the way from the machine description to the hardware model are explained by giving an example.","1995-09-27"
768,281038,"Object-Oriented Concurrent Constraint Programming in Oz","Gert Smolka; Jorg Wurtz; Martin Henz;","Oz is a higher-order concurrent constraint programming system under development at DFKI. It combines ideas from logic and concurrent programming in a simple yet expressive language. From logic programming Oz inherits logic variables and logic data structures, which provide for a programming style where partial information about the values of variables is imposed concurrently and incrementally. A novel feature of Oz is the support of higher-order programming without sacrificing that denotation and equality of variables are captured by first-order logic. Another new feature of Oz are cells, a concurrent construct providing a minimal form of state fully compatible with logic data structures. These two features allow to express objects as procedures with state, avoiding the problems of stream communication, the conventional communication mechanism employed in concurrent logic programming. Based on cells and higher-order programming, Oz readily supports concurrent object-oriented programming including object identity, late method binding, multiple inheritance, ""self"", ""super"", batches, synchronous and asynchronous communication.","1970-01-01"
769,281075,"Solving Very Large Weakly Coupled Markov Decision Processes","Craig Boutilier; Kee-eung Kim; Leonid Peshkin; Leslie Pack Kaelbling; Milos Hauskrecht; Nicolas Meuleau; Thomas Dean;","We present a technique for computing approximately optimal solutions to stochastic resource allocation problems modeled as Markov decision processes (MDPs). We exploit two key properties to avoid explicitly enumerating the very large state and action spaces associated with these problems. First, the problems are composed of multiple tasks whose utilities are independent. Second, the actions taken with respect to (or resources allocated to) a task do not influence the status of any other task. We can therefore view each task as an MDP. However, these MDPs are weakly coupled by resource constraints: actions selected for one MDP restrict the actions available to others. We describe heuristic techniques for dealing with several classes of constraints that use the solutions for individual MDPs to construct an approximate global solution. We demonstrate this technique on problems involving thousandsof tasks, approximating the solution to problems that are far beyond the reach...","1970-01-01"
770,281128,"A Framework for Defining Logics","Furio Honsell; Gordon Plotkin; Robert Harper;","The Edinburgh Logical Framework (LF) provides a means to define (or present) logics. It is based on a general treatment of syntax, rules, and proofs by means of a typed -calculus with dependent types. Syntax is treated in a style similar to, but more general than, Martin-Löf's system of arities. The treatment of rules and proofs focuses on his notion of a judgement. Logics are represented in LF via a new principle, the judgements as types principle, whereby each judgement is identified with the type of its proofs. This allows for a smooth treatment of discharge and variable occurrence conditions and leads to a uniform treatment of rules and proofs whereby rules are viewed as proofs of higher-order judgements and proof checking is reduced to type checking. The practical benefit of our treatment of formal systems is that logic-indep endent tools such as proof editors and proof checkers can be constructed. Categories and subject descriptors: F.3.1 [Logics and Meanings of Prog...","1999-02-05"
771,281158,"Describing Instruction Set Processors Using nML","A. Fauth; J. Van Praet; M. Freericks;","Programmable processors offer a high degree of flexibility and are therefore increasingly being used in embedded systems. We introduce the formalism nML which is especially suited to describe such processors in terms of their instruction set, an nML description is directly related to the standard description as found in the usual programmer's manuals. The nML formalism is based on a mixed structural and behavioural model facilitating exact yet concise descriptions. The philosophy of nML is already applied in two approaches to retargetable code generation and instruction set simulation. 1 Introduction In consumer electronics and telecommunications high product volumes are increasingly combined with short life-times and high system complexity. The pressure on development times together with the demand to react on late specification changes make mask or field programmability a desired feature. The thereby obtained flexibility not only helps to shorten the design cycle, but also allows fo...","1994-12-08"
772,281159,"Symbolic Protocol Verification with Queue BDDs","David E. Long;",". Symbolic verification based on Binary Decision Diagrams (BDDs) has proven to be a powerful technique for ensuring the correctness of digital hardware. In contrast, BDDs have not caught on as widely for software verification, partly because the data types used in software are more complicated than those used in hardware. In this work, we propose an extension of BDDs for dealing with dynamic data structures. Specifically, we focus on queues, since they are commonly used in modeling communication protocols. We introduce Queue BDDs (QBDDs), which include all the power of BDDs while also providing an efficient representation of queue contents. Experimental results show that QBDDs are well-suited for the verification of communication protocols. Keywords: communication protocols, queues, symbolic verification, BDDs, state explosion, state-space exploration, model checking 1. Introduction Binary Decision Diagrams (BDDs) [5] have proven to be a powerful tool for the verification of digital ...","1999-07-01"
773,282066,"A Unified Scheduling Model for High-Level Synthesis and Code Generation","Augusli Kifli; Gert Goossens; Hugo De Man;","Scheduling is an essential task both in high-level synthesis and in code generation for programmable processors. In this paper we discuss the impact of the controller model on the scheduling task for DSP applications. Existing techniques in high-level synthesis mostly assume a simple controller model in the form of a single FSM. However, in reality more complex controller architectures are often used. On the other hand, in the case of programmable processors, the controller architecture is largely defined by the available control-flow instructions in the instruction set. In this paper, a unified scheduling model is presented to handle a wide range of controller architectures, from the application specific to programmableprocessorsolutions. The impact of chosing a certain controller architecture on the scheduling phase is investigated. Finally, the tasks of controller generation and code assembly are discussed, which will generate the FSM or machine code description from the correct sche...","1994-12-08"
774,283680,"A Discipline of Specification-Based Test Derivation","Michael R. Donat;","System-level requirements-based testing is an important task in software development, providing evidence that each requirement has been satis#ed. There are two major problems with how these tests are derived. First, the notion of coverage is subjective, i.e., there is a lack of objective nitionsions of coverage criteria. Second, there is a surprising lack of automation in deriving system-level requirements-based tests. Research into solutions for these problems has led to the formulation of the discipline of speci#cation-based test derivation presented in this dissertation. This discipline, which is based on predicate logic, provides a scienti#c foundation for objective definitions of coverage criteria and algorithms for partially automating test derivation. This dissertation de#nes some fundamental coverage criteria as examples. A general test frame generation process illustrates a general application of the discipline to a broad range of formal speci#cations, whichcan include existent...","1999-04-24"
775,284612,"Symbolic Model Checking with Rich Assertional Languages","A. Pnueli; E. Shahar; M. Marcus; O. Maler; Y. Kesten;","The paper shows that, by an appropriate choice of a rich assertional language, it is possible to extend the utility of symbolic model checking beyond the realm of bdd- represented finite-state systems into the domain of infinite-state systems, leading to a powerful technique for uniform verification of unbounded (parameterized) process networks. The main contributions of the paper are a formulation of a general framework for symbolic model checking of infinite-state systems, a demonstration that many individual examples of uniformly verified parameterized designs that appear in the literature are special cases of our general approach, verifying the correctness of the Futurebus+ design for all single-bus configurations, and extending the technique to tree architectures. Keywords: Symbolic model checking; Parametric systems; Tree automata; Regular expressions 1 Introduction The problem of uniform verification of parameterized systems is one of the most thoroughly researched problems...","1999-10-24"
776,285106,"Reasoning about Infinite Computations","Moshe Y. Vardi;","We investigate extensions of temporal logic by connectives defined by finite automata on infinite words. We consider three different logics, corresponding to three different types of acceptance conditions (finite, looping and repeating) for the automata. It turns out, however, that these logics all have the same expressive power and that their decision problems are all PSPACE-complete. We also investigate connectives defined by alternating automata and show that they do not increase the expressive power of the logic or the complexity of the decision problem. 1 Introduction For many years, logics of programs have been tools for reasoning about the input/output behavior of programs. When dealing with concurrent or nonterminating processes (like operating systems) there is, however, a need to reason about infinite computations. Thus, instead of considering the first and last states of finite computations, we need to consider the infinite sequences of states that the program goes through...","2000-01-19"
777,286414,"A Constraint-based approach for examination timetabling using local repair techniques","null","We present in this paper an algorithm based upon the Constraint Satisfaction Problem model, used at the "" ' Ecole des Mines de Nantes"" to generate examination timetables. A strong constraint is that the computing time must be less than 1 minute. This led us to develop an incomplete algorithm, using local repair techniques, instead of an exhaustive search method. The program has been validated on fifty ""hand-made"" problems, and has succesfully solved the thirteen ""real"" problems. keywords: Implementation, Constraint Programming, Constraint Satisfaction Problem 1 Introduction Every year, the "" ' Ecole des Mines de Nantes"", a french school of engineering, organizes an entrance examination to select its students. After a first step (written examination), there remain about 1500 candidates. Those candidates are then called for a second step consisting of a sequence of oral examinations. Every day, a number of candidates have to take several oral exams. Section 2 describes the comp...","1997-12-12"
778,287290,"Polynomial Time Algorithms for Testing Probabilistic Bisimulation and Simulation","Christel Baier;",". Various models and equivalence relations or preorders for probabilistic processes are proposed in the literature. This paper deals with a model based on labelled transition systems extended to the probabalistic setting and gives an O(n 2 Delta m) algorithm for testing probabilistic bisimulation and an O(n 5 Delta m 2 ) algorithm for testing probabilistic simulation where n is the number of states and m the number of transitions in the underlying probabilistic transition systems. 1 Introduction Transition systems have proved to be very useful for modelling concurrent processes. A variety of widely accepted equivalence relations and preorders for such systems support the use of transition systems for the design and verification of concurrent systems. In this context, testing equivalences and preorders become important and have been studied e.g. in [3, 4, 8, 11, 17]. For instance, (strong) bisimulation can be decided in time O(m Delta log n) [22], weak bisimulation in t...","1999-10-14"
779,289035,"Model Reduction Techniques for Computing Approximately Optimal Solutions for Markov Decision Processes","Robert Givan; Sonia Leach; Thomas Dean;","We present a method for solving implicit (factored) Markov decision processes (MDPs) with very large state spaces. We introduce a property of state space partitions which we call ffl-homogeneity. Intuitively, an ffl-homogeneous partition groups together states that behave approximately the same under all or some subset of policies. Borrowing from recent work on model minimization in computer-aided software verification, we present an algorithm that takes a factored representation of an MDP and an 0 ffl 1 and computes a factored ffl-homogeneous partition of the state space. This partition defines a family of related MDPs---those MDP's with state space equal to the blocks of the partition, and transition probabilities ""approximately"" like those of any (original MDP) state in the source block. To formally study such families of MDPs, we introduce the new notion of a ""bounded parameter MDP"" (BMDP), which is a family of (traditional) MDPs defined by specifying upper...","1997-08-16"
780,289188,"Efficient Data Structure for Fully Symbolic Verification of Real-Time Software Systems","Farn Wang;","A new data-structure called DDD (Data-Decision Diagram) for the fully symbolic model-checking of realtime software systems is proposed. DDD is a BDD-like data-structure for the encoding of regions [2]. Unlike DBM which records differences between pairs of clock readings, DDD only uses one auxiliary binary variable for each clock. Thus the number of variables used in DDD is always linear to the number of clocks declared in the input system description. Experiment has been carried out to compare DDD with previous technologies. 1 Introduction Fully symbolic verification of real-time systems is desirable with the promise of efficient data-sharing. We propose Data Decision Diagram (DDD) as the new data-structure for such a purpose. DDD is a BDD-like data-structure [5, 8] for the encoding of regions [2]. The ordering among fractional parts of clock readings is explicitly encoded in the variable ordering of DDD. To record sets of clock readings with the same fractional parts, we add one...","1970-01-01"
781,289801,"Mona: Decidable Arithmetic in Practice","Morten Biehl; Nils Klarlund; Theis Rauhe;",". In this note, we describe how a fragment of arithmetic can be decided in practice. We follow essentially the ideas of [8], which we have embedded in the Mona tool. Mona is an implementation of Monadic Second-order Logic on finite strings (and trees). The previous semantics used in Mona is the one provided in current literature [7, 9], where the meaning of first-order terms is restricted to being a position in the string over which the formula is interpreted. In this note, we describe our new semantics, where terms are interpreted relative to all natural numbers. With this semantics Mona becomes a decision procedure for the calculus called WS1S, the Weak Second-order theory of 1 Successor. We also show how the Mona representation of automata subsumes a recent proposal for representing queues. We exploit the natural semantics to carry out automated reasoning about queue operations. In practice, the fundamental concept of regularity (finite-state acceptance of strings) is...","1998-11-20"
782,290620,"A New One-Pass Tableau Calculus for PLTL","Stefan Schwendimann;",". The paper presents a one-pass tableau calculus PLTLT for the propositional linear time logic PLTL. The calculus is correct and complete and unlike in previous decision methods, there is no second phase that checks for the fulfillment of the so-called eventuality formulae. This second phase is performed locally and is incorporated into the rules of the calculus. Derivations in PLTLT are cyclic trees rather than cyclic graphs. When used as a basis for a decision procedure, it has the advantage that only one branch needs to be kept in memory at any one time. It may thus be a suitable starting point for the development of a parallel decision method for PLTL. 1 Introduction Temporal logic has proved to be a useful formalism for reasoning about execution sequences of programs. It can be employed to formulate and verify properties of concurrent programs, protocols and hardware (see for instance [1], [13], [14]). A prominent variant is the propositional linear time logic PLTL wher...","1998-05-12"
783,291026,"Principles And Methods Of Testing Finite State Machines - A Survey","David Lee; Mihalis Yannakakis;","With advanced computer technology, systems are getting larger to fulfill more complicated tasks, however, they are also becoming less reliable. Consequently, testing is an indispensable part of system design and implementation; yet it has proved to be a formidable task for complex systems. This motivates the study of testing finite state machines to ensure the correct functioning of systems and to discover aspects of their behavior. A finite state machine contains a finite number of states and produces outputs on state transitions after receiving inputs. Finite state machines are widely used to model systems in diverse areas, including sequential circuits, certain types of programs, and, more recently, communication protocols. In a testing problem we have a machine about which we lack some information; we would like to deduce this information by providing a sequence of inputs to the machine and observing the outputs produced. Because of its practical importance and theoretical intere...","1999-02-02"
784,291165,"A Relational Algebraic Approach to the Representation and Analysis of Discrete Event Systems","Enke Chen;","The analysis and control of discrete event systems modeled by finite-state machines require possibly complex manipulations of these finite-state machines. Algorithms that involve such manipulations must be described formally, uniformly, concisely, and must be easy to program. This paper shows that the relational algebra from relational database theory can be employed to achieve these goals. We consider representative operations on finite-state machines that arise in the analysis and control of discrete event systems and show how these operations can be formally specified as algebraic expressions in relational algebra and how they can be implemented as SQL queries on a relational database management system. 1. Introduction Finite-state machines are widely used to model the logical behavior of discrete event systems. For example, the operation of an automated manufacturing system, the communication protocol between nodes in a computer network, or the execution of concurrent ...","1991-09-10"
785,291177,"From Fairness to Chance","Luca De Alfaro;","Fairness is a mathematical abstraction used in the modeling of a wide range of phenomena, including concurrency, scheduling, and probability. In this paper, we study fairness in the context of probabilistic systems, and we introduce probabilistic fairness, a novel notion of fairness that is itself defined in terms of probability. The definition of probabilistic fairness makes it invariant with respect to synchronous composition, and facilitates the design of model-checking algorithms for quantitative properties of probabilistic systems. We compare probabilistic fairness with other notions of fairness for probabilistic systems, and we provide algorithms that solve the verification problem for various classes of probabilistic properties on finite-state systems with fairness. 1 Introduction The use of formal methods for the analysis and verification of systems requires a mathematical model of the system being studied. Many system models include nondeterminism, which enables the represen...","2000-02-23"
786,292009,"Planning with Incomplete Information as Heuristic Search in Belief Space","Blai Bonet;","The formulation of planning as heuristic search with heuristics derived from problem representations has turned out to be a fruitful approach for classical planning. In this paper, we pursue a similar idea in the context planning with incomplete information. Planning with incomplete information can be formulated as a problem of search in belief space, where belief states can be either sets of states or more generally probability distribution over states. While the formulation (as the formulation of classical planning as heuristic search) is not particularly novel, the contribution of this paper is to make it explicit, to test it over a number of domains, and to extend it to tasks like planning with sensing where the standard search algorithms do not apply. The resulting planner appears to be competitive with the most recent conformant and contingent planners (e.g., cgp, sgp, and cmbp) while at the same time is more general as it can handle probabilistic actions and se...","1970-01-01"
787,292288,"Compositional Minimisation of Finite State Systems Using Interface Specifications","Bernhard Steffen; Gerald Luttgen; Susanne Graf;","In this paper we present a method for the compositional construction of the minimal transition system that represents the semantics of a given distributed system. Our aim is to control the state explosion caused by the interleavings of actions of communicating parallel components by reduction steps that exploit global communication constraints given in terms of interface specifications. The effect of the method, which is developed for bisimulation semantics here, depends on the structure of the distributed system under consideration, and the accuracy of the interface specifications. However, its correctness does not: every ""successful"" construction is guaranteed to yield the desired minimal transition system, independent of the correctness of the interface specifications provided by the program designer. Keywords bisimulation, distributed system, interface specification, minimization, reduction operator, specification preorder, state explosion problem, transition system, verification...","1997-03-07"
788,292767,"Combining Computer Algebra and Rule Based Reasoning","Reinhard Bundgen;",": We present extended term rewriting systems as a means to describe a simplification relation for an equational specification with a built-in domain of external objects. Even if the extended term rewriting system is canonical, the combined relation including built-in computations of `ground terms' needs neither be terminating nor confluent. We investigate restrictions on the extended term rewriting systems and the built-in domains under which these properties hold. A very important property of extended term rewriting systems is decomposition freedom. Among others decomposition free extended term rewriting systems allow for efficient simplifications. Some interesting algebraic applications of canonical simplification relations are presented. 1 Introduction There has always been mutual interest in the areas of computer algebra and term rewriting systems as can be seen for example from the calls of papers of the main conferences in the two areas (ISSAC and RTA resp.) which each...","1995-03-09"
789,293435,"Formal Methods for the Specification and Design of Real-Time Safety Critical Systems","Jonathan S. Ostro;","Safety critical computers increasingly a#ect nearly every aspect of our lives. Computers control the planes we #y on, monitor our health in hospitals and do our work in hazardous environments. Computers with software de#ciencies that fail to meet stringent timing constraints have resulted in catastrophic failures. This paper surveys formal methods for specifying, designing and verifying real-time systems, so as to improve their safety and reliability. # To appear in Journal of Systems and Software,Vol. 18, Number 1, pages 33#60, April 1992. Jonathan Ostro# is with the Department of Computer Science, York University 4700 Keele Street, North York, Ontario, Canada, M3J 1P3. This work is supported by the Natural Sciences and Engineering Research Council of Canada. 1 CONTENTS 2 Contents 1 Introduction 3 2 De#ning the terms 6 2.1 Major issues that formal theories must address ::::::: 13 3 Real-Time Programming Languages 14 4 Structured Methods and#or Graphical Languages 15 4.1 Str...","2000-03-30"
790,293625,"Model Checking in CLP","Andreas Podelski; Giorgio Delzanno;","We show that Constraint Logic Programming (CLP) can serve as a conceptual basis and as a practical implementation platform for the model checking of infinite-state systems. Our contributions are: (1) a semantics-preserving translation of concurrent systems into CLP programs, (2) a method for verifying safety and liveness properties on the CLP programs produced by the translation. We have implemented the method in a CLP system and verified well-known examples of infinitestate programs over integers, using here linear constraints as opposed to Presburger arithmetic as in previous solutions.","2000-01-20"
791,293938,"A Markov Chain Model Checker","Holger Hermanns; Joachim Meyer-kayser; Joost-pieter Katoen; Markus Siegle;",". Markov chains are widely used in the context of performance and reliability evaluation of systems of various nature. Model checking of such chains with respect to a given (branching) temporal logic formula has been proposed for both the discrete [17, 6] and the continuous time setting [4, 8]. In this paper, we describe a prototype model checker for discrete and continuous-time Markov chains, the Erlangen--Twente Markov Chain Checker (E MC 2 ), where properties are expressed in appropriate extensions of CTL. We illustrate the general benefits of this approach and discuss the structure of the tool. Furthermore we report on first successful applications of the tool to non-trivial examples, highlighting lessons learned during development and application of E T MC 2 . 1 Introduction Markov chains are widely used as simple yet adequate models in diverse areas, ranging from mathematics and computer science to other disciplines such as operations research, industrial engine...","2000-04-06"
792,294409,"Machine Learning For Object Recognition and Scene Analysis","S. Moscatelli; Y. Kodratoff;",". Learning is a critical research field for autonomous computer vision systems. It can bring solutions to the knowledge acquisition bottleneck of image understanding systems. Recent developments of machine learning for computer vision are reported in this paper. We describe several different approaches for learning at different levels of the image understanding process, including learning 2-D shape models, learning strategic knowledge for optimizing model matching, learning for adaptative target recognition systems, knowledge acquisition of constraint rules for labelling and automatic parameter optimization for vision systems. Each approach will be commented and its strong and weak points will be underlined. In conclusion we will suggest what could be the ""ideal"" learning system for vision. 1. Introduction An Image Understanding System aims at providing a semantic description of images by interpreting low level information with the help of background knowledge about images a...","2000-02-22"
793,295574,"VeriSoft: A Tool for the Automatic Analysis of Concurrent Reactive Software","Patrice Godefroid;",". VeriSoft is a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary code written in full-fledged programming languages such as C or C++. It can automatically detect coordination problems between concurrent processes. Specifically, VeriSoft searches the state space of the system for deadlocks, livelocks, divergences, and violations of user-specified assertions. An interactive graphical simulator/debugger is also available for following the execution of all the processes of the concurrent system. 1 Introduction State-space exploration techniques are increasingly being used for analyzing the correctness of concurrent reactive systems. These techniques consist of exploring a directed graph, called the state space, representing the combined behavior of all concurrent components in a system. In the case of software systems, existing state-space exploration tools can compute automatically a state space from an abs...","1999-07-01"
794,295968,"Symbolic Verification of Communication Protocols with Infinite State Spaces using QDDs","Bernard Boigelot;",". We study the verification of properties of communication protocols modeled by a finite set of finite-state machines that communicate by exchanging messages via unbounded FIFO queues. It is well-known that most interesting verification problems, such as deadlock detection, are undecidable for this class of systems. However, in practice, these verification problems may very well turn out to be decidable for a subclass containing most ""real"" protocols. Motivated by this optimistic (and, we claim, realistic) observation, we present an algorithm that may construct a finite and exact representation of the state space of a communication protocol, even if this state space is infinite. Our algorithm performs a loop-first search in the state space of the protocol being analyzed. A loop-first search is a search technique that attempts to explore first the results of successive executions of loops in the protocol description (code). A new data structure named Queue-content Decision Diagram (QDD)...","1999-07-01"
795,296098,"Minimal State Graph Generation","A. Bouajjani; C. Ratel; J-c. Fern; N. Halbwachs; P. Raymond;","We address the problem of generating a minimal state graph from a program, without building the whole state graph. Minimality is considered here with respect to bisimulation. A generation algorithm is derived and illustrated. Applications concern program verification and control synthesis in reactive program compilation. 1 Introduction This paper concerns the problem of explicitly building a state graph from a program, a formula or any implicit expression of a transition system. Such state graphs are used in program verification (""model checking"" [7, 14], behavioral equivalence [12]) and compiling (control structure synthesis [1, 6]). A crucial problem with state graph generation is the size of the graph, which can be prohibitive. This size can be large not only because of the intrinsic complexity of the program, but also because the graph contains a lot of states which are in some sense equivalent. Of course, the proliferation of equivalent states increases the number of states, but ...","1999-12-15"
796,296226,"Comparing Connectionist and Symbolic Learning Methods","J. R. Quinlan;",": Experimental comparison of back-propagation and decision tree methods have provided many data points but less understanding of why one method works better for some tasks than for others. This paper observes that, just as there are sequential and parallel classification methods, there are certain classification tasks that lend themselves to methods of one or the other type. Introduction Numerous papers that have appeared over the last few years compare the performance of a variety of learning algorithms on real and constructed datasets. Such comparisons, uncovering the strengths and weaknesses of algorithms on different tasks, provide valuable data points that help to map and understand the inherent capabilities of the methods. One emerging theme is that these capabilities appear to be taskdependent -- few researchers would claim that one method is uniformly superior to another. This paper focuses on two kinds of learning algorithms: symbolic methods, that represent what is le...","1998-07-12"
797,296425,"A Growth Algorithm for Neural Network Decision Trees","Mario March; Mostefa Golea;","This paper explores the application of neural network principles to the construction of decision trees from examples. We consider the problem of constructing a tree of perceptrons able to execute a given but arbitrary Boolean function defined on N i input bits. We apply a sequential (from one tree level to the next) and parallel (for neurons in the same level) learning procedure to add hidden units until the task in hand is performed. At each step, we use a perceptron-type algorithm over a suitable defined input space to minimise a classification error. The internal representations obtained in this way are linearly separable. Preliminary results of this algorithm are presented. 1 Introduction Feed-forward layered neural networks[1, 2] are in principle able to learn any arbitrary mapping provided that enough hidden units are present[3]. A way to improve the performance of a neural network is to match its topology to a specific task as closely as possible. However, the determi...","1999-10-15"
798,296508,"Using Lower Bounds during Dynamic BDD Minimization","Rolf Drechsler; Wolfgang Gunther;","Ordered Binary Decision Diagrams (BDDs) are a data structure for representation and manipulation of Boolean functions often applied in VLSI CAD. The choice of the variable ordering largely influences the size of the BDD; its size may vary from linear to exponential. The most successful methods for finding good orderings are based on dynamic variable reordering, i.e. exchanging of neighboring variables. This basic operation has been used in various variants, like sifting and window permutation. In this paper we show that lower bounds computed during the minimization process can speed up the computation significantly. First, lower bounds are studied from a theoretical point of view. Then these techniques are incorporated in dynamic minimization algorithms. By the computation of good lower bounds large parts of the search space can be pruned resulting in very fast computations. Experimental results are given to demonstrate the efficiency of our approach. 1 Introduction Decision Diagram...","1999-08-10"
799,296670,"CUDD: CU Decision Diagram Package Release 2.2.0","Fabio Somenzi;","The CUDD package provides functions to manipulate Binary Decision Diagrams (BDDs) [5,3], Algebraic Decision Diagrams (ADDs) [1], and Zero suppressed Decision Diagrams (ZDDs) [12]. BDDs are used to represent switch functions","1998-05-13"
800,297317,"Constructing Conditional Plans by a Theorem-Prover","Jussi Rintanen;","The research on conditional planning rejects the assumptions that there is no uncertainty or incompleteness of knowledge with respect to the state and changes of the system the plans operate on. Without these assumptions the sequences of operations that achieve the goals depend on the initial state and the outcomes of nondeterministic changes in the system. This setting raises the questions of how to represent the plans and how to perform plan search. The answers are quite different from those in the simpler classical framework. In this paper, we approach conditional planning from a new viewpoint that is motivated by the use of satisfiability algorithms in classical planning. Translating conditional planning to formulae in the propositional logic is not feasible because of inherent computational limitations. Instead, we translate conditional planning to quantified Boolean formulae. We discuss three formalizations of conditional planning as quantified Boolean formulae, and pr...","1999-09-22"
801,297323,"The Concurrency Factory: A Development Environment for Concurrent Systems","Oleg Sokolsky; Philip M. Lewis; Rance Cleavel; Scott A. Smolka;",". The Concurrency Factory supports the specification, simulation, verification, and implementation of real-time concurrent systems such as communication protocols and process control systems. While the system uses process algebra as its underlying design formalism, the primary focus of the project is practical utility: the tools should be usable by engineers who are not familiar with formal models of concurrency, and it should be capable of handling large-scale systems such as those found in the telecommunications industry. This paper serves as a status report for the Factory project and briefly describes a case-study involving the GNU UUCP i-protocol. 1 Introduction The Concurrency Factory is an integrated toolset for specification, simulation, verification, and implementation of real-time concurrent systems such as communication protocols and process control systems. Two themes underpin the work done on the project: the use of process algebra [Mil89, BK84, Hoa85] as a form...","1999-03-26"
802,297641,"Automatic Verification of Real-time Systems with Discrete Probability Distributions","And Jeremy Sproston; Marta Kwiatkowska; Roberto Segala;","We consider the timed automata model of [3], which allows the analysis of realtime systems expressed in terms of quantitative timing constraints. Traditional approaches to real-time system description express the model purely in terms of nondeterminism; however, it is often desirable to express the likelihood of the system making certain transitions. In this paper, we present a model for real-time systems augmented with discrete probability distributions. Furthermore, two approaches to model checking are introduced for this model. The first uses the algorithm of [6] to provide a verification technique for our model against temporal logic properties which can refer both to timing properties and probabilities. The second, generally more efficient, technique concerns the verification of probabilistic, real-time reachability properties. 1 Introduction The proliferation of digital technology embedded into real-life environments has led to increased interest in computer systems expressed i...","1970-01-01"
803,298007,"Argonaute: Graphical Description, Semantics and Verification of Reactive Systems by Using a Process Algebra","Florence Maraninchi;","The Argonaute system is specifically designed to describe, specify and verify reactive systems such as communication protocols, real-time applications, man-machine interfaces, . . . It is based upon the Argos graphical language, whose syntax relies on the Higraphs formalism by D. Harel [HAR88], and whose semantics is given by using a process algebra. Automata form the basic notion of the language, and hierarchical or parallel decompositions are given by using operators of the algebra. The complete formalization of the language inherits notions from both classical process algebras such as ccs [MIL80], and existing programming languages used in the same field such as Esterel [BG88] or the Statecharts formalism [HAR87]. Concerning complex system description, Argos allows to describe intrinsic states directly --- with the basic automaton notion --- and only them: connections between components need no extra-state. The Argonaute system allows to describe reactive systems graphically, to spe...","1992-09-04"
804,298240,"Expressing Constraints for Data Display Specification: A Visual Approach","Isabel F. Cruz;","In this paper we introduce a constraint-based language that has a visual syntax, and allows for the declarative specification of the display of data. Other features of the proposed language include: (1) simplicity and genericity of the basic constructs; (2) ability to specify a variety of displays (graphs, bar charts, pie charts, etc.); (3) compatibility with the object-oriented framework of the database language doodle. We provide the syntax and the semantics of the language, and examples of applications that demonstrate the expressiveness of our language. 1 Introduction Mappings between the data domain and the visual domain have been used for extracting information from the data, by reasoning in the visual domain [Gre83, CNI93]. For example, Venn diagrams are visual representations of abstract sets and of their inclusion relationships. Other diagrams are close to the concrete entities that they represent, such as transportation and communication networks. Also, bar charts, pie...","1994-02-17"
805,299020,"A Mathematically Precise Two-Level Formal Hardware Verification Methodology","Carl-johan H. Seger; Jeffrey J. Joyce;","Theorem-proving and symbolic trajectory evaluation are both described as methods for the formal verification of hardware. They are both used to achieve a common goal---correctly designed hardware---and both are intended to be an alternative to conventional methods based on non-exhaustive simulation. However, they have different strengths and weaknesses. The main significance of this paper is the description of a two-level approach to formal hardware verification, where the HOL theorem prover is combined with the Voss verification system. From symbolic trajectory evaluation we inherit a high degree of automation and accurate models of circuit behavior and timing. From interactive theorem-proving we gain access to powerful mathematical tools such as induction and abstraction. The interface between the HOL and Voss is, however, more than just an ad hoc translation of verification results obtained by one tool into input for the other tool. We have developed a ""mathematical"" inte...","1994-02-15"
806,300119,"Satisfiability Testing with More Reasoning and Less Guessing","Allen Van; Gelder Yumi; K. Tsuji;","A new algorithm for testing satisfiability of propositional formulas in conjunctive normal form (CNF) is described. It applies reasoning in the form of certain resolution operations, and identification of equivalent literals. Resolution produces growth in the size of the formula, but within a global quadratic bound; most previous methods avoid operations that produce any growth, and generally do not identify equivalent literals. Computational experience indicates that the method does substantially less ""guessing"" than previously reported algorithms, while keeping a polynomial time bound on the work done between guesses. Experiments indicate that, for larger problems, the time investment in reasoning returns a profit in reduced searching, and the profit increases with increasing problem size. Experimental data compares six branching strategies of the proposed algorithm on a variety of problems, including several Dimacs benchmarks. These branching strategies were shown to perfor...","1995-11-08"
807,301565,"Modelling and Analysis of a Commercial Field Bus Protocol","Alexandre David; Wang Yi;","We report on an industrial application of UPPAAL, in which a commercial field bus protocol (AF100) is modelled and analysed using the tool. During the case study, a number of imperfections in the protocol logic and its implementation are found and the error sources are debugged based on abstract models of the protocol; respective improvements have been suggested. In this paper, we shall summarize our experiences in dealing with the complexity of the protocol using various modelling and abstraction features provided in UPPAAL. As an example, we study the bus coupler of AF100, which serves as the data link layer of the protocol. 1. Introduction In recent years, a number of modelling and verification tools for real-time systems [HHWT95, BLL + 95, BLL + 96, DOTY95] have been developed based on the theory of timed automata [AD94]. They have been successfully applied in various case-studies (e.g. [BGK + 96, JLS96, SMF97]). However, the tools have been mainly used in the academic commu...","2000-05-22"
808,301948,"On the Implementation of MIPS","Malte Helmert; Stefan Edelkamp;","Planning is a central topic of AI and provides solutions to problems given in a problem independent formalism. Recent successes in the exploration of model checking and single-agent search problems have led to a generalization of the symbolic exploration method with binary decision diagrams (BDDs). In this paper we present the use, architecture, implementation and performance of our STRIPS planner MIPS abbreviating intelligent model checking and planning system. With BDD refinements, symbolic and single state heuristic search engines we highlight recent improvements that have been added to the system. Introduction In classical planning an object is a unique name for an entity in the modeled domain and predicates are unique names for modeling attributes of objects. Instantiated predicates are facts. A state in the planning problem is a set of facts. An operator is a 3-tuple (P; A; D) of preconditions, add-effects and del-effects, with P , A and D being sets of facts. An op...","2000-03-14"
809,302003,"Admissible Heuristics for Optimal Planning","Patrik Haslum;","hsp and hspr are two recent planners that search the state-space using an heuristic function extracted from Strips encodings. hsp does a forward search from the initial state recomputing the heuristic in every state, while hspr does a regression search from the goal computing a suitable representation of the heuristic only once. Both planners have shown good performance, often producing solutions that are competitive in time and number of actions with the solutions found by Graphplan and sat planners. hsp and hspr, however, are not optimal planners. This is because the heuristic function is not admissible and the search algorithms are not optimal. In this paper we address this problem. We formulate a new admissible heuristic for planning, use it to guide an ida search, and empirically evaluate the resulting optimal planner over a number of domains. The main contribution is the idea underlying the heuristic that yields not one but a whole family of polynomial and adm...","2000-02-10"
810,302347,"Context Dependent Minimization of State/Event Systems","Gerd Behrmann; Kare Kristoffersen; Kim Larsen;","This paper presents a technique for efficiently checking reachability properties of concurrent state/event systems. The technique improves the traditional algorithm for forwards exploration of the global state space by incremental construction of subsystems which are kept small using a context dependent minimization. A tool has been implemented with the specific purpose of verifying state/event systems. Experimental results reports on a feasible automatic verification of correctness of Milner's Scheduler -- an often used benchmark -- with 100 cells. This result dramatically improves the previous best results for this benchmark. Also our technique has been shown to perform well on industrial designs of sizes up to 400 concurrent state machines. Keywords: State/event systems; Reachability analysis; Compositionality; Subsystems; Context Dependent Minimization. 1 Introduction Model checking of finite--state concurrent systems suffers from the potential combinatorial explosion o...","2000-03-27"
811,302704,"The Stable Model Semantics For Logic Programming","Michael Gelfond; Vladimir Lifschitz;","We propose a new declarative semantics for logic programs with negation. Its formulation is quite simple; at the same time, it is more general than the iterated fixed point semantics for stratified programs, and is applicable to some useful programs that are not stratified.","2000-06-20"
812,303368,"Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization","Michael Peter Perrone; N Cooper; Peter Perrone; Prof Charles Elbaum; Prof Leon; Prof Nathan Intrator; This Michael;"," Perrone, M. P. and N. Intrator (1992) Unsupervised Splitting Rules for Neural Tree Classifiers. Proceedings of the International Joint Conference on Neural Networks pp. III:820-825. Perrone, M. P., (1991) A Novel Recursive Partitioning Criterion. Proceedings of the International Joint Conference on Neural Networks p. II:989. iii Acknowledgements I am indebted to my advisor, Professor Leon N Cooper, for sharing his insights, intuitive ability for cutting straight to the heart of things, and his earnest curiousity for all things. Leon, you have made it all worthwhile. I would also like to express my deep appreciation to Dr. Nathan Intrator who was the source of uncounted stimulating discussions including the one which lead to this dissertation. Nathan, thanks for your focus and encouragement! The preprocessed NIST OCR data used in this thesis was provided by Nestor Inc., the Sunspot data was supplied by Andreas Weigend, and the prep","1994-01-31"
813,303810,"Fighting Livelock in the i-Protocol: A Comparative Study of Verification Tools","David S. Warren; Eugene W. Stark; I. V. Ramakrishnan; Oleg Sokolsky; Scott A. Smolka; Xiaoqun Du; Y. S. Ramakrishna; Yifei Dong;",". The i-protocol, an optimized sliding-window protocol for GNU UUCP, came to our attention two years ago when we used the Concurrency Factory's local model checker to detect, locate, and correct a non-trivial livelockinversion 1.04 of the protocol. Since then, wehave repeated this veri#cation e#ort with #ve widely used model checkers, namely, COSPAN, Mur', SMV, Spin, and XMC. It is our contention that the i-protocol makes for a particularly compelling case study in protocol veri#cation and for a formidable benchmark of veri#cation-tool performance, for the following reasons: 1# The i-protocol can be used to gauge a tool's ability to detect and diagnose livelock errors. 2# The size of the i-protocol's state space grows exponentially in the window size, and the entirety of this state space must be searched to verify that the protocol, with the livelock error eliminated, is deadlock- or livelock-free. 3# The i-protocol is an asynchronous, low-level software system equipped w...","1999-06-10"
814,304747,"Feasibility of Model Checking Software Requirements: A Case Study","Joanne M. Atlee; Tirumale Sreemani;",": Model checking is an effective technique for verifying properties of a finite specification. A model checker accepts a specification and a property, and it searches the reachable states to determine if the property is a theorem of the specification. Because model checking examines every state of the specification, it is a more thorough validation technique than testing executable specifications. However, some researchers question the feasibility of model checking, because the size of a specification 's state space grows exponentially with respect to the number of variables in the specification. This paper demonstrates the feasibility of symbolically model checking a non-trivial specification: the software requirements of the A-7E aircraft. The A-7E requirements document lists five properties that the designers manually derived from the requirements. Using McMillan's Symbolic Model Verifier, we were able to verify or find a counterexample of each property in less than 10-15 C...","1996-02-23"
815,306286,"A Robust and Fast Action Selection Mechanism for Planning","Blai Bonet;","The ability to plan and react in dynamic environments is central to intelligent behavior yet few algorithms have managed to combine fast planning with a robust execution. In this paper we develop one such algorithm by looking at planning as real time search. For that we develop a variation of Korf's Learning Real Time A algorithm together with a suitable heuristic function. The resulting algorithm interleaves lookahead with execution and never builds a plan. It is an action selection mechanism that decides at each time point what to do next. Yet it solves hard planning problems faster than any domain independent planning algorithm known to us, including the powerful SAT planner recently introduced by Kautz and Selman. It also works in the presence of perturbations and noise, and can be given a fixed time window to operate. We illustrate each of these features by running the algorithm on a number of benchmark problems. 1 Introduction The ability to plan and react ...","1970-01-01"
816,306645,"Automatic Verification of Parameterized Linear Networks of Processes","David Lesens; Nicolas Halbwachs; Pascal Raymond;","This paper describes a method to verify safety properties of parameterized linear networks of processes. The method is based on the construction of a network invariant, defined as a fixpoint. Such invariants can often be automatically computed using heuristics based on Cousot's widening techniques. These techniques have been implemented and some non-trivial examples are presented. 1 Introduction After the very first success of automatic verification of finite-state systems [QS82, CES86], Apt and Kozen [AK86] have shown that these techniques cannot be applied to regular networks of processes of unknown size. More formally, let fP 1 ; : : : ; P k g be a finite multi-set of processes, and fTheta 1 ; : : : ; Theta k g be a multi-set of binary composition operators over processes, and consider the family F of processes generated by: 8i = 1 : : : k; (P i 2 F) and (P 2 F =) P Theta i P i 2 F) The problem of verifying that every P 2 F satisfies a property ' is undecidable [AK86], even ...","1999-12-15"
817,309293,"High Performance Fortran Language Specification","null","processors as a user-declared Cartesian mesh Physical processors ALIGN (static) or REALIGN (dynamic) DISTRIBUTE (static) or REDISTRIBUTE (dynamic) Optional implementationdependent directive The underlying assumptions are that an operation on two or more data objects is likely to be carried out much faster if they all reside in the same processor, and that it may be possible to carry out many such operations concurrently if they can be performed on different processors. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 22 SECTION 3. DATA ALIGNMENT AND DISTRIBUTION DIRECTIVES Fortran 90 provides a number of features, notably array syntax, that make it easy for a compiler to determine that many operations may be carried out concurrently. The HPF directives provide a way to inform the compiler of the recommendation that certain data objects should reside in the same processor: if...","1996-03-31"
818,309547,"A Platform for Combining Deductive with Algorithmic Verification","Amir Pnueli; Elad Shahar;",". We describe a computer-aided verification system which combines deductive with algorithmic (model-checking) verification methods. The system, called tlv (for temporal verification system), is constructed as an additional layer superimposed on top of the cmu smv system, and can verify finite-state systems relative to linear temporal logic (ltl) as well as ctl specifications. The systems to be verified can be either hardware circuits written in the smv design language or finite-state reactive programs written in a simple programming language (spl). The paper presents a common computational model which can support these two types of applications and a high-level interactive language tlv-Basic, in which temporal verification rules, proofs, and complex assertions can be written. We illustrate the efficiency and generality gained by combining deductive with algorithmic techniques on several examples, culminating in verification of fragments of the Futurebus+ system. In the ana...","2000-02-17"
819,309666,"Extracting Effective and Admissible State Space Heuristics from the Planning Graph","Subbarao Kambhampati;","Graphplan and heuristic state space planners such as HSP-R and UNPOP are currently two of the most effective approaches for solving classical planning problems. These approaches have hither-to been seen as largely orthogonal. In this paper, we show that the planning graph structure that Graphplan builds in polynomial time, provides a rich substrate for deriving more effective heuristics for state space planners. Specifically, we show that the heuristics used by planners such as HSP-R and UNPOP do badly in several domains due to their failure to consider the interactions between subgoals, and that the mutex information in the planning graph captures exactly this interaction information. We develop several families of heuristics, some aimed at search speed and others at optimality of solutions. Our empirical studies show that our heuristics significantly out-perform the existing state space heuristics. 1 Introduction The last few years have seen a number of attractive a...","2000-04-04"
820,309992,"Unknown Attribute Values In Induction","J. R. Quinlan;","Simple techniques for the development and use of decision tree classifiers assume that all attribute values of all cases are available. Numerous approaches have been proposed with the aim of extending these techniques to cover real-world situations in which unknown attribute values are not uncommon. This paper compares the effectiveness of several approaches as measured by their performance on a collection of datasets. INTRODUCTION The `standard' technique for constructing a decision tree classifier from a training set of cases with known classes, each described in terms of fixed attributes, can be summarised as follows: ffl If all training cases belong to a single class, the tree is a leaf labelled with that class. ffl Otherwise, -- select a test, based on one attribute, with mutually exclusive outcomes; -- divide the training set into subsets, each corresponding to one outcome; and -- apply the same procedure to each subset Once constructed, such a decision tree can be used t...","1998-07-12"
821,310383,"Operational and Compositional Semantics of Synchronous Automaton Compositions","Florence Maraninchi;",": The state/transition paradigm has been used extensively for the description of event-driven, parallel systems. However, the lack for hierarchic structure in such descriptions usually prevents us from using this paradigm in a real programming language. We propose the Argos language for reactive systems. The basic components of a program are input/output-labeled transition systems verifying reactivity (a property similar to input-enabling in IOautomata) . The composition operations (parallel composition and refinement, providing hierarchy) are based upon the synchronous broadcast mechanism of Esterel. We define the language formally in an algebraic framework, and give an operational semantics. The main result is the compositionality of the semantics; we prove that the bisimulation of models induces an equivalence which is a congruence for the operators we propose. An interesting point is the way we introduce hierarchy in a compositional way. 1 1 Introduction The problem ...","1997-01-22"
822,311344,"Model Checking Without a Model: An Analysis of the Heart-Beat Monitor of a Telephone Switch using VeriSoft","Lalita Jategaonkar Jagadeesan; Patrice Godefroid; Robert S. Hanmer;","ing with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. Model Checking Without a Model: An Analysis of the Heart-Beat Monitor of a Telephone Switch using VeriSoft Patrice Godefroid Bell Laboratories Lucent Technologies 1000 E. Warrenville Road Naperville, IL 60566, USA god@bell-labs.com Robert S. Hanmer Lucent Technologies 2000 N. Naperville Road Naperville, IL 60566, USA hanmer@lucent.com Lalita Jategaonkar Jagadeesan Bell Laboratories Lucent Technologies 1000 E. Warrenville Road Naperville, IL 60566, USA lalita@bell-labs.com Abstract VeriSoft is a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary code written in full-fledged programming languages such as C or C++. The state space of a c...","1999-07-01"
823,311445,"Static Inference of Modes and Data Dependencies in Logic Programs","Saumya K. Debray;",": Mode and data dependency analyses find many applications in the generation of efficient executable code for logic programs. For example, mode information can be used to generate specialized unification instructions where permissible; to detect determinacy and functionality of programs; to generate index structures more intelligently; to reduce the amount of runtime tests in systems that support goal suspension; and in the integration of logic and functional languages. Data dependency information can be used for various source-level optimizing transformations, to improve backtracking behavior, and to parallelize logic programs. This paper describes and proves correct an algorithm for the static inference of modes and data dependencies in a program. The algorithm is shown to be quite efficient for programs commonly encountered in practice. Categories and Subject Descriptors: D.3 [Software]: Programming Languages; D.3.4 [Programming Languages]: Processors - compilers, optimization; I.2...","1999-11-16"
824,311541,"Optimal Ordered Binary Decision Diagrams for Tree-like Circuits","Ingo Wegener; Martin Sauerhoff; Ralph Werchner;","Many Boolean functions have short representations by OBDDs (ordered binary decision diagrams) if appropriate variable orderings are used. For tree-like circuits, which may contain EXOR-gates, it is proved that some depth first traversal leads to an optimal variable ordering. Moreover, an optimal variable ordering and the resulting OBDD can be computed in time linear in the number of variables and the size of the OBDD, respectively. Upper and lower bounds on the OBDD size of the functions representable by tree-like circuits are derived. For, e. g., 1024 inputs, we show that all tree-like circuits have OBDDs of size at 5349 and we give an example of a tree-like circuit requiring an OBDD of size 5152. Index Terms Ordered binary decision diagram, efficient algorithms, Boolean function, variable ordering, tree-like circuit. Email Addresses sauerhof/wegener@ls2.informatik.uni-dortmund.de werchner@mi.informatik.uni-frankfurt.de Acknowledgements The first and second author have been suppo...","1996-05-10"
825,311623,"Explaining Subsumption in Description Logics","Alexander T. Borgida; Deborah L. Mcguinness;","This paper explores the explanation of subsumption reasoning in Description Logics that are implemented using normalization methods, focusing on the perspective of knowledge engineers. The notion of explanation is specified using a proof-theoretic framework for presenting the inferences supported in these systems. The problem of overly long explanations is addressed by decomposing them into smaller, independent steps, using the notions of ""atomic description"" and ""atomic justification"". Implementation aspects are explored by considering the design space and some desiderata for explanation modules. This approach has been implemented for the classic knowledge representation system. 1 Introduction Knowledge-based systems, like other software systems, need to be debugged while being developed. In addition, systems providing ""expert advice"" need to be able to justify their conclusions. Traditionally, developers have been supported during debugging by tools which o#er a trac...","1999-07-20"
826,311747,"Trailblazing: A Hierarchical Approach to Percolation Scheduling","Ru Nicolau; Steven Novack;",": Percolation Scheduling (PS) is a system for performing parallelizing transformations for the VLIW and superscalar computation models. PS has various useful properties, such as completeness with respect to local transformations, and appears to be an effective means of exploiting instruction level parallelism. However, compilers based on PS typically suffer from inefficiencies caused by the incremental application of PS transformations and significant code explosion. In this paper we present a non-incremental extension of PS that provides asymptotic efficiency improvements over normal PS. This approach dramatically reduces compilation time and achieves better parallelization by performing non-local transformations not feasible in PS. Simulation results comparing normal PS with our new technique are presented. The approach presented is adaptable to other global instruction level parallelization systems. 1 Introduction Percolation Scheduling (PS)[14] was shown in to have various desirabl...","1999-04-28"
827,311874,"Graph-Based Algorithms for Boolean Function Manipulation","Randal E. Bryant;","In this paper we present a new data structure for representing Boolean functions and an associated set of manipulation algorithms. Functions are represented by directed, acyclic graphs in a manner similar to the representations introduced by Lee [1] and Akers [2], but with further restrictions on the ordering of decision variables in the graph. Although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical applications have a more reasonable representation. Our algorithms have time complexity proportional to the sizes of the graphs being operated on, and hence are quite efficient as long as the graphs do not grow too large. We present experimental results from applying these algorithms to problems in logic design verification that demonstrate the practicality of our approach. Index Terms: Boolean functions, symbolic manipulation, binary decision diagrams, logic design verification 1. Introduction ...","2000-06-12"
828,311984,"History-Based Dynamic Minimization During BDD Construction","Rolf Drechsler; Wolfgang G Unther;","Binary Decision Diagrams (BDDs) are the state-of-the-art data structure in VLSI CAD. Since their size largely depends on the chosen variable ordering, dynamic variable reordering methods, like sifting, often have to be applied while the BDD for a given circuit is constructed. Usually sifting is called each time a given node limit is reached and it is therefore called frequently during the construction of large BDDs. Often most of the runtime is spent for sifting while the BDD is built. In this paper we propose an approach to reduce runtime (and space requirement) during BDD construction by using history-based decision procedures. Dependent on the history of the construction process different types of sifting are called. We propose two methods that consider the quality of the hash table and the size reduction of previous sifting runs, respectively. Experimental results show that both approaches reduce the runtime significantly, i.e. by more than 40% on average. Keywords: BDDs, Verific...","2000-04-05"
829,312156,"Modelling a Real-Time Language","Thomas Hune;","We present a compositional method for translating real-time programs into networks of timed automata. Programs are written in an assembly like real-time language and translated into models supported by the tool Uppaal. We have implemented the translation and give an example of its application on a simple control program for a car. Some properties of the behaviour of the control program are verified using the derived model.","1999-06-03"
830,313021,"Model Checking for Programming Languages using VeriSoft","Patrice Godefroid;","ing with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. Model Checking for Programming Languages using VeriSoft Patrice Godefroid Bell Laboratories Lucent Technologies 1000 E. Warrenville Road Naperville, IL 60566, U.S.A. god@bell-labs.com http://www.bell-labs.com/people/god Abstract Verification by state-space exploration, also often referred to as ""model checking"", is an effective method for analyzing the correctness of concurrent reactive systems (e.g., communication protocols). Unfortunately, existing model-checking techniques are restricted to the verification of properties of models, i.e., abstractions, of concurrent systems. In this paper, we discuss how model checking can be extended to deal directly with ""actual"" descriptions of concurrent systems, e.g., impleme...","1999-07-01"
831,313186,"State-Based Model Checking of Event-Driven System Requirements","Joanne Atlee; John Gannon;","In this paper, we demonstrate how model checking can be used to verify safety properties for event driven systems. We present a technique for transforming event-oriented system requirements into state based structures, which we can then analyze using a state-based model checker. This technique was effective in uncovering violations of system invariants in both an automobile cruise control system and a water-level monitoring system. 1 Introduction A requirements specification is usually the first document to describe the required behavior of a system under development. If errors in this document are propagated to the design phase (or, worse, to the implementation) , they are more difficult and expensive to correct than if detected during the requirements phase [21]. Designers need techniques to analyze requirements before system design begins. The requirements specification is a behavioral specification of the system's activities, that describes the system's modes of operation and spec...","1996-02-23"
832,313796,"Partial Order Reductions for Timed Systems","Bengt Jonsson; Johan Bengtsson; Johan Lilius; Wang Yi;",". In this paper, we present a partial-order reduction method for timed systems based on a local-time semantics for networks of timed automata. The main idea is to remove the implicit clock synchronization between processes in a network by letting local clocks in each process advance independently of clocks in other processes, and by requiring that two processes resynchronize their local time scales whenever they communicate. A symbolic version of this new semantics is developed in terms of predicate transformers, which enjoys the desired property that two predicate transformers are independent if they correspond to disjoint transitions in different processes. Thus we can apply standard partial order reduction techniques to the problem of checking reachability for timed systems, which avoid exploration of unnecessary interleavings of independent transitions. The price is that we must introduce extra machinery to perform the resynchronization operations on local clocks. Fin...","1999-05-10"
833,314234,"On Reasonable and Forced Goal Orderings and their Use in an Agenda-Driven Planning Algorithm","Jana Koehler Jana;","The paper addresses the problem of computing goal orderings, which is one of the longstanding issues in AI planning. It makes two new contributions. First, it formally denes and discusses two dierent goal orderings, which are called the reasonable and the forced ordering. Both orderings are dened for simple STRIPS operators as well as for more complex ADL operators supporting negation and conditional eects. The complexity of these orderings is investigated and their practical relevance is discussed. Secondly, two dierent methods to compute reasonable goal orderings are developed. One of them is based on planning graphs, while the other investigates the set of actions directly. Finally, it is shown how the ordering relations, which have been derived for a given set of goals G, can be used to compute a so-called goal agenda that divides G into an ordered set of subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing sets of subgoals. This ...","2000-07-01"
834,314502,"The Impressive Power of Stopwatches","And Kim Larsen; Franck Cassez;","In this paper we de ne and study the class of stopwatch automata which are timed automata augmented with stopwatches and unobservable behaviour. In particular, we investigate the expressive power of this class of automata, and show as a main result that any finite or infinite timed language accepted by a linear hybrid automaton is also acceptable by a stopwatch automaton. The consequences of this result are two-fold: firstly, it shows that the seemingly minor upgrade from timed automata to stopwatch automata immediately yields the full expressive power of linear hybrid automata. Secondly, reachability analysis of linear hybrid automata may effectively be reduced to reachability analysis of stopwatch automata. This, in turn, may be carried out using an easy (over-approximating) extension of the efficient reachability analysis for timed automata to stopwatch automata. We report on preliminary experiments on analyzing translations of linear hybrid automata using a stopwatch-extension of the ...","2000-05-22"
835,315693,"An Old-Fashioned Recipe for Real Time","Leslie Lamport; Mart N Abadi;","Traditional methods for specifying and reasoning about concurrent systems work for real-time systems. Using TLA (the temporal logic of actions), we illustrate how they work with the examples of a queue and of a mutual-exclusion protocol. In general, two problems must be addressed: avoiding the real-time programming version of Zeno's paradox, and coping with circularities when composing real-time assumption/guarantee specifications. Their solutions rest on properties of machine closure and realizability. Contents 1 Introduction 1 2 Closed Systems 2 2.1 The Lossy-Queue Example : : : : : : : : : : : : : : : : : : : : : 3 2.2 The Semantics of TLA : : : : : : : : : : : : : : : : : : : : : : : 7 2.3 Safety and Fairness : : : : : : : : : : : : : : : : : : : : : : : : : 8 2.4 History-Determined Variables : : : : : : : : : : : : : : : : : : : 10 3 Real-Time Closed Systems 11 3.1 Time and Timers : : : : : : : : : : : : : : : : : : : : : : : : : : 11 3.2 Reasoning About Time : : : : : : : : ...","1996-04-20"
836,315867,"Chaotic Fixed Point Iterations","Alfons Geser; Bernhard Steen; D Kiel; D Passau; Gerald Luttgen; Jens Knoop; Oliver Ruthing;","In this paper we present a new fixed point theorem applicable for a countable system of recursive equations over a wellfounded domain. Wellfoundedness is an essential feature of many computer science applications as it guarantees termination of the corresponding fixed point computation algorithms. Besides being a natural restriction, it marks a new area of application, where not even monotonicity is required. We demonstrate the power and versatility of our fixed point theorem, which under the wellfoundedness condition covers all the known `synchronous' versions of fixed point theorems, by means of applications in data flow analysis and program optimization.","1999-11-24"
837,316694,"Speeding Up Image Computation by using RTL Information","Christian Stangier; Christoph Meinel; Fb Informatik;","Image computation is the core operation for optimization and formal verification of sequential systems like controllers or protocols. State exploration techniques based on OBDDs use a partitioned representation of the transition relation to keep the OBDD-sizes manageable. This paper presents a new approach that significantly increases the quality of the partitioning of the transition relation of controllers given in the hardware description language Verilog. The heuristic has been successfully applied to reachability analysis and symbolic model checking of real life designs, resulting in a significant reduction both in CPU time and memory consumption. 1 Introduction The computation of the reachable states (RS) of a sequential circuit is an important task for synthesis, logic optimization and formal verification. The increasing complexity of sequential systems like controllers or protocols requires efficient RS computation methods. If the RS are computed by using Ordered Binary...","2000-07-18"
838,318548,"Verifying Logic Circuits by Benders Decomposition","H. Yan; J. N. Hooker;","We describe a new tautology checking algorithm that can determine whether a logic circuit correctly implements a given boolean function. Although nonnumeric, the algorithm is equivalent to a numeric algorithm obtained by applying Benders decomposition to an integer programming formulation of the circuit verification problem. Computational testing suggests that the algorithm may be superior to methods based on binary decision diagrams on certain types of circuits. 1.2 Introduction One of the fundamental problems of VLSI design is checking whether a logic circuit implements a desired boolean function. This can be done by checking whether the circuit is equivalent to another circuit known to implement the desired function. The two circuits are typically compared by constructing binary decision diagrams (BDD's) for them or, when BDD's require too much computer memory, simply by simulating the circuits and comparing their outputs for every possible input (if it is practical to do so) [6, ...","1995-08-17"
839,319390,"The ffgraph Library","Carsten Friedrich; Lehrstuhl Fur Informatik;","this paper. Graphs can be loaded and saved in a slightly extended sgraph[Him93] file format","1999-01-10"
840,322240,"Verification of Linear Hybrid Systems By Means of Convex Approximations","Nicolas Halbwachs; Pascal Raymond; Yann-eric Proy;","We present a new application of the abstract interpretation by means of convex polyhedra, to a class of hybrid systems, i.e., systems involving both discrete and continuous variables. The result is an efficient automatic tool for approximate, but conservative, verification of reachability properties of these systems. 1 Introduction Timed automata [AD90] have been recently introduced to model real-time systems. A timed automaton is a finite automaton associated with a finite set of clocks, each clock counting the continuous elapsing of time. Each transition of the automaton can be guarded by a simple linear condition on the clock values, and can result in resetting some clocks to zero. A nice feature of this model is that it can be abstracted into a finite state system, and that all the standard verification problems (reachability, TCTL model-checking [ACD90, HNSY92]) are decidable. However, many interesting extensions of this model have been shown to lose this decidability propert...","1999-12-15"
841,322454,"Information Retrieval","null","Information retrieval is a wide, often loosely-defined term but in these pages I shall be concerned only with automatic information retrieval systems. Automatic as opposed to manual and information as opposed to data or fact. Unfortunately the word information can be very misleading. In the context of information retrieval (IR), information, in the technical meaning given in Shannon's theory of communication, is not readily measured (Shannon and Weaver1). In fact, in many cases one can adequately describe the kind of retrieval by simply substituting 'document' for 'information'. Nevertheless, 'information retrieval' has become accepted as a description of the kind of work published by Cleverdon, Salton, Sparck Jones, Lancaster and others. A perfectly straightforward definition along these lines is given by Lancaster2: 'Information retrieval is the term conventionally, though somewhat inaccurately, applied to the type of activity discussed in this volume. An information retrieval system does not inform (i.e. change the knowledge of) the user on the subject of his inquiry. It merely informs on the existence (or non-existence) and whereabouts of documents relating to his request.' This specifically excludes Question-Answering systems as typified by Winograd3 and those described by Minsky4. It also excludes data retrieval systems such as used by, say, the stock exchange for on-line quotations.","1999-05-07"
842,322864,"Information Technology And Control Needs For In-Situ Resource Utilization","Anthony R. Gross; Charles Pecheur; Daniel J. Clancy; Geoffrey A. Briggs; William E. Larson;","With the rapidly increasing performance of information technologies, a new capability is being developed that holds the clear promise of greatly increased exploration possibilities, along with dramatically reduced design, development, and operating costs. In addition, specific technologies such as neural nets will provide a degree of machine intelligence and associated autonomy which has previously been unavailable to the mission and spacecraft designer and the system operator. One of the most promising applications of these new information technologies is to the area of in-situ resource utilization. Useful resources such as oxygen, carbon dioxide, methane, and water can be extracted and/or generated from planetary atmospheres, to be used for propulsion and life-support needs. This can provide significant savings in the launch mass and costs. This paper will present the concepts that are currently under investigation and development for mining the Martian atmosphere, such as temperatur...","2000-05-27"
843,323613,"On the Hardness of Approximating the Minimum Consistent OBDD Problem","Ayumi Shinohara; Kouichi Hirata; Shinichi Shimozono;",". Ordered binary decision diagrams (OBDD, for short) represent Boolean functions as directed acyclic graphs. The minimum consistent OBDD problem is, given an incomplete truth table of a function, to find the smallest OBDD that is consistent with the truth table with respect to a fixed variable ordering. We show that this problem is NP-hard, and prove that there is a constant ffl ? 0 such that no polynomial time algorithm can approximate the minimum consistent OBDD within the ratio n ffl unless P=NP, where n is the number of variables. This result suggests that OBDDs are unlikely to be polynomial time learnable in PAC-learning model. Furthermore, we give a polynomial time learnable subclass of OBDDs representing symmetric functions. 1 Introduction For a class of representations of languages, the minimum consistent problem is to find a representation that is as small size as possible and is consistent with given positive and negative examples. The computational complexity o...","1996-09-18"
844,324937,"The Representation of Legal Contracts","Aspassia Daskalopulu; Marek Sergot;",": The paper outlines ongoing research on logic-based tools for the analysis and representation of legal contracts, of the kind frequently encountered in large-scale engineering projects and complex, long-term trading agreements. We consider both contract formation and contract performance, in each case identifying the representational issues and the prospects for providing automated support tools. Keywords: Artificial Intelligence and Law; Contract Drafting; Document Assembly; Knowledge Representation. 1. Introduction Over the last twenty years or so a growing body of research in Artificial Intelligence has focussed on the representation of legislation and regulations (for a comprehensive discussion see (Sergot, 1991)). The motivation for this has been twofold: on the one hand there have been opportunities for developing advisory systems for legal practitioners; on the other hand the Law is a complex domain in which diverse modes of reasoning are employed, offering ample opportunity...","2000-09-05"
845,325158,"A Refined Architecture for Terminological Systems: Terminology = Schema + Views","A. Schaerf; F. M. Donini; M. Buchheit; W. Nutt;","Traditionally, the core of a Terminological Knowledge Representation System #TKRS# consists of a TBox or terminology, where concepts are introduced, and an ABoxorworld description, where facts about individuals are stated in terms of concept memberships. This design has a drawback because in most applications the TBox has to meet two functions at a time: On the one hand---similarly to a database schema---frame-like structures with type information are introduced through primitive concepts and primitive roles; on the other hand, views on the objects in the knowledge base are provided through de#ned concepts. We propose to account for this conceptual separation by partitioning the TBox into two components for primitive and de#ned concepts, whichwe call the schema and the view part. Weenvision the two parts to di#er with respect to the language for concepts, the statements allowed, and the semantics. We argue that this separation achieves more conceptual clarity about the role of primit...","1999-11-12"
846,325396,"Timed Automata as Task Models for Event-Driven Systems","Anders Wall; Christer Ericsson; Wang Yi;","In this paper, we extend the classic model of timed automata with a notion of real time tasks. The main idea is to associate each discrete transition in a timed automaton with a task (an executable program). Intuitively, a discrete transition in an extended timed automaton denotes an event releasing a task and the guard on the transition species all the possible arriving times of the event (instead of the so{called minimal inter-arrival time). This yields a general model for hard real-time systems in which tasks are non-periodic. We show that the schedulability problem for the extended model can be transformed to a reachability problem for timed automata and thus it is decidable. This allows us to apply model-checking tools for timed automata to schedulability analysis for event-driven systems. In addition, based on the same model of a system, we may use the tools to verify other properties (e.g. safety and functionality) of the system. This unies schedulability analysis ...","1999-08-13"
847,325612,"Distributing Timed Model Checking - How the Search Order Matters","Frits Va; Gerd Behrmann; Thomas Hune;",". In this paper we address the problem of distributing model checking of timed automata. We demonstrate through four real life examples that the combined processing and memory resources of multiprocessor computers can be effectively utilized. The approach assumes a distributed memory model and is applied to both a network of workstations and a symmetric multiprocessor machine. However, certain unexpected phenomena have to be taken into account. We show how in the timed case the search order of the state space is crucial for the effectiveness and scalability of the exploration. An effective heuristic to counter the effect of the search order is provided. Some of the results open up for improvements in the single processor case. 1 Introduction The technical challenge in model checking is in devising algorithms and data structures that allow one to handle large state spaces. Over the last two decades numerous approaches have been developed that address this problem: symbolic metho...","2000-08-15"
848,326935,"Verification of Designs Containing Black Boxes","Bernd Becker; Nicole Drechsler; Rolf Drechsler; Wolfgang Gunther;","Often modern designs contain regions where the implementation of certain components is not (fully) known. These regions are called black boxes in the following. They occur e.g. if different designers work on a project in parallel or if IP cores are used. In this paper an approach based on a symbolic representation of characteristic functions for verifying circuits with black boxes is presented. We show that by this method more faults can be detected than with pure binary simulation and symbolic simulation using BDDs, respectively, only. This results from the formulation of our algorithm that allows implications over the black box. Experimental results are given to show what parts of a design can be proven to be correct, if black boxes are assumed. Of course, the probability for the detection of a fault in general depends on the size of the unknown regions. But fault injection experiments on benchmarks show that for many circuits even up to 90% of the faults are detected, even though ...","2000-08-14"
849,327917,"Fault Tree Analysis: 1020 Prime Implicants and Beyond","null","This paper presents a new analysis method of coherent as well as noncoherent fault trees that overcomes this limitation because its computational cost is not related to either the number of variables or the number of gates or the number of prime implicants of these trees. The interactive fault tree analyser METAPRIME that is based on this new method has been shown by experience to be able to perform in seconds the complete analysis of noncoherent fault trees with more than 10","1995-06-02"
850,328188,"An Overview of SAL","Ashish Tiwari; Eli Singerman; Harald Rue; John Rushby; N. Shankar; Saddek Bensalem; Sam Owre; Vijay Ganesh; Vlad Rusu; Yassine Lakhnech;","To become practical for assurance formal methods must be made more cost-eective and must contribute to both debugging and certication. Furthermore, the style of interaction must reect the concerns of a designer rather than the peculiarities of a prover. SAL (Symbolic Analysis Laboratory) attempts to address these issues. It is a framework for combining dierent tools to calculate properties (i.e., performing symbolic analysis) of concurrent systems. The heart of SAL is a language, developed in collaboration with Stanford, Berkeley, and Verimag, for specifying concurrent systems in a compositional way. Our instantiation of the SAL framework augments PVS with tools for abstraction, invariant generation, program analysis (such as slicing), theorem proving, and model checking to calculate properties (i.e., perform symbolic analysis) of concurrent systems. We describe the motivation, the language, the tools, and their integration in SAL/PVS, and some preliminary experience of their use. ...","1970-01-01"
851,328445,"Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment","null","this paper. Twoscheduling algorithms for this type of programming are studied; both are priority driven and preemptive; meaning that the processing of any task is interrupted by a request for any higher priority task. The rst algorithm studied uses a xed priority assignment and can achieve processor utilization on the order of 70 percent or more. The second scheduling algorithm can achieve full processor utilization by means of a dynamic assignment of priorities. Combination of these two algorithms is also discussed.","2000-08-28"
852,328511,"Automated Deduction and Formal Methods","John Rushby;",". The automated deduction and model checking communities have developed techniques that are impressively effective when applied to suitable problems. However, these problems seldom coincide exactly with those that arise in formal methods. Using small but realistic examples for illustration, I will argue that effective deductive support for formal methods requires cooperation among different techniques and an integrated approach to language, deduction, and supporting capabilities such as simulation and the construction of invariants and abstractions. Successful application of automated deduction to formal methods will enrich both fields, providing new opportunities for research and use of automated deduction, and making formal methods a truly useful and practical tool. 1 Introduction Formal methods are a natural application area for automated deduction---yet, with few exceptions, tools for mainstream formal methods provide little more than rudimentary support for deduction,...","1996-08-07"
853,328755,"Storage Assignment to Decrease Code Size","Albert Wang; Kurt Keutzer; Srinivas Devadas; Stan Liao; Steve Tjiang;","DSP architectures typically provide indirect addressing modes with auto-increment and decrement. In addition, indexing mode is not available, and there are usually few, if any, general-purpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into auto-increment and auto-decrement modes improves the size of the generated code. In this paper we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NP-complete. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given. Our experimental results indicate an improvement of 3 1 Introduction Microprocessors such as microcontrollers and fixedpoint digital signal processors (DSPs) are increasingly being embedded into many electron...","1995-06-02"
854,329196,"Symbolic Reachability Analysis based on SAT Solvers","Parosh Aziz Abdulla;",". The introduction of symbolic model checking using Binary Decision Diagrams (BDDs) has led to a substantial extension of the class of systems that can be algorithmically verified. Although BDDs have played a crucial role in this success, they have some well-known drawbacks, such as requiring an externally supplied variable ordering and causing space blowups in certain applications. In a parallel development, SAT-solving procedures, such as Stalmarck's method or the DavisPutnam procedure, have been used successfully in verifying very large industrial systems. These efforts have recently attracted the attention of the model checking community resulting in the notion of bounded model checking. In this paper, we show how to adapt standard algorithms for symbolic reachability analysis to work with SAT-solvers. The key element of our contribution is the combination of an algorithm that removes quantifiers over propositional variables and a simple representation that allows reu...","2000-01-17"
855,329665,"The Omega Test: a fast and practical integer programming algorithm for dependence analysis","William Pugh;","The Omega testi s ntegereger programmi ng algori thm that can determi ne whether a dependence exi sts between two array references, and i so, under what condi7: ns. Conventi nalwi[A m holds thati nteger programmiB techni:36 are far too expensi e to be used for dependence analysis except as a method of last resort for si:8 ti ns that cannot be deci:A by si[976 methods. We present evi[77B that suggests thiwi sdomi s wrong, and that the Omega testi s competi ti ve wi th approxi mate algori thms usedi n practi ce and suitable for use in production compilers. Experiments suggest that, for almost all programs, the average time required by the Omega test to determine the directi on vectors for an array pair is less than 500 secs on a 12 MIPS workstati on. The Omega testi based on an extension of Four i0-Motzki variable eli937 (aliB: programming method) to i nteger programming, and has worst-case exponential time complexity. However, we show that for manysiB7 ti ns i whi h ...","2000-08-21"
856,330460,"Recursive Layout Generation","Jeremy Dion; Louis M. Monier;","We present a recursive method for generating layout for VLSI chips which combines the flexibility of gate array and standard cell layout with the control and density of custom layout. The method allows seamless integration of hand-drawn and synthesized layout, so that hand layout need only be used where the increase in density is justified. Layout is generated automatically with predictable results; small changes in the source result in small changes of the overall layout. The system is versatile enough to build dense VLSI microprocessor chips automatically. d i g i t a l Western Research Laboratory 250 University Avenue Palo Alto, California 94301 USA ii Table of Contents 1. Introduction 1 2. The Annotated Hierarchical Netlist 1 2.1. Cell Generators 2 2.2. Netlist Traversal 5 3. Layout Generation 5 3.1. Hand-Drawn Cells 6 3.2. Leaf Cells 7 3.3. Composite Cells 8 3.4. Routing 9 3.5. Netlist Hierarchy Equals Layout Hierarchy? 10 4. Results 11 References 13 iii iv List of Figures ...","1999-01-27"
857,331393,"Synthesis and Testing of Bounded Wire Delay Asynchronous Circuits from Signal Transition Graphs","Luciano Lavagno;","Synthesis and Testing of Bounded Wire Delay Asynchronous Circuits from Signal Transition Graphs by Luciano Lavagno Doctor of Philosophy in Electrical Engineering and Computer Sciences University of California at Berkeley Professor Alberto Sangiovanni-Vincentelli, Chair The design of asynchronous circuits is increasingly important in solving problems such as complexity management, modularity, power consumption and clock distribution in large digital integrated circuits. The task is difficult mainly for the possible presence of hazards, i.e. deviations from the expected circuit behavior due to gate and wire delays. Efficient synthesis tools, which take into account the need for testing manufactured circuits, are required. The problem has been studied extensively in the past, but no satisfactory automated solution using a realistic delay model has been presented. This thesis introduces the problem through an extensive literature review and then proposes a synthesis procedure based on the...","1995-01-26"
858,331854,"Drawing Graphs With Dot","Eleftherios Koutsofios; Sortedlist Intset; Sparcascode Sparcmcemit; Stephen C. North;","dot draws directed graphs as hierarchies. Like its predecessor, dag, it is a Unix filter, makes good drawings, and runs quickly. Its important new features are node ports for drawing data structures with pointers; improved placement of nodes, edge splines and labels; cluster layouts; and an underlying file language for graph tools. Here is a reduced module dependency graph of the SML-NJ compiler. The layout took 3.5 seconds of user time on an HP-9000/730 computer.","1996-12-06"
859,332114,"Formal Specification and Verification for Critical Systems: Tools, Achievements, and Prospects","null","Formal specification and verification use mathematical techniques to help document, specify, design, analyze, or certify computer software and hardware. Mathematically-based notation can provide specifications that are precise and unambiguous and that can be checked mechanically for certain types of error. Formal verification uses theorem proving techniques to establish consistency between one level of formal specification and another. This paper describes some of the issues in the design and use of formal specification languages and verification systems, outlines some examples of the application of formal methods to critical systems, and identifies the benefits that may be obtained from this technology. 1 Introduction Formal specification and verification are examples of what are often called formal methods in computer science. And formal methods are simply those that use mathematical techniques to help document, specify, design, analyze, or certify computer software and ha...","1998-09-28"
860,332670,"The PVS Specification Language","null",", 16 completion analysis, 33 CONJECTURE, 16 conservative extension, 13 constants, 12--13 CONTAINING, 18 COROLLARY, 16 curried applications, 25 declarations, 9--16 formulas, 16 multiple, 9 dependent types, 21--26 empty types, 18 enumeration types, 10, 12 equality, 22 EXISTS, 25 exporting, 9 expression, 29 expressions, 22 f91, 14 FACT, 16 FALSE, 22 FORALL, 25 formal parameters, see theory parameters FORMULA, 16 formula declarations, 16 function types, 19--20 IF-THEN-ELSE, 24 IFF, 22 IMPLIES, 22 importing, 9 importings, 34 inequality, 22 interpreted type declarations, 11--12 interpreted types, 10 kind, 9 expr, 9 prop, 9 theory, 9 type, 9 LAM, see LAMBDA LAMBDA, 25 lambda expressions, 26 L","1994-09-01"
861,332791,"An Integration of Model Checking with Automated Proof Checking","M. K. Srivas; N. Shankar; S. Rajan;","Although automated proof checking tools for general-purpose logics have been successfully employed in the verification of digital systems, there are inherent limits to the efficient automation of expressive logics. If the expressiveness is constrained, there are useful logic fragments for which efficient decision procedures can be found. The model checking paradigm yields an important class of decision procedures for establishing temporal properties of finite-state systems. Model checking is remarkably effective for automatically verifying finite automata with relatively small state spaces, but is inadequate when the state spaces are either too large or unbounded. For this reason, it is useful to integrate the complementary technologies of model checking and proof checking. Such an integration has to be carried out in a delicate manner in order to be more than just the sum of the techniques. We describe Supported by ARPA under contract PR8556, by NSF Grant CCR-930044, and ...","1995-05-19"
862,335190,"Kernels for Safety?","John Rushby;","Secure systems are often built around a ""security kernel""---a relatively small and simple component that guarantees the security of the overall system. In this paper we ask whether this approach can be used to ensure system properties other than security---in particular, we are interested in whether ""safety"" properties can be handled in this way. Our conclusion is that kernelized system structures can provide rigorous guarantees that certain faults of commission will not occur. We give a more precise characterization in terms of the formal statement that can be asserted for a kernelized system and we outline an approach to system design that uses these insights and draws on experience with secure systems in order guarantee certain safety properties. 1 Introduction Computer systems perform many critical tasks: tasks where certain kinds of failures cannot be tolerated. Failures are the result of faults and the prevention of failure therefore depends on eliminating faults, or ...","1994-11-17"
863,335552,"PAM: A Process Algebra Manipulator","Huimin Lin;","PAM is a general proof tool for process algebras. It allows users to define their own calculi and then perform algebraic style proofs in these calculi by directly manipulating process terms. The logic that PAM implements is equational logic plus recursion, with some features tailored to the particular requirements of process algebras. Equational reasoning is implemented by rewriting, while recursion is dealt with by induction. Proofs are constructed interactively, giving users the freedom to control the proof processes. 1 Introduction It has been gradually recognized that computer assistance is essential for the analysis of concurrent systems. There are already a number of proof tools, among them are the Concurrency Workbench [CPS 89], TAV [GLZ 89], and Auto [BRSV 89]. Most of these tools are behaviourally based and perform proofs automatically. They interpret processes as labelled transition systems and proofs are established by automatically searching the resulting spaces. M...","1993-03-01"
864,335678,"A Partial Order Approach to Branching Time Logic Model Checking","And Ruurd Kuiper; Doron Peled; Rob Gerth; Wojciech Penczek;","Partial order techniques enable reducing the size of the state space used for model checking, thus alleviating the `state space explosion' problem. These reductions are based on selecting a subset of the enabled operations from each program state. So far, these methods have been studied, implemented and demonstrated for assertional languages that model the executions of a program as computation sequences, in particular the logic LTL (linear temporal logic). The present paper shows, for the first time, how this approach can be applied to languages that model the behavior of a program as a tree. We study here partial order reductions for branching temporal logics, e.g., the logics CTL and CTL (with the nexttime operator removed) and process algebra logics such as Hennesy-Milner Logic (with actions). Conditions on the selection of subset of successors from each state during the state-space construction, which guarantee reduction that preserves CTL properties, are given. Prov...","1999-08-30"
865,335701,"A Formal Methodology for Hardware/Software Co-design of Embedded Systems","Alberto Sangiovanni-vincentelli; Attila Jurecska; Harry Hsieh; Luciano Lavagno; Massimiliano Chiodo; Paolo Giusto;","Our methodology for control-dominated embedded reactive systems is based on an implementation-independent representation, Codesign Finite State Machines (CFSMs). This representation allows us to preserve semantical correctness throughout the design process, because CFSMs assume unbounded, non-zero reaction delays, that correspond both to hardware and software behavior. CFSMs can be generated from a variety of specication languages, such as Esterel, StateCharts, VHDL, Verilog. Partitioning is user-driven, while our codesign environment performs automatically all other synthesis tasks. We derive hardware directly from CFSMs, assuming single-cycle reaction delays (but pipelining is also allowed by the model semantics). We synthesize software using an intermediate representation, Software Graphs, that allows better optimization and tighter control over implementation costs than general-purpose compilers. We are also planning to synthesize automatically a real-time scheduler a...","1999-11-05"
866,335812,"Formal Verification of the AAMP5 Microprocessor 1 - A Case Study in the . . .","null","ion function between these levels (Figure 1) and showing that the sequence of micro--instructions f 1 , f 2, ..., f n making up each machine instruction F causes a corresponding change in the micro--state s 1 as F does to the macro--state S 1 , i.e., that F(Abstraction(s 1 )) = Abstraction(f n (...(f 2 (f 1 (s 1 ))))). This basic notion of correctness must be supplemented with a few additional assurances, such as a demonstration that the machine reaches a valid initial state after power--up and that each instruction eventually terminates. Further refinements are also necessary to deal with the internal pipelining of the AAMP5. These details are discussed more fully in Sections 4, 5, and 6. ############################## This project was selected by Collins and SRI for a number of reasons. Both Collins and SRI wanted to explore the usefulness of formal verification on an example that was large enough to provide realistic insight, yet small enough to be completed at reasonable cost. Ve...","1994-10-21"
867,336038,"Theoretical Results on Reinforcement Learning with Temporally Abstract Options","null","We present new theoretical results on planning within the framework of temporally abstract reinforcement learning (Precup & Sutton, 1997; Sutton, 1995). Temporal abstraction is a key step in any decision making system that involves planning and prediction. In temporally abstract reinforcement learning, the agent is allowed to choose among ""options"", whole courses of action that may be temporally extended, stochastic, and contingent on previous events. Examples of options include closed-loop policies such as picking up an object, as well as primitive actions such as joint torques. Knowledge about the consequences of options is represented by special structures called mu...","2000-07-17"
868,336461,"Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers","Of A Small Fully associative;","Projections of computer technology forecast processors with peak performance of 1,000 MIPS in the relatively near future. These processors could easily lose half or more of their performance in the memory hierarchy if the hierarchy design is based on conventional caching techniques. This paper presents hardware techniques to improve the performance of caches. Miss caching places a small fully-associative cache between a cache and its refill path. Misses in the cache that hit in the miss cache have only a one cycle miss penalty, as opposed to a many cycle miss penalty without the miss cache. Small miss caches of 2 to 5 entries are shown to be very effective in removing mapping conflict misses in first-level direct-mapped caches. Victim caching is an improvement to miss caching that loads the small fully-associative cache with the victim of a miss and not the requested line. Small victim caches of 1 to 5 entries are even more effective at removing conflict misses than miss caching. St...","1999-01-27"
869,336989,"Implicit Prime Cover Computation: An Overview","Henri Fraisse; Jean Christophe Madre; Olivier Coudert;","A set of products is a prime cover of a Boolean function f if it is made of prime implicants of f , and if the sum of its products covers f . Finding a prime cover, an irredundant prime cover, or a minimal prime cover of a function f is a problem that arises in several fields of computer science, for instance in logic synthesis, automated reasoning, realiability analysis, and some optimization problems. This paper shows how the three prime cover computation problems mentioned above can be efficiently solved using implicit manipulations of sets of products. 1 Introduction Computing a prime cover, an irredundant prime cover, or a minimal prime cover of a Boolean function has several applications in computer science. In logic synthesis, an irredundant prime cover, or better, a minimal prime cover, provides the user with an efficient 2-level logic implementation of a single or multi output Boolean function [2, 24, 14]. In reliability analysis, prime covers are a way for either exhaustive...","1995-06-02"
870,337055,"Some Theorems We Should Prove","David Lorge Parnas;","Mathematical techniques can be used to produce precise, provably complete documentation for computer systems. However, such documents are highly detailed and oversights and other errors are quite common. To detect the ""early"" errors in a document, one must attempt to prove certain simple theorems. This paper gives some examples of such theorems. 1 Introduction In [4], we have shown how the contents of key computer systems documents can be defined in terms of mathematical functions and relations. We also reminded our readers that (1) functions and relations can be viewed as sets of ordered pairs, (2) sets can be characterised by predicates and described by logical expressions, (3) predicates can be represented in a more readable way using multidimensional (tabular) expressions whose components are logical expressions and terms, and (4) the meaning of these tables can be defined by rules for translating those tables into more conventional expressions. A complete discussion of these...","1995-06-13"
871,337653,"A Formal Model of the Ada Ravenscar Tasking Profile; Protected Objects","Kristina Lundqvist; Lars Asplund; Stephen Michell;","The definition of the Ravenscar Tasking Profile for Ada95 provides a definition of a tasking runtime system with deterministic behaviour and low enough complexity to permit a formal description of the model. The complete model of the Protected Object portion of the Ravenscar Model is presented in Uppaal. Some important properties are verified such as timing of calls to protected procedure. This is the first time a part of an Ada run-time has been formally verified.","2000-04-26"
872,339304,"The OTI Constraint Solver: A Constraint Library for Constructing Interactive Graphical User Interfaces","Alan Borning; Bjorn N. Freeman-benson;","ENVY/Constraints is an IBM/Smalltalk library that provides a constraint satisfier and a collection of useful constraints, targeted at developing interactive graphical user interfaces. The solver supports multi-way constraints and constraint hierarchies. It uses a hybrid algorithm, combining local propagation with pluggable cycle solvers, allowing constraints to range over arbitrary objects, while at the same time satisfying constraints representing simultaneous equations over the reals. 1 Introduction A variety of research projects have demonstrated the utility of constraints in building interactive graphical applications. Some applications of constraints in user interface software are maintaining consistency between application data and views of that data on the screen, maintaining consistency among multiple views, and determining the layout of windows and other graphical entities, in particular specifying geometric constraints on manipulable graphical objects. Early examples of...","1997-10-19"
873,340126,"New Directions in Cryptography","Martin E. Hellman; Whitfield Diffie;","Two kinds of contemporary developments in cryp- communications over an insecure channel order to use cryptogtography are examined. Widening applications of teleprocess- raphy to insure privacy, however, it currently necessary for the ing have given rise to a need for new types of cryptographic communicating parties to share a key which is known to no systems, which minimize the need for secure key distribution one else. This is done by sending the key in advance over some channels and supply the equivalent of a written signature. This secure channel such a private courier or registered mail. A paper suggests ways to solve these currently open problems. private conversation between two people with no prior acquainIt also discusses how the theories of communication and computance is a common occurrence in business, however, and it is tation are beginning to provide the tools to solve cryptographic unrealistic to expect initial business contacts to be postponed problems of long standing. ...","2000-09-18"
874,340170,"A Systematic Study of Heuristic Analysis","Macha Nikolskaia;","this paper we study the efficiency of conventional methods such as static and dynamic variable ordering heuristics. For a more detailes presentation of this study see (?).","2000-03-30"
875,340531,"BDD Minimization by Truth Table Permutations","Masahiro Fujita; Robert K. Brayton; Yuji Kukimoto;","Bern, Meinel and Slobodova [1] have recently proposed a novel technique called cube transformations to minimize the size of binary decision diagrams. Given a Boolean function, they try to find a domain transformation such that the function after the transformation has a smaller BDD. It has been shown that well-chosen transformations allow us to represent compactly the hidden-weight-bit functions, which are known to have exponential-sized BDDs regardless of variable orderings. Although their results are promising, no heuristics for picking good transformations are provided in [1] , i.e. they carefully chose a particular transformation for each function by investigating the characteristic of the function. In this paper, we present a set of heuristic algorithms which construct effective transformations starting from given BDDs. The algorithms are incremental in the sense that they construct a transformation as the combination of local transformations, each of which is guarant...","1995-03-27"
876,340927,"Utilizing Symmetry when Model Checking under Fairness Assumptions","A. P. Sistla; E. A. Emerson;","One technique for combating the state explosion problem is to exploit symmetry [12, 5, 9] when performing temporal logic model checking [3, 4]. The works of Clarke, Filkorn, & Jha [5] and Emerson & Sistla [9] show how, using some basic notions of group theory, symmetry may be exploited for the full range of correctness properties expressible in the very expressive temporal logic CTL*. Surprisingly, while fairness properties are readily expressible in CTL*, the methods of [5] and [9] are not powerful enough to admit any amelioration of state explosion, when standard fairness assumptions are involved. In [5; 9] model checking over a large structure M is reduced to model checking over a small quotient structure M derived from M by identifying ""G-symmetric"" states, where G is a subgroup of permutations on process indices respecting the symmetry of M and leaving invariant the ""maximal"" propositonal subformulas of the specification f . The latter requirement is crucial but is also the sour...","1994-05-25"
877,341356,"Learning from Examples: Generation and Evaluation of Decision Trees for Software Resource Analysis","Adam A. Porter; Richard W. Selby;","Solutions to the problem of learning from examples will have far-reaching benefits, and therefore, the problem is one of the most widely studied in the field of machine learning. The purpose of this study is to investigate a general solution method for the problem, the automatic generation of decision (or classification) trees. The approach is to provide insights through in-depth empirical characterization and evaluation of decision trees for one problem domain, software resource data analysis. The purpose of the decision trees is to identify classes of objects (software modules) that had high development effort or faults, where ""high"" was defined to be in the uppermost quartile relative to past data. Sixteen software systems ranging from 3000 to 112,000 source lines have been selected for analysis from a NASA production environment. The collection and analysis of 74 attributes (or metrics), for over 4700 objects, capture a multitude of information about the objects: development effort...","1994-06-15"
878,342910,"Hardware Verification using Monadic Second-Order Logic","David A. Basin; Nils Klarlund;","We show how the second-order monadic theory of strings can be used to specify hardware components and their behavior. This logic admits a decision procedure and counter-model generator based on canonical automata for formulas. We have used a system implementing these concepts to verify, or find errors in, a number of circuits proposed in the literature. The techniques we use make it easier to identify regularity in circuits, including those that are parameterized or have parameterized behavioral specifications. Our proofs are semantic and do not require lemmas or induction as would be needed when employing a conventional theory of strings as a recursive data type. Keywords: Monadic second order logic, automatic theorem proving, hardware verification, mathematical induction. The first author was funded by the German Ministry for Research and Technology (BMFT) under grant ITS 9102. Responsibility for the content lies with the authors. The authors thank Alan Bundy, Harald Ganzinge...","1995-02-20"
879,343026,"Fine-Tuning of Boolean Formulae Preprocessing Techniques","A. Rauzy; M. Nikolskaia;",": This paper presents constant propagation and common factors rewriting methods for boolean formula preprocessing. Formula rewriting performs symbolic simplification, and attempts to reconstitute hidden structure in the model that was lost by the conversion to a boolean formula. We show examples of fault trees whose assessment becomes feasible only after this kind of preprocessing. 1 INTRODUCTION Fault trees are widely used for safety analysis. In this context fault trees are treated as boolean formulae. The major problem of their assessment is combinatorial explosion in time and space. Binary Decision Diagrams (BDD) are the state-of-the-art compact representation for boolean formulae (Bryant, 1986; Rauzy, 1997). By limiting combinatorial growth, this representation makes possible the assessment of complex fault trees, both qualitatively by minimal cutset search and quantitatively by exact calculation of top event probability. In this paper we focus on a new method that enables the ...","1999-12-01"
880,343214,"SCF - State Machine Controlled Flow Diagrams","Dirk Ziegenbein; Jurgen Teich; Karsten Strehl; Lothar Thiele; Martin Naedele;","In this paper, existing state machine and dataflow models of computation are revisited. A common representation called SCF is presented that enables the representation of several dataflow models using a mixture of functional programming and state machines. In particular, models like cyclostatic dataflow [3, 8], Synchronous Data Flow [14, 15], marked graphs [7] and communicating state machines [11, 18, 1] as well Petri nets turn out to be special subclasses of the SCF model. The model can be extended with information on the timing of computations and timing constraints. In addition, as abstraction and refinement are defined, the new model supports a hierarchical approach to problems like scheduling, verification and implementation. ETH Zurich, Computer Engineering and Networks Laboratory, CH-8092 Zurich, email: fthiele, teich, naedele, strehlg@tik.ee.ethz.ch and Technische Universitaet Braunschweig, Institut fuer Datenverarbeitungsanlagen, D-38106 Braunschweig, email: ziegenb...","2000-08-08"
881,343597,"Specification and Verification of System-Level Hardware Designs using Timing Diagrams","Abteilung Rechnerarchitektur; And Werner Damm; Fachbereich Informatik; Rainer Schlr;","In this paper we present a novel approach to the specification and verification of system-level hardware designs. It is based on Timing Diagrams, a graphical specification language with an intuitive semantics, which is especially appropriate for the description of asynchronous distributed systems such as hardware designs. Timing Diagrams and their semantics are formally defined based on a translation to Temporal Logic. It is shown that for the resulting type of formulas there is an efficient model checking procedure, thus allowing fully automatic verification of hardware designs.","1999-06-25"
882,344181,"Using Qualitative Models to Guide Inductive Learning","Peter Clark; Stan Matwin;","This paper presents a method for using qualitative models to guide inductive learning. Our objectives are to induce rules which are not only accurate but also explainable with respect to the qualitative model, and to reduce learning time by exploiting domain knowledge in the learning process. Such explainability is essential both for practical application of inductive technology, and for integrating the results of learning backinto an existing knowledge-base. We apply this method to two process control problems, a water tank network and an ore grinding process used in the mining industry. Surprisingly, in addition to achieving explainability the classi#cational accuracy of the induced rules is also increased. We showhow the value of the qualitative models can be quanti #ed in terms of their equivalence to additional training examples, and #nally discuss possible extensions. 1 INTRODUCTION 1.1 OVERVIEW This paper presents and evaluates a technique for using qualit...","1970-01-01"
883,344666,"Size of OBDD representation of 2-level redundancies functions","L. Nikolskaia; M. Nikolskaia;","We consider an explicit boolean function depending on n = (i + 1) j variables encoding 2-level cascade redundancies in critical systems. We establish lower and upper bounds on the size of its OBDD representation. The exact lower bound of this function is comprised between e min( i 2 ; j 2 ) ) and O(2 min(i;j) ). The upper bound is 3 i(j 2log 2 i) ). We further show that, when one of the parameters i or j is constant, this function admits OBDDs whose size is polynomial in other parameter. The latter result is constructive, and we show variable orderings that witness this polynomial size. 1 Introduction Ordered binary decision diagrams (OBDD) [3] are of immense practical interest as representations of boolean functions, since they provide compact and canonical representations that admit efficient operations for a wide variety of practial problems. OBDDs and their variants are widely used in logic verification [10,6], quantitative and qualitative analysis of reliability mode...","2000-02-15"
884,345161,"Constraint Solving by Narrowing in Combined Algebraic Domains","Christophe Ringeissen;","Narrowing is a way to integrate function evaluation and equality definition into logic programming. Here we show how this can be combined with the constraint paradigm. We propose a solver for goals with constraints in theories defined by unconstrained equalities and rewrite rules with constraints expressed in an algebraic built-in structure. The narrowing method reduces the goal solving problem in the whole theory to rewriting and constraint solving in an adequate combined theory. The combined solver is obtained through the combination of a solver in the built-in structure and a solver for the unconstrained equalities. Sufficient syntactic conditions are proposed to get a process that enumerates a complete set of solutions. 1 Introduction Narrowing provides integration of function evaluation and equality definition into logic programming [6, 12, 8, 18, 10]. In this work, we show how this can be connected with the constraint paradigm to get a constraint solver on combined algebraic dom...","1994-10-27"
885,345650,"Symbolic Bisimulation Minimisation","Amar Bouali; Robert De Simone;","We adapt the Coarsest Partition Refinement algorithm to its computation using the specific data structures of Binary Decision Diagrams. This allows to generate symbolically the set of equivalence classes of a finite automaton with respect to bisimulation, without constructing the automaton itself. These equivalence classes represent of course the (new) states of the canonical minimal automaton bisimilar to the early one. The method works from labeled synchronised vectors of automata as the distributed system description. We report on performances of Hoggar, a tool implementing our method. 1 Introduction Bisimulation is a central notion in the domain of verification of concurrent systems [18]. It was introduced as the major behavioural equivalence in the setting of process algebras [18, 2], but works at the interpretation level of labeled transition systems. Algorithmic properties of bisimulation in the finite state case have been widely studied [16, 20, 11], leading to a lar...","1996-02-21"
886,345830,"Heuristics for BDD handling of sum-of-products formulae","A. Rauzy; M. Nikolskaia;",": This paper presents the result of research on heuristic methods for assessment of fault-trees (boolean formulae) using Binary Decision Diagrams (BDDs for short). BDDs are the state-of-the-art compact representation of boolean formulae. The fault trees under consideration are given in the sum-of-products or products-of-sums form. We propose a new heuristic that is both robust and discriminating on these particular formulae. The sizes of the BDD obtained are much smaller than those obtained using standard heuristics. However from the theoretical point of view the size of the corresponding BDD remains exponentially large in the worst case. Nevertheless, in practice we manage to deal with complex formulae by means of a new rewriting method which we propose in this article. 1 INTRODUCTION Faced with the intractability of most of the interesting properties of boolean formulae --- assessing a fault tree, finding satisfying solutions, counting the number of solutions---it becomes necessary...","1999-12-01"
887,348764,"Remote Agent: To Boldly Go Where No AI System Has Gone Before","Barney Pell; Brian C. Williams; Nicola Muscettola; P. Pandurang Nayak;","Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous fleets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraint-based temporal planning and scheduling, rob...","1999-04-21"
888,349836,"Truth Maintenance","David Mcallester;","General purpose truth maintenance systems have received considerable attention in the past few years. This paper discusses the functionality of truth maintenance systems and compares various existing algorithms. Applications and directions for future research are also discussed. Introduction In 1978 Jon Doyle wrote a masters thesis at the MIT AI Laboratory entitled ""Truth Maintenance Systems for Problem Solving"" [ Doyle, 1979 ] . In this thesis Doyle described an independent module called a truth maintenance system, or TMS, which maintained beliefs for general problem solving systems. In the twelve years since the appearance of Doyle's TMS a large body of literature has accumulated on truth maintenance. The seminal idea appears not to have been any particular technical mechanism but rather the general concept of an independent module for truth (or belief) maintenance. All truth maintenance systems manipulate proposition symbols and relationships between proposition symbols. I will use...","1999-11-19"
889,350038,"Pleiades: An Object Management System for Software Engineering Environments","Lori A. Clarke; Peri Tarr;","Software engineering environments impose challenging requirements on the design and implementation of an object management system. Existing object management systems have been limited in both the kinds of functionality they have provided and in the models of support they define. This paper describes a system, called Pleiades, which provides many of the object management capabilities required to support software engineering environments. 1 Introduction Software engineering environments support the process of producing and maintaining software systems. One of the most common and pervasive activities of software developers is the creation and manipulation of software objects that represent artifacts of the software development process, such as requirements, specifications, designs, source code, test data, and analysis results. Objects that are created during the software development process tend to be large, complex structures with complex interrelationships to other objects. For exampl...","1994-06-23"
890,350770,"Safe BDD Minimization Using Don't Cares","null","In many computer-aided design tools, binary decision diagrams (BDDs) are used to represent Boolean functions. To increase the efficiency and capability of these tools, many algorithms have been developed to reduce the size of BDDs. This paper presents heuristic algorithms that minimize the size of BDDs representing incompletely specified functions by intelligently assigning don't cares to binary values. The traditional algorithm, restrict [8], is often effective in BDD minimization, but can increase the BDD size. We propose new algorithms based on restrict which are guaranteed never to increase the size of the BDD, thereby significantly reducing peak memory requirements. Experimental results show that our techniques typically yield significantly smaller BDDs than restrict. 1 Introduction The efficient representation and manipulation of Boolean functions is critical to many computer-aided design applications including logic synthesis, formal verification, and testing. Binary decision ...","1997-07-04"
891,350950,"Integration of Medium-Throughput Signal Processing Algorithms on Flexible Instruction-Set Architectures","Augusli Kifli; Dirk Lanneer; Francis Depuydt; Francky Catthoor; Gert Goossens; Hugo De Man; Koen Schoofs; Marc Pauwels; Marco Cornero; Paolo Petroni;","Integrated circuits in telecommunications and consumer electronics are rapidly evolving towards single chip solutions. New IC architectures are emerging, which combine instruction-set processor cores with customised hardware. This paper describes a highlevel synthesis system for integration of real-time signal processing systems on such processor cores. The compiler supports a flexible architectural model. It can handle certain types of incompletely specified architectures, and offers capabilities for retargetable compilation and architectural exploration. Results for a realistic application from the domain of audio processing indicate the feasibility and power of the presented approach. 1 Introduction The electronic systems industry of the nineties is confronted with the challenge of integrating complex multi-functional systems in silicon. High-volume markets like end-user telecommunications and consumer electronics require cost efficient solutions in the form of applications...","1994-09-09"
892,352182,"Staggered Striping in Multimedia Information Systems","R. Muntz; Richard Muntz; S. Ghandeharizadeh Csd; Shahram Ghandeharizadeh; Steven Berson; X. Ju; Xiangyu Ju;","Multimedia information systems have emerged as an essential component of many application domains ranging from library information systems to entertainment technology. However, most implementations of these systems cannot support the continuous display of multimedia objects and suffer from frequent disruptions and delays termed hiccups. This is due to the low I/O bandwidth of the current disk technology, the high bandwidth requirement of multimedia objects, and the large size of these objects that almost always requires them to be disk resident. One approach to resolve this limitation is to decluster a multimedia object across multiple disk drives in order to employ the aggregate bandwidth of several disks to support the continuous retrieval (and display) of objects. This paper describes staggered striping as a novel technique to provide effective support for multiple users accessing the different objects in the database. Detailed simulations confirm the superiority of stagge...","1995-10-10"
893,352898,"Rematerialization","Keith D. Cooper; Linda Torczon; Preston Briggs;","This paper examines a problem that arises during global register allocation -- rematerialization. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graph-coloring allocator handled simple instances of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance. Our approach is to tag individual values in the procedure 's SSA graph with information specifying how it should be spilled. We use a variant of Wegman and Zadeck's sparse simple constant algorithm to propagate tags throughout the graph. The allocator then splits live ranges into values with different tags. This isolates those values that can be easily rematerialized from values that require general spilling. We modify the base allocator to use this information when estimating spill costs and introducing spill code. Our p...","1995-05-09"
894,352956,"Model Checking and Modular Verification","David E. Long; Orna Grumberg;","We describe a framework for compositional verification of finite state processes. The framework is based on two ideas: a subset of the logic CTL for which satisfaction is preserved under composition; and a preorder on structures which captures the relation between a component and a system containing the component. Satisfaction of a formula in the logic corresponds to being below a particular structure (a tableau for the formula) in the preorder. We show how to do assume-guarantee style reasoning within this framework. In addition, we demonstrate efficient methods for model checking in the logic and for checking the preorder in several special cases. We have implemented a system based on these methods, and we use it to give a compositional verification of a CPU controller. 1 Introduction Temporal logic model checking procedures are useful tools for the verification of finite state systems [3, 12, 20]. However, these procedures have traditionally suffered from the state explosion proble...","1996-04-15"
895,353044,"A Lower Bound For Integer Multiplication With Read-Once Branching Programs","Stephen Ponzio;",". We prove that read-once branching programs computing integer multiplication require size 2 ## # n) . This is the first nontrivial lower bound for multiplication on branching programs that are not oblivious. By the appropriate problem reductions, we obtain the same lower bound for other arithmetic functions. Key words. multiplication, read-once, branching programs, BDD, verification AMS subject classifications. 68Q05, 68Q25, 68M15 PII. S0097539795290349 1. Introduction and background. It is well known that many functions, some of them very simple, cannot be computed by read-once branching programs of polynomial size [We88, Za84, Du85, We87, BHST87, Ju88, Kr88]. Interest in whether integer multiplication can be so computed has been created by recent developments in the field of digital design and hardware verification. 1.1. Hardware verification and branching programs. The central problem of verification is to check whether a combinational hardware circuit has been correctly designe...","1970-01-01"
896,355768,"Linear Sifting Decision Diagrams","Christoph Meinel; Fabio Somenzi; Thorsten Theobald;","We propose a new algorithm, called linear sifting, for the optimization of decision diagrams that combines the e#- ciency of siftingandthepower of linear transformations. Weshowthatthenew algorithm is applicable to large examples, andthatinmany cases it leads tosubstantially more compact diagrams when compared to simple variable reordering. Weshowinwhat sense linear transformations complement variable reordering, Andre discuss applications of then technique to synthesis andveri#cation. 1 Introduction Binary decision diagrams #6# are widely used in formal veri#cation and synthesis algorithms because they provide compact representation of logic functions. Successful application to complex problems, however, almost invariably entails resortingtovarious techniques toreduce the size of the diagrams that the programs must manipulate. Some techniques are lossy,and rely on several forms of approximations #9, 14, 8, 20#. Other techniques try to#nd a more compact representation without any los...","1997-07-04"
897,356801,"Verification of Hierarchical State/Event Systems using Reusability and Compositionality","Gerd Behrmann; Henrik Hulgaard; Henrik R. Andersen; Jrn Lind-nielsen; Kim G. Larsen; Using Reusability;","We investigate techniques for verifying hierarchical systems, i.e., finite state systems with a nesting capability. The straightforward way of analyzing a hierarchical system is to first flatten it into an equivalent non-hierarchical system and then apply existing finite state system verification techniques. Though conceptually simple, flattening is severely punished by the hierarchical depth of a system. To alleviate this problem, we develop a technique exploiting the hierarchical structure to reuse earlier reachability checks of superstates to conclude reachability of substates. We combine the reusability technique with the successful compositional technique of [13] and investigate the combination experimentally on industrial systems and hierarchical systems generated according to our expectations to real systems. The experimental results are very encouraging: whereas a flattening approach degrades in performance with an increase in the hierarchical depth (even when appl...","1998-10-09"
898,357898,"Verification Tools for Finite-State Concurrent Systems","D. Long; E. Clarke; O. Grumberg;",": Temporal logic model checking is an automatic technique for verifying finite-state concurrent systems. Specifications are expressed in a propositional temporal logic, and the concurrent system is modeled as a state-transition graph. An efficient search procedure is used to determine whether or not the state-transition graph satisfies the specification. When the technique was first developed ten years ago, it was only possible to handle concurrent systems with a few thousand states. In the last few years, however, the size of the concurrent systems that can be handled has increased dramatically. By representing transition relations and sets of states implicitly using binary decision diagrams, it is now possible to check concurrent systems with more than 10 120 states. In this paper we describe in detail how the new implementation works and give realistic examples to illustrate its power. We also discuss a number of directions for future research. The necessary background information...","1993-10-17"
899,358755,"Report on the CE on the Transcoding Hint DS","null","This document contains the report of the core experiment validating the Media Transcoding Hint DS in a video content delivery and transcoding application based on network conditions and user preferences. The Media Transcoding Hint DS currently includes the Utility Scaling DS, the Motion Hint DS, the Difficulty Hint DS, and the Importance Hint attribute whose validation results are described in this document. The Media Transcoding Hint DS is a DS for the Universal Multimedia Access (UMA). UMA is used in an application that deals with delivery of image, video, audio and multimedia content under different network conditions, user and publisher preferences, and capabilities of terminal devices [1]. The primary motivation of UMA is to enable terminals with limited communication, processing, storage and display capabilities to access rich multimedia content [4]. Several UMArelated requirements have been introduced into the MPEG-7 Requirements document [2]. 1 Transcoding Hint DS","2000-06-10"
900,358813,"Verifying Safety Properties of a PowerPC Microprocessor Using Symbolic Model Checking without BDDs","Armin Biere; Edmund Clarke; Richard Raimi; Yunshan Zhu;",". In [1] Bounded Model Checking with the aid of satisfiability solving (SAT) was introduced as an alternative to symbolic model checking with BDDs. In this paper we show how bounded model checking can take advantage of specialized optimizations. We present a bounded version of the cone of influence reduction. We have successfully applied this idea in checking safety properties of a PowerPC microprocessor at Motorola 's Somerset PowerPC design center. Based on that experience, we propose a verification methodology that we feel can bring model checking into the mainstream of industrial chip design. 1 Introduction Model checking has only been partially accepted by industry as a supplement to traditional verification techniques. The reason is that model checking, which, to date, has been based on BDDs or on explicit state graph exploration, has not been robust enough for industry. Model checking [3, 12] was first proposed as a verification technique eighteen years ago. However, ...","1999-09-10"
901,358828,"Validating The Ds1 Remote Agent Experiment","Benjamin D. Smith; Bob Kanefsky; Douglas E. Bernard; Edward B. Gamble; Gregory Dorais; James Kurien; Kanna Rajan; Nicola Muscettola; Nicolas Rouquette; P. Pandurang Nayak; William Millar; William Taylor;","This paper describes the validation of the Remote Agent Experiment. A primary goal of this experiment was to provide an onboard demonstration of spacecraft autonomy. This demonstration included both nominal operations with goal-oriented commanding and closed-loop plan execution, and fault protection capabilities with failure diagnosis and recovery, on-board replanning following unrecoverable failures, and system-level fault protection. Other equally important goals of the experiment were to decrease the risk of deploying Remote Agents on future missions and to familiarize the spacecraft engineering community with the Remote Agent approach. These goals were achieved by successfully integrating the Remote Agent with the Deep Space 1 flight software, developing a layered testing approach, and taking various steps to gain the confidence of the spacecraft team. In this paper we describe how we achieved our goals, and discuss the actual on-board demonstration in May, 199...","1999-06-01"
902,360089,"Two Lower Bounds for Branching Programs","Vojtech Rodl;",". The first result concerns branching programs having width (log n) O(1) . We give an OmegaGamma n log n= log log n) lower bound for the size of such branching programs computing almost any symmetric Boolean function and in particular the following explicit function: ""the sum of the input variables is a quadratic residue mod p"" where p is any given prime between n 1=4 and n 1=3 . This is a strengthening of previous nonlinear lower bounds obtained by Chandra, Furst, Lipton and by Pudl'ak. We mention that by iterating our method the result can be further strengthened to OmegaGamma n log n). The second result is a C n lower bound for read-once-only branching programs computing an explicit Boolean function. For n = Gamma v 2 Delta , the function computes the parity of the number of triangles in a graph on v vertices. This improves previous exp(c p n) lower bounds for other graph functions by Wegener and Z'ak. The result implies a linear lower bound for the space comp...","1999-09-06"
903,361140,"CONCURRENCY WITHOUT TOIL a systematic method for parallel program design","E. Pascal Gribomont;",". Formal tools and methods for the design of concurrent programs can be very similar to their sequential counterparts, but nevertheless concurrent programming seems more difficult than sequential programming. Detailed examples in the literature suggest that this particular difficulty originates from interaction problems, when a fine grain of parallelism is required. A systematic technique is proposed to transform a coarse-grained version of a concurrent system into a finer-grained one, through a series of refinements. This technique is illustrated with a classical but still unproved algorithm for mutual exclusion. The incremental development clearly involves two kinds of steps. ""Creative"" transformations appear mainly at the beginning; these steps are short but rather subtle. ""Technical "" transformations are routine steps but involve lengthy formal developments. With a careful separation of creative and technical refinements, developments of concurrent programs become lon...","1996-03-13"
904,361801,"What if Model Checking Must Be Truly Symbolic","Hardi Hungar; Orna Grumberg; Werner Damm;","There are many methodologies whose main concern it is to reduce the complexity of a verification problem to be ultimately able to apply model checking. Here we propose to use a model checking like procedure which operates on a small, truly symbolic description of the model. We do so by exploiting systematically the separation between the (small) control part and the (large) data part of systems which often occurs in practice. By expanding the control part, we get an intermediate description of the system which already allows our symbolic model checking procedure to produce meaningful results but which is still small enough to allow model checking to be performed. 1 Introduction This paper is about a close marriage of two well known verification paradigms: that of model checking and generation of verification conditions . There is no need for reiterating the success story of model checking in the verification of reactive systems originating with the seminal paper by Clarke, Emer...","1995-04-27"
905,361896,"Combining Partial Orders and Symbolic Traversal for Efficient Verification of Asynchronous Circuits","Alexei Semenov; And Alexandre Yakovlev;","Petri nets (PNs) are most adequate for the modelling of the event-triggered behaviour of asynchronous circuits, whose correctness is primarily concerned with freedom from hazards and deadlocks. A recently proposed method for the verification of Petri nets is based on implicit symbolic traversal of the net markings, which often yields better performance than using standard reachability graph analysis. It employs a Binary Decision Diagram (BDD) representation of the boolean functions characterising the state space of the model. This method may, however, suffer from the problem of a bad ordering of the BDD variables. In this paper, we propose an algorithm combining two approaches to PN verification: PN unfolding and BDD-based traversal. We introduce a new application of the PN unfolding method. The results of unfolding construction are used for obtaining the close to optimal ordering of BDD variables. The effect of this combination is demonstrated on a set of benchmarks. The ove...","1997-06-27"
906,362182,"A Compositional Proof System for the Modal µ-Calculus","Colin Stirling; Dr. Henrik; Glynn Winskel; Henrik Reif Andersen; Reif Andersen;","We present a proof system for determining satisfaction between processes in a fairly general process algebra and assertions of the modal -calculus. The proof system is compositional in the structure of processes. It extends earlier work on compositional reasoning within the modal -calculus and combines it with techniques from work on local model checking. The proof system is sound for all processes and complete for a class of finite-state processes. 3 4 1 Introduction The propositional -calculus of Kozen [Kozen, 1983] which was introduced as a powerful extension of propositional dynamic logic has received growing interest as a logic for concurrent systems. This is mainly due to the expressiveness of the logic, which is known to subsume many modal and temporal logics, and the fact that very few operators are needed in achieving this: The logic is an extension of relativized, minimal modal logic K -- also known as Hennessy-Milner logic in the process algebra community -- with minim...","1995-09-08"
907,362459,"Implementation of the Sentry System","Mohamed G. Gouda; Sarah E. Chodrow;","The sentry of a concurrent program P is a program that observes the execution of P, and issues a warning if P does not behave correctly with respect to a given set of logical properties (due to a programming error or a failure). The synchronization between the program and sentry is such that the program never waits for the sentry, the shared storage between them is very small (in fact linear in the number of program variables being observed), and the snapshots read by the sentry are consistent. To satisfy these three requirements, some snapshots may be overwritten by the program before being read by the sentry. We develop a family of algorithms that preserve these requirements for properties involving scalar variables, then extend the algorithms to permit the observation of large data structures without additional overhead. We describe in detail the annotation language with which the properties can be expressed, and a prototype system that we have implemented to generate the ...","1994-06-23"
908,362578,"Predicting Nearly as Well as the Best Pruning of a Decision Tree","David P. Helmbold; Robert E. Schapire;","Many algorithms for inferring a decision tree from data involve a two-phase process: First, a very large decision tree is grown which typically ends up ""over-fitting"" the data. To reduce over-fitting, in the second phase, the tree is pruned using one of a number of available methods. The final tree is then output and used for classification on test data. In this paper, we suggest an alternative approach to the pruning phase. Using a given unpruned decision tree, we present a new method of making predictions on test data, and we prove that our algorithm's performance will not be ""much worse"" (in a precise technical sense) than the predictions made by the best reasonably small pruning of the given decision tree. Thus, our procedure is guaranteed to be competitive (in terms of the quality of its predictions) with any pruning algorithm. We prove that our procedure is very efficient and highly robust. Our method can be viewed as a synthesis of two previously studied techniques. First, we ap...","1995-09-26"
909,362650,"Making the Difference: A Subtraction Operation for Description Logics","Gunnar Teege;","We define a new operation in description logics, the difference operation or subtraction operation. This operation allows to remove from a description as much as possible of the information contained in another description. We define the operation independently of a specific description logic. Then we consider its implementation in several specific logics. Finally we describe practical applications of the operation. 1 INTRODUCTION Description Logics, also called Terminological Logics are a popular formalism for knowledge representation and reasoning. They allow the formation of terms denoting descriptions. A term describes a set of individuals by restricting their properties. Terms are formed using atomic descriptions and a fixed set of term constructors. Usually, the meaning of the terms is given by a denotational semantics. A typical example of a description logic is the concept language of KRIS [Baader and Hollunder, 1992] . In addition to the constructors, a description ...","1994-03-22"
910,362688,"Applying New Scheduling Theory to Static Priority Preemptive Scheduling","A. Burns; A. J. Wellings; K. Tindell; M. Richardson; N. Audsley;","The paper presents exact schedulability analyses for real-time systems scheduled at run-time with a static priority preemptive dispatcher. The tasks to be scheduled are allowed to experience internal blocking (from other tasks with which they share resources) and (with certain restrictions) release jitter --- such as waiting for a message to arrive. The analysis presented is more general than that previously published, and subsumes, for example, techniques based on the Rate Monotonic approach. In addition to presenting the theory, an existing avionics case study is described and analysed. The predictions that follow from this analysis are seen to be in close agreement with the behaviour exhibited during simulation studies. 1. INTRODUCTION One proposed method of building a hard real time system is from a number of periodic and sporadic tasks, and a common way of scheduling such tasks is by using a static priority preemptive scheduler --- at run-time the highest priority runnable task...","1993-07-05"
911,363138,"Building decision procedures for modal logics from propositional decision procedures - The case study of modal K(M)","Fausto Giunchiglia; Istituto Trentino Di Cultura; Roberto Sebastiani;",". The goal of this paper is to propose a new technique for developing decision procedures for propositional modal logics. The basic idea is that propositional modal decision procedures should be developed on top of propositional decision procedures. As a case study, we consider satisfiability in modal K(m), that is modal K with m modalities, and develop an algorithm, called Ksat, on top of an implementation of the Davis-Putnam-Longemann-Loveland procedure. Ksat is thoroughly tested and compared with various procedures and in particular with the state-of-the-art tableau-based system Kris. The experimental results show that Ksat outperforms Kris and the other systems of orders of magnitude, highlight an intrinsic weakness of tableau-based decision procedures, and provide partial evidence of a phase transition phenomenon for K(m). 1 Introduction The goal of this paper is to describe a new technique for developing decision procedures for propositional modal logics. Our approa...","1996-11-22"
912,364091,"Coverage Preserving Reduction Strategies for Reachability Analysis","And Didier Pirottin; Gerard J. Holzmann; Patrice Godefroid;","We study the effect of three new reduction strategies for conventional reachability analysis, as used in automated protocol validation algorithms. The first two strategies are implementations of partial order semantics rules that attempt to minimize the number of execution sequences that need to be explored for a full state space exploration. The third strategy is the implementation of a state compression scheme that attempts to minimize the amount of memory that is used to built a state space. The three strategies are shown to have a potential for substantially improving the performance of a conventional search. The paper discusses the optimal choices for reducing either run time or memory requirements by four to six times. The strategies can readily be combined with each other and with alternative state space reduction techniques such as supertrace or state space caching methods. Keyword Codes: D.1.3; D.2.4 Keywords: Concurrent Programming, Program Verification 1. INTRODU...","1994-12-23"
913,364138,"Checking Signal Transition Graph Implementability by Symbolic BDD Traversal","Alex Kondratyev; Alex Yakovlev; Enric Pastor; Jordi Cortadella; Michael Kishinevsky; Oriol Roig;","This paper defines conditions for a Signal Transition Graph to be implemented by an asynchronous circuit. A hierarchy of the implementability classes is presented. Our main concern is the implementability of the specification under the restricted input-output interface between the design and the environment, i.e., when no additional interface signals are allowed to be added to the design. We develop algorithms and present experimental results of using BDD-traversal for checking STG implementability. These results demonstrate efficiency of the symbolic approach and show a way of improving existing tools for STG-based asynchronous circuit design. 1 Introduction Synthesis frameworks for asynchronous circuits based on STGs (see, e.g., [2, 6]) involve methods for STG analysis and verification. The main problem here is to check if a given STG is implementable by an asynchronous circuit. Although the existing literature defines such conditions (namely, Consistency and Complete State Coding ...","1999-12-17"
914,364140,"Schedule Validation for Embedded Reactive Real-Time Systems","Alberto Sangiovanni-vincentelli; Felice Balarin;","Task scheduling for reactive real time systems is a di#cult problem due to tight constraints that the schedule must satisfy. A static priorityscheme is proposed here that can be formally validated. The method is applicable both for preemptive and non-preemptiveschedules and is conservative in the sense that a valid schedule maybe declared invalid, but no invalid schedule maybe declared valid. Experimental results show that the run time of our validation method is negligible with respect to other steps in system design process, and compares favorably with other methods of schedule validation. 1 Introduction There is no universally accepted formal model for embedded systems, or even a universally accepted de#nition of which systems are considered embedded. In our approach, a system is considered embedded if it has the following characteristics: reactive: We consider systems consisting of many tasks which are executed in reaction to some external events, or to some other ta...","1997-07-04"
915,364218,"Combining Constraint Solving and Symbolic Model Checking for a Class of Systems with Non-linear Constraints","David Notkin; Paul Beame; Richard Anderson; William Chan;",". We extend the conventional BDD-based model checking algorithms to verify systems with non-linear arithmetic constraints. We represent each constraint as a BDD variable, using the information from a constraint solver to prune the BDDs by removing paths that correspond to infeasible constraints. We illustrate our technique with a simple example, which has been analyzed with our prototype implementation. 1 Introduction Although symbolic model checking [BCM + 90] based on Binary Decision Diagrams [Bry86], or BDDs, has been remarkably successful for verifying finite state systems, it fails when complex arithmetic constraints are present. For example, if the bits of the integers x, y and z are represented as BDD variables, the BDD for the non-linear constraint xy = z has exponential size [LS81]. In this paper, we tightly couple a constraint solver with a BDD-based model checker to verify systems with possibly non-linear arithmetic constraints. A large class of embedded, reactive ...","1997-04-24"
916,364303,"Decision Tree Induction Based on Efficient Tree Restructuring","Jeffery A. Clouse; Paul E. Utgoff;",". The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive. Two such approaches are described here, one being incremental tree induction (ITI), and the other being non-incremental tree induction using a measure of tree quality instead of test quality (DMTI). These approaches and several variants offer new computational and classifier characteristics that lend themselves to particular applications. Keywords: decision tree, incremental induction, direct metric, binary test, example incorporation, missing value, tree transposition, installed test, virtual pruning, update cost. 1. Introduction Decision tree induction offers a highly practical method for generalizing from examples whose class membership is known. The most common approach to inducing a decision tree is to partition the labelled examples recursively until a stopping criterion is met. The partition is defined by selectin...","1997-06-20"
917,364543,"Automatic OBDD-based Generation of Universal Plans in Non-Deterministic . . .","A. Cimatti; M. Roveri; P. Traverso;","Most real world environments are non-deterministic. Automatic plan formation in non-deterministic domains is, however, still an open problem. In this paper we present a practical algorithm for the automatic generation of solutions to planning problems in nondeterministic domains. Our approach has the following main features. First, the planner generates Universal Plans, and exploits the compactness of OBDD's (Ordered Binary Decision Diagrams) to express in a practical way plans of extremely large size. Second, the planner generates plans which are guaranteed to achieve the goal in spite of non-determinism, if such plans exist. Otherwise, the planner generates plans which encode iterative trial-and-error strategies (e.g. try to pick up a block until succeed), which are guaranteed to achieve the goal under the assumption that if there is a non-deterministic possibility for the iteration to terminate, this will not be ignored forever. Third, the implementation of the pla...","1998-05-05"
918,364721,"Improving the Efficiency of Supervision by Software Through State Aggregation","R. E. Seviora; T. Savor;","Software supervision is an approach to the realtime detection of failures. A software supervisor is a unit which monitors both the inputs and outputs of a real-time system and reports discrepancies between observed and expected behaviors. The major difficulty with software supervision is the running-time complexity of the supervisor. A significant source of runningtime complexity was determined to be the total number of signals at the inputs queues of specification processes for which no ordering information is known. This paper describes an approach to reduce the running-time complexity of a software supervisor. The approach makes use of a transformed specification in which several state transitions are grouped into a single transition or aggregate state. This effectively reduces the number of signals at the input queues of processes. In this paper, the case where the system under supervision is specified in the ITU Specification and Description Language (SDL) [6] is considered . An ...","1995-05-04"
919,364725,"Sharlit - A Tool for Building Optimizers","null","This paper presents Sharlit, a tool to support the construction of modular and extensible global optimizers. We will show how Sharlit helps in constructing data-flow analyzers and the transformations that use data-flow analysis information: both are major components of any optimizer.","1970-01-01"
920,365109,"Constructing Small Sample Spaces Satisfying Given Constraints","Daphne Koller; Nimrod Megiddo;","Abstract. The subject of this paper is finding small sample spaces for joint distributions of n discrete random variables. Such distributions are often only required to obey a certain limited set of constraints of the form Pr(E) = . We show that the problem of deciding whether there exists any distribution satisfying a given set of constraints is NP-hard. However, if the constraints are consistent, then there exists a distribution satisfying them which is supported by a ""small"" sample space (one whose cardinality is equal to the number of constraints). For the important case of independence constraints, where the constraints have a certain form and are consistent with a joint distribution of n independent random variables, a small sample space can be constructed in polynomial time. This last result is also useful for de-randomizing algorithms. We demonstrate this technique by an application to the problem of finding large independent sets in sparse hypergraphs. Department of Computer ...","1995-09-22"
921,365527,"On the Semantics of Optimization Predicates in CLP languages","Francois Fages;","The Constraint Logic Programming systems which have been implemented include various higherorder predicates for optimization. In CLP(FD) systems, several optimization predicates, such as minimize(G(X),f(X)), minimize-maximum(G(X),[f1(X),...,fn(X)]), are implemented by using branch and bound algorithms. In CLP(R) systems, the Simplex algorithm used for satisfiability checks can also be used for linear optimization through the predicate rmin(f(X)) which adds to the constraints on X the ones defining the space where the linear term f(X) is minimized. These optimization constructs do not belong however to the formal CLP scheme of Jaffar and Lassez, and they lack a declarative semantics. In this paper we propose a general definition for optimization predicates, for which one can provide both a logical and a fixpoint semantics based on Kunen-Fitting's semantics of negation. We show that the branch and bound algorithm can be derived as a refinement of the implementation of the semantics usin...","1993-05-10"
922,365839,"KQML as an Agent Communication Language","Don Mckay; Richard Fritzson; Robin Mcentire; Tim Finin;","This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language #KQML#, a new language and protocol for exchanging information and knowledge. This work is part of a larger e#ort, the ARPA Knowledge Sharing E#ort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML focuses on an extensible set of performatives, which de#nes the permissible #speech acts"" agents may use and comprise a substrate on which to develop higher-level models of interagentinteraction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called communication facilitators which coordinate the interactions of other agents The ideas which underlie the evolving design of KQML are currently being exp...","1998-03-12"
923,366210,"Strong Planning in Non-Deterministic Domains via Model Checking","A. Cimatti; M. Roveri; P. Traverso;","Most real world domains are non-deterministic: the state of the world can be incompletely known, the effect of actions can not be completely foreseen, and the environment can change in unpredictable ways. Automatic plan formation in nondeterministic domains is, however, still an open problem. In this paper we show how to do strong planning in non-deterministic domains, i.e. finding automatically plans which are guaranteed to achieve the goal regardless of non-determinism. We define a notion of planning solution which is guaranteed to achieve the goal independently of non-determinism, a notion of plan including conditionals and iterations, and an automatic decision procedure for strong planning based on model checking techniques. The procedure is correct, complete and returns optimal plans. The work has been implemented in mbp, a planner based on model checking techniques. 1 Introduction A fundamental assumption underlying most of the work in classical planning (see e.g. [6;...","1998-01-27"
924,366858,"Verifying Quantitative Properties of Continuous Probabilistic Timed Automata","And Jeremy Sproston; Marta Kwiatkowska; Roberto Segala;","We consider the problem of automatically verifying real-time systems with continuously distributed random delays. We generalise probabilistic timed automata introduced in [17], an extension of the timed automata model of [4], with clock resets made according to continuous probability distributions. Thus, our model exhibits nondeterministic and probabilistic choice, the latter being made according to both discrete distributions and continuous density functions with nite support. To facilitate algorithmic verication, we modify the standard region graph construction by subdividing the unit intervals in order to approximate the probability to within an interval. We then develop a model checking method for continuous probabilistic timed automata, taking as our specication language Probabilistic Timed Computation Tree Logic (PTCTL). Our method improves on the previously known techniques in that it allows the verication of quantitative probability bounds, as opposed to qualitative prope...","1970-01-01"
925,367658,"Constraint-based Maintenance Scheduling on an Electric Power-Distribution Network","Carles Ferrarons; Jordi Riera; Josep Roca; Llus Ros Giralt; Tom Creemers; Xavier Corbella;",": The exploitation of a power-distribution network involves the scheduling of multiple maintenance and unforeseen repair tasks. The main resource is a network subject to topological, economical and electric constraints. A line section being maintained needs to be isolated from the rest of the network by opening all surrounding switches. This, in turn, would leave other areas of the network de-energized, which is unacceptable in most cases. Hence, these areas have to get their supply via some alternative way, i.e., service needs being restored closing switches connecting to an energized part of the network, taking into account overloading of branches, energy losses, and the cost of the necessary switching operations. In case tasks are carried out in the same area, switching operations might be shared among them. In some cases a valid network reconfiguration might not even exist. Finally, typical scheduling constraints have to be met: resources of limited availability (manpo...","1997-01-05"
926,368281,"Decision Theoretic Planning: Structural Assumptions and Computational Leverage","Craig Boutilier; Steve Hanks; Thomas Dean;","Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these fields often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory. This paper presents an overview and synthesis of MDP-related methods showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to des...","1999-04-28"
927,368718,"Symbolic Model Checking using SAT procedures instead of BDDs","A. Biere; A. Cimatti; E. M. Clarke; M. Fujita; Y. Zhu;","In this paper, we study the application of propositional decision procedures in hardware verification. In particular, we apply bounded model checking, as introduced in [1], to equivalence and invariant checking. We present several optimizations that reduce the size of generated propositional formulas. In many instances, our SAT-based approach can significantly outperform BDD-based approaches. We observe that SAT-based techniques are particularly efficient in detecting errors in both combinational and sequential designs. 1 Introduction A complex hardware design can be error-prone and mistakes are costly. Formal verification techniques such as symbolic model checking are gaining wide industrial acceptance. Compared to traditional validation techniques based on simulation, they provide more extensive coverage and can detect subtle errors. Representing and manipulating boolean expressions is critical to many formal verification techniques. BDDs [3] have traditionally been used for this p...","1999-09-10"
928,371317,"Applying GSAT To Non-Clausal Formulas","R. Sebastiani; Roberto Sebastiani;","In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a function which computes the number of clauses of the CNF conversion of a formula which are false under a certain truth assignment, without constructing the conversion itself. The proposed methodology applies to most variants of GSAT. 1 The goal Many algorithms have been devised to solve the well-known NP-Complete SAT problem [Coo71, GJ79]. Among them, GSAT (Greedy SAT) [SLM92, SK93], an incomplete search algorithm for SAT, is now state of the art. Though incomplete, GSAT has been shown to be generally much faster than the traditional complete Davis-Putnam algorithm [DP60]. Like most SAT algorithms, GSAT requires that the input formula be in clausal form. A possible way to find a truth assignment for a non-clausal formula ' is (i) to convert ' into clausal form, and then (ii) to apply GSAT to the converted formula. Step (i) causes an extra amount of computation time, wh...","1994-03-14"
929,371548,"Model Checking and Abstraction","David E. Long; Edmund M. Clarke;","MODELS In this section, we consider the problem of deriving an approximate abstract model of M directly from a finite-state program describing M . The actual process will be described in subsection 4.2. However, we would like this discussion to be relatively independent of the particular finite-state language used. To accomplish this, we are going to argue that a program in a finite-state language can be transformed into relational expressions I and R that can be evaluated to obtain the initial states I and the transition relation R of the transition system M represented by the program. These relational expressions are simply formulas in first-order predicate logic that will built up from a set of primitive relations for the basic operators and constants in the language. Then in subsection 4.2, we will show how to manipulate I and R to obtain the approximation to M . There will typically be types associated with the variables and relation arguments in the relational expressions that w...","1995-12-14"
930,372221,"Nitpick: A Checkable Specification Language","Daniel Jackson;","Nitpick is a formal specification language designed to be amenable to a new checking method called case enumeration. The paper explains how a compromise was reached in the language's design between expressive power and tractability. Fragments of a larger specification we have written are given to illustrate the language's features. Simplified versions of two operations are explained in detail, along with the results of running the Nitpick checker to detect some flaws. 1 Introduction Nitpick is a language and a checking tool for investigating the intended function of a software system. The language has much in common with Z [1] ; it is based on sets and relations, and represents operations with logical formulae. But Nitpick's motivation differs markedly from Z's. Completeness has been the yardstick of much work in formal specification, both in description and analysis. Formal specifications, it was once thought, might alleviate the problems of poor contracts by filling in details mis...","1995-12-18"
931,372808,"Remembrance of Things Past: Locality and Memory in BDDs","Dirk Grunwald; Fabio Somenzi; Srilatha Manne;","Binary Decision Diagrams #BDDs# are e#cientatmanipulating large sets in a compact manner. BDDs, however, are inef- #cientatutilizing thememory hierarchyofthe computer. Recentwork addresses this problem bymanipulating the BDDs in breath-#rst manner #BFS#. BFS processing is quitesuccessful atreducingthenumber of page faults when the BDDs do not #t in theavailable physical memory.When paging does not take place, it is much less clear which paradigm leads tothebetter performance. In this paper, we perform a detailed analysis of BFS and DFS packages usingsimulation and direct performance monitoringofthememory hierarchy. Weshowthatthere is very little di#erence in TLB and cache miss rates for DFS and BFS paradigms. We also showthat di#erences in execution timebetween carefully tuned BFS and DFS implementations are primarily a function of the lossless computed table used in BFS implementations, and not a function of memory locality.Furthermore, we present implementation changes tothetheCudd ...","1997-07-04"
932,373307,"An Efficient Generation of the Timed Reachability Graph for the Analysis of Real-Time Systems","Inhye Kang; Insup Lee;","As computers become ubiquitous, they are increasingly used in safety critical environments. Since many safety critical applications are real-time systems, automated analysis technique of real-time properties is desirable. Most widely used automated analysis techniques are based on state space exploration. Automatic analysis techniques based on state space exploration suffer from the state space explosion problem. In particular, a real-time system may have an unbounded number of states due to infinitely many possible time values. This paper presents our approach for generating a finite and efficient representation of the reachable states called a timed reachability graph for a real-time system. In this paper, a real-time system is specified using a timed automaton which is a timed extension of the well-known finite automaton. Our approach for coping with the state explosion problem is to extract timing information from states and to represent it as relative time relations bet...","1996-03-07"
933,374683,"Records for Logic Programming","Gert Smolka; Ralf Treinen;","CFT is a new constraint system providing records as logical data structure for constraint (logic) programming. It can be seen as a generalization of the rational tree system employed in Prolog II, where finer-grained constraints are used, and where subtrees are identified by keywords rather than by position. CFT is defined by a first-order structure consisting of so-called feature trees. Feature trees generalize the ordinary trees corresponding to first-order terms by having their edges labeled with field names called features. The mathematical semantics given by the feature tree structure is complemented with a logical semantics given by five axiom schemes, which we conjecture to comprise a complete axiomatization of the feature tree structure. We present a decision method for CFT, which decides entailment / disentailment between possibly existentially quantified constraints. Since CFT satisfies the independence property, our decision method can also be employed for checking the sat...","1996-12-10"
934,375002,"Minimizing Conflicts: A Heuristic Repair Method for Constraint-Satisfaction and Scheduling Problems","Andy Philips; Mark D. Johnston; Philip Laird; Steven Minton;","This paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. We demonstrate empirically that on the n- queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. We also describe a scheduling application where the approach has been used successfully. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be most effective. 1. Introduction One of the most promising general approaches for solving...","2000-09-27"
935,375054,"Visual Programming Languages and the Empirical Evidence For and Against","K. N. Whitley;","The past decade has witnessed the emergence of an active visual programming research community.Yet, there has also been a noteworthy shortage of empirical evidence supporting the resulting research. This paper summarizes empirical data relevant to visual programming languages, both to show the current empirical status and to act as a call to arms for further empirical work. 1 Introduction The past decade has witnessed the emergence of an active visual programming research community whose e#orts have yielded many visual programming languages #VPLs# and visualization systems. To date, there has also been a shortage of empirical studies backing the design decisions in these VPLs and visualization systems. De#nite negative consequences stem from this lack of evidence. For example, critics and the larger computer science community are more inclined to dismiss visual programming as an unpromising research fad. In addition, visual programming researchers are forced to make design choice...","2000-09-29"
936,375670,"Living with CLASSIC: When and How to Use a KL-ONE-Like Language","Alexander Borgida; Deborah L. Mcguinness; Lori Alperin Resnick; Peter F. Patel-schneider; Ronald J. Brachman;","classic is a recently-developed knowledge representation system that follows the paradigm originally set out in the kl-one system: it concentrates on the definition of structured concepts, their organization into taxonomies, the creation and manipulation of individual instances of such concepts, and the key inferences of subsumption and classification. Rather than simply presenting a description of classic, we complement a brief system overview with a discussion of how to live within the confines of a limited object-oriented deductive system. By analyzing the representational strengths and weaknesses of classic, we consider the circumstances under which it is most appropriate to use (or not use) it. We elaborate a knowledge-engineering methodology for building kl-one-style knowledge bases, with emphasis on the modeling choices that arise in the process of describing a domain. We also address some of the key difficult issues encountered by new users, including primitive vs. d...","2000-10-20"
937,376086,"Approximation and Decomposition of Binary Decision Diagrams","Fabio Somenzi; Kavita Ravi; Kenneth L. Mcmillan; Thomas R. Shiple;","Efficient techniques for the manipulation of Binary Decision Diagrams (BDDs) are key to the success of formal verification tools. Recent advances in reachability analysis and model checking algorithms have emphasized the need for efficient algorithms for the approximation and decomposition of BDDs. In this paper we present a new algorithm for approximation and analyze its performance in comparison with existing techniques. We also introduce a new decomposition algorithm that produces balanced partitions. The effectiveness of our contributions is demonstrated by improved results in reachability analysis for some hard problem instances. 1 Introduction Symbolic state enumeration techniques based on Binary Decision Diagrams (BDDs [2]) have revolutionized formal verification [8, 4, 17, 1, 14]. They have two key features that make them suitable to the exploration of very large state graphs: They represent sets compactly, and they avoid explicit enumeration in image computation. Given the t...","1998-08-24"
938,377337,"Fast Exact Minimization of BDDs","Nicole Drechsler; Rolf Drechsler; Wolfgang Gunther;","We present a new exact algorithm for #nding the optimal variable ordering for reduced ordered Binary Decision Diagrams #BDDs#. The algorithm makes use of a lower bound technique known from VLSI design. Up to now this technique has been used only for theoretical considerations and it is adapted here for our purpose. Furthermore, the algorithm supports symmetry aspects and makes use of a hashing based data structure. Experimental results are given to demonstrate the e#ciency of our approach. We succeeded in minimizing adder functions with up to 64 variables, while all other previously presented approaches fail. 1 Introduction Recently, several design methods have been proposed that are based on ordered Binary Decision Diagrams #BDDs# #7#. The resulting circuits have very nice properties, like e.g. testability #2, 1# and lowpower #17#. For synthesis approaches based on Pass Transistor Logic #PTL# BDDs seem to be a good starting point. First promising results on how to transform a decisi...","1998-08-24"
939,378161,"Disjunctive Partitioning and Partial Iterative Squaring: an effective approach for symbolic traversal of large circuits","Gianpiero Cabodi; Luciano Lavagno; Paolo Camurati; Politecnico Di Torino; Stefano Quer;","Extending the applicability of reachability analysis to large and real circuits is a key issue. In fact they are still limited for the following reasons: peak BDD size during image computation, BDD explosion for representing state sets and very high sequential depth. Following the promising trend of partitioning and problem decomposition, we present a new approach based on a disjunctive partitionedtransition relation and on an improved iterative squaring. In this approach a Finite State Machine is decomposed and traversed one ""functioning--mode"" at a time by means of the ""disjunctive"" partitioned approach. The overall algorithm aims at lowering the intermediate peak BDD size pushing further reachability analysis. Experiments on a few industrial circuits containing counters and on some large benchmarks show the feasibility of the approach. 1 Introduction State-of-the-art approaches for state space exploration of Finite State Machines (FSMs) exploit symbolic techniques based on Binar...","1997-07-04"
940,378200,"Magic Templates: A Spellbinding Approach to Logic Programs","Raghu Ramakrishnan;","We consider a bottom-up query-evaluation scheme in which facts of relations are allowed to have nonground terms. The Magic Sets query-rewriting technique is generalized to allow arguments of predicates to be treated as bound even though the rules do not provide ground bindings for those arguments. In particular, we regard as ""bound"" any argument containing a function symbol or a variable that appears more than once in the argument list. Generalized ""magic "" predicates are thus defined to compute the set of all goals reached in a top-down exploration of the rules, starting from a given query goal; these goals are not facts of constants as in previous versions of the Magic Sets algorithm. The magic predicates are then used to restrict a bottom-up evaluation of the rules so that there are no redundant actions; that is, every step of the bottom-up computation must be performed by any algorithm that uses the same sideways information passing strategy (sips). The price paid, compared to prev...","1994-10-03"
941,378812,"Equivalence Checking Using Cuts and Heaps","Andreas Kuehlmann; Florian Krohm;","This paper presents a veri#cation technique whichis speci#cally targeted to formally comparing large combinational circuits with some structural similarities. The approach combines the application of BDDs with circuit graph hashing, automatic insertion of multiple cut frontiers, and a controlled elimination of false negativeveri#cation results caused by the cuts. Two ideas fundamentally distinguish the presented technique from previous approaches. First, originating from the cut frontiers, multiple BDDs are computed for the internal nets of the circuit, and second, the BDD propagation is prioritized by size and discontinued once a given limit is exceeded. 1 Introduction In recentyears, formal techniques have become widely accepted in practical design methodologies to verify properties of complex systems. The computational complexity of the corresponding algorithms results in a fundamental trade-o# between the generality of the veri#cation model and the size of the designs that can b...","1997-07-04"
942,379396,"Structural Methods for the Synthesis of Speed-Independent Circuits","Alex Kondratyev; Jordi Cortadella;","Most existing tools for the synthesis of asynchronouscircuits from Signal Transition Graphs (STGs) derive the reachability graph for the calculation of logic equations. This paper presents novel methods exclusively based on the structural analysis of the underlying Petri net. This methodology can be applied to any STG that can be covered by State Machines and, in particular, to all live and safe free-choice STGs. Significant improvements with regard to existing structural methods are provided. The new techniques have been implemented in an experimental tool that has been able to synthesize specificationswith over10 27 markings, some of them being non-free choice. 1 Introduction Petri nets (PNs) are a powerful formalism to model concurrent systems. As a model, their most interesting feature is the capability of implicitly describing a vast state space by a succinct representation, which gracefully captures the notions of causality, concurrency and conflict between events. Petri nets...","1997-04-24"
943,380059,"An Implementation of an Efficient Algorithm for Bisimulation Equivalence","Jean-claude Fernandez;","We present an efficient algorithm for bisimulation equivalence. Generally, bisimulation equivalence can be tested in O(mn) for a labeled transition system with m transitions and n states. In order to come up with a more efficient algorithm, we establish a relationship between bisimulation equivalence and the relational coarsest partition problem, solved by Paige & Tarjan in O(m log n) time. Given an initial partition and a binary relation, the problem is to find the coarsest partition compatible with them. Computing bisimulation equivalence can be viewed both as an instance and as a generalization of this problem: an instance, because only the universal partition is considered as an initial partition and a generalization since we want to find a partition compatible with a family of binary relations instead of one single binary relation. We describe how we have adapted the Paige & Tarjan algorithm of complexity O(m log n) to minimize labeled transition systems modulo bisimulation equiva...","2000-04-06"
944,380922,"Incremental CTL Model Checking Using BDD Subsetting","Abelardo Pardo; Gary D. Hachtel;","An automatic abstraction#re#nement algorithm for symbolic CTL model checking is presented. Conservative model checking is thus done for the full CTL language#no restriction is made to the universal or existential fragments. The algorithm begins with conservative veri#cation of an initial abstraction. If the conclusion is negative, it derives a #goal set"" of states which require further resolution. It then successively re#nes, with respect to this goal set, the approximations made in the sub-formulas, until the given formula is veri#ed or computational resources are exhausted. This method applies uniformly to the abstractions based in over-approximation as well as under-approximations of the model. Both the re#nement and the abstraction procedures are based in BDD-subsetting. Note that re#nement procedures which are based on error traces, are limited to overapproximation on the universal fragment # or for language containment#, whereas the goal set method is applicable to all consistent...","1998-08-24"
945,382179,"Modular State Space Analysis of Coloured Petri Nets","Laure Petrucci; Sren Christensen;",". State Space Analysis is one of the most developed analysis methods for Petri Nets. The main problem of state space analysis is the size of the state spaces. Several ways to reduce it have been proposed but cannot yet handle industrial size systems. Large models often consist of a set of modules. Local properties of each module can be checked separately, before checking the validity of the entire system. We want to avoid the construction of a single state space of the entire system. When considering transition sharing, the behaviour of the total system can be captured by the state spaces of modules combined with a Synchronisation Graph. To verify that we do not lose information we show how the full state space can be constructed. We show how it is possible to determine usual Petri Nets properties, without unfolding to the ordinary state space. 1 Introduction State spaces, also called Occurrence Graphs or Reachability Graphs, grow exponentially with respect to the number of...","1999-03-02"
946,384027,"Weak Alternating Automata Are Not That Weak","Moshe Y. Vardi;","Automata on infinit words are used for specification... In this paper we describe a quadratic translation, which circumvents the need for determinization, of Büchi and co-Büchi alternating automata to weak alternating automata. Beyond the independent interest of such a translation, it gives rise to a simple complementation algorithm for nondeterministic Büchi automata.","2000-11-21"
947,384037,"Strong Cyclic Planning Revisited","Marco Daniele; Moshe Y. Vardi; Paolo Traverso;",". Several realistic non-deterministic planning domains require plans that encode iterative trial-and-error strategies, e.g., ""pick up a block until succeed"". In such domains, a certain effect (e.g., action success) might never be guaranteed a priori of execution and, in principle, iterative plans might loop forever. Here, the planner should generate iterative plans whose executions always have a possibility of terminating and, when they do, they are guaranteed to achieve the goal. In this paper, we define the notion of strong cyclic plan, which formalizes in temporal logic the above informal requirements for iterative plans, define a planning algorithm based on model-checking techniques, and prove that the algorithm is guaranteed to return strong cyclic plans when they exist or to terminate with failure when they do not. We show how this approach can be extended to formalize plans that are guaranteed to achieve the goal and do not involve iterations (strong plans) and plan...","2000-12-04"
948,384587,"Reinforcement Learning I: Introduction","null","Introduction Richard S. Sutton and Andrew G. Barto c fl All rights reserved [In which we try to give a basic intuitive sense of what reinforcement learning is and how it differs and relates to other fields, e.g., supervised learning and neural networks, genetic algorithms and artificial life, control theory. Intuitively, RL is trial and error (variation and selection, search) plus learning (association, memory). We argue that RL is the only field that seriously addresses the special features of the problem of learning from interaction to achieve long-term goals.] 1 Learning from Interaction The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of infor","1996-12-10"
949,385215,"Coloured Petri Nets Extended with Channels for Synchronous Communication","Niels Damgaard Hansen; Sren Christensen;","This paper shows how Coloured Petri Nets (CP-nets) can be extended to support synchronous communication. We introduce coloured communication channels through which transitions are allowed to communicate complex values. Small examples show how channel communication is convenient for creating compact and comprehensive models. The concepts introduced in this paper originate from the practical use of Petri nets for modelling, and they are formally defined in such a way that they preserve the basic properties of CP-nets. We show how a CP-net with channels can be transformed into a behaviourally equivalent CP-net. This allows us to deduce properties of CP-nets with channels from well-known properties of CP-nets. As an example, we extend the concept of place invariants to cope with CP-nets with channels and show how place invariants can be found. This is done without transforming the CP-nets with channels into their equivalent CP-nets. The reader is assumed to be familiar with th...","1999-04-14"
950,385987,"Debugging Constraint Programs","Ecrc- Ecrc; Micha Meier;","Constraint programming (CP) is in its substance non-algorithmic programming, not last because it is often being applied to problems for which no efficient algorithms exist. A not immediately obvious consequence of this fact is that debugging CP programs is principally different from debugging algorithmic programs, including imperative, functional or Prolog programs. It is also more difficult. Moreover, it is frequently necessary to apply performance debugging to CP programs, which are correct but too slow to be feasible. The whole area of CP debugging is still lacking both methodology and tools to support users in improving their programs. In this paper, we present a paradigm for tracing constraint programs and the design and implementation of Grace, a graphical environment for tracing CLP(FD) programs on top of ECL i PS e . III 1 Introduction Developing CLP applications is a difficult task. This is to a large extent due to the fact that CLP is usually being applied to combinator...","2000-11-28"
951,386081,"Verification of Temporal Properties of Concurrent Systems","Dansk Sammenfatning;","This thesis is concerned with the verification of concurrent systems. It provides methods and techniques for reasoning about temporal properties as described by assertions from an expressive modal logic - the modal ""-calculus. It describes a compositional approach to verifying whether processes satisfy assertions from the logic where processes are drawn from a process language encompassing CCS, CSP and related process languages. This compositional approach is based on the notion of a reduction which transforms a satisfaction problem for a composite process into satisfaction problems for the subcomponents. Although the modal ""-calculus is very expressive from a theoretical point of view, it leaves much to be desired in practical applications. Hence, we introduce an extended version of the modal ""-calculus which is more convenient for expressing properties. Among other things it allows for a compact representation of assertions by simultaneous fixed-points. As a side-effect it provides, using the compositional method, a means for constructing efficient local and global model checkers for automatically dec...","2000-09-12"
952,386741,"The Mutual Exclusion Problem Part II: Statement and Solutions","L. Lamport;","The theory developed in Part I is used to state the mutual exclusion problem and several additional fairness and failure-tolerance requirements. Four ""distributed "" N-process solutions are given, ranging from a solution requiring only one communication bit per process that permits individual starvation, to one requiring about N ! communication bits per process that satisfies every reasonable fairness and failure-tolerance requirement that we can conceive of. Contents 1 Introduction 3 2 The Problem 4 2.1 Basic Requirements . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Fairness Requirements . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Premature Termination . . . . . . . . . . . . . . . . . . . . . 8 2.4 Failure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 The Solutions 14 3.1 The Mutual Exclusion Protocol . . . . . . . . . . . . . . . . . 15 3.2 The One-Bit Solution . . . . . . . . . . . . . . . . . . . . . . 17 3.3 A Digression . . . . . . . . . . . ...","2000-06-26"
953,386841,"Mechanical Verification of Concurrent Systems with TLA","Leslie Lamport; Peter Grnning; Urban Engberg;",". We describe an initial version of a system for mechanically checking the correctness proof of a concurrent system. Input to the system consists of the correctness properties, expressed in TLA (the temporal logic of actions), and their proofs, written in a humanly readable, hierarchically structured form. The system uses a mechanical verifier to check each step of the proof, translating the step's assertion into a theorem in the verifier's logic and its proof into instructions for the verifier. Checking is now done by LP (the Larch Prover), using two di#erent translations---one for action reasoning and one for temporal reasoning. The use of additional mechanical verifiers is planned. Our immediate goal is a practical system for mechanically checking proofs of behavioral properties of a concurrent system; we assume ordinary properties of the data structures used by the system. 1 Introduction TLA, the Temporal Logic of Actions, is a logic for specifying and reasoning about concurrent s...","2000-06-27"
954,387834,"Conformant Planning via Model Checking","Alessandro Cimatti; And Marco Roveri;",". Conformant planning is the problem of nding a sequence of actions that is guaranteed to achieve the goal for any possible initial state and nondeterministic behavior of the planning domain. In this paper we present a new approach to conformant planning. We propose an algorithm that returns the set of all conformant plans of minimal length if the problem admits a solution, otherwise it returns with failure. Our work is based on the planning via model checking paradigm, and relies on symbolic techniques such as Binary Decision Diagrams to compactly represent and eciently analyze the planning domain. The algorithm, called cmbp, has been implemented in the mbp planner. cmbp is strictly more expressive than the state of the art conformant planner cgp. Furthermore, an experimental evaluation suggests that cmbp is able to deal with uncertainties more eciently than cgp. 1 Introduction The planning via model checking [5, 8, 7, 9] paradigm is based on the interpretation of a pla...","1999-08-10"
955,389383,"NUSMV: a new Symbolic Model Verifier","A. Cimatti; E. Clarke; F. Giunchiglia;","This paper describes NUSMV, a new symbolic model checker developed as a joint project between Carnegie Mellon University (CMU) and Istituto per la Ricerca Scientifica e Tecnolgica (IRST). NUSMV is designed to be a well structured, open, flexible and documented platform for model checking. In order to make NUSMV applicable in technology transfer projects, it was designed to be very robust, close to the standards required by industry, and to allow for expressive specification languages.","1999-03-19"
956,389521,"Planning as Heuristic Search: New Results","Blai Bonet; Depto De Computaci'on;",". In the recent AIPS98 Planning Competition, the hsp planner, based on a forward state search and a suitable domain-independent heuristic, showed that heuristic search planners can be competitive with state of the art Graphplan and Satisfiability planners. hsp solved more problems than the other planners but often took more time or produced longer plans. The bottleneck in hsp is the computation of the heuristic for every new state. In this paper, we reformulate hsp so that this problem is avoided. The new planner, that we call hsp-r, is based on the same ideas and heuristic as hsp, but searches backward from the goal rather than forward from the initial state. We show that this change allows hsp-r to compute the heuristic only once. As a result, hsp-r can produce better plans, often in less time. For example, hsp-r solves each of the 30 logistics problems from Kautz and Selman in less than 3 seconds. This is two orders of magnitude faster than blackbox. At the same time, ...","1999-07-09"
957,389559,"A Fast Mutual Exclusion Algorithm","null","A new solution to the mutual exclusion problem is presented that, in the absence of contention, requires only seven memory accesses. It assumes atomic reads and atomic writes to shared registers. Capsule Review To build a useful computing system from a collection of processors that communicate by sharing memory, but lack any atomic operation more complex than a memory read or write, it is necessary to implement mutual exclusion using only these operations. Solutions to this problem have been known for twenty years, but they are linear in the number of processors. Lamport presents a new algorithm which takes constant time (five writes and two reads) in the absence of contention, which is the normal case. To achieve this performance it sacrifices fairness, which is probably unimportant in practical applications. The paper gives an informal argument that the algorithm's performance in the absence of contention is optimal, and a fairly formal proof of safety and freedom from deadlock, u...","2000-06-26"
958,389869,"Verification of a Multiplier: 64 Bits and Beyond","And Leslie Lamport; Leslie Lamport; R. P. Kurshan;",". Verifying a 64-bit multiplier has a computational complexity that puts it beyond the grasp of current finite-state algorithms, including those based upon homomorphic reduction, the induction principle, and bdd fixed-point algorithms. Theorem proving, while not bound by the same computational constraints, may not be feasible for routinely coping with the complex, low-level details of a real multiplier. We show how to verify such a multiplier by applying COSPAN, a model-checking algorithm, to verify local properties of the complex low-level circuit, and using TLP, a theorem prover based on the Temporal Logic of Actions, to prove that these properties imply the correctness of the multiplier. Both verification steps are automated, and we plan to mechanize the translation between the languages of TLP and COSPAN. 1 Introduction For finite-state systems, it is in principle possible to use model checking to verify properties of a system automatically, with little human intervention. However...","2000-06-27"
959,390163,"Abstraction in Planning via Model Checking","A. Cimatti; F. Giunchiglia; M. Roveri;","Planning via Model Checking is a novel approach to planning. It is based on the reformulation of a planning problem to the exploration of a finite state automaton. Automata exploration can be efficiently performed with Model Checking, a formal verification technique widely applied in design of industrial systems. For very large domains, however, a state explosion can arise. Abstractions can be used to tackle the problem. In this paper we show how Abstraction techniques developed in classical planning can be lifted to the Planning via Model Checking paradigm. Introduction Planning via Model Checking is a novel approach to planning (Cimatti et al. 1997). It is based on the reformulation of a planning problem to the exploration of a finite state automaton. The advantages of this approach are many-fold. First, the domain can be described with an expressive language, allowing for the expression of ramifications, constraints and nondeterministic actions. Second, very efficient mo...","1998-08-13"
960,390309,"Bisimulation Algorithms for Stochastic Process Algebras and their BDD-based Implementation","And Markus Siegle; Holger Hermanns;",". Stochastic process algebras have been introduced in order to enable compositional performance analysis. The size of the state space is a limiting factor, especially if the system consists of many cooperating components. To fight state space explosion, various proposals for compositional aggregation have been made. They rely on minimisation with respect to a congruence relation. This paper addresses the computational complexity of minimisation algorithms and explains how efficient, BDD-based data structures can be employed for this purpose. 1 Introduction Compositional application of stochastic process algebras (SPA) is particularly successful if the system structure can be exploited during Markov chain generation. For this purpose, congruence relations have been developed which justify minimisation of components without touching behavioural properties. Examples of such relations are strong equivalence [22], (strong and weak) Markovian bisimilarity [16] and extended Markovian bisimil...","1970-01-01"
961,390637,"Fast Training Algorithms For Multi-Layer Neural Nets","Richard P. Brent;","Training a multilayer neural net by back-propagation is slow and requires arbitrary choices regarding the number of hidden units and layers. This paper describes an algorithm which is much faster than back-propagation and for which it is not necessary to specify the number of hidden units in advance. The relationship with other fast pattern recognition algorithms, such as algorithms based on k-d trees, is mentioned. The algorithm has been implemented and tested on articial problems such as the parity problem and on real problems arising in speech recognition. Experimental results, including training times and recognition accuracy, are given. Generally, the algorithm achieves accuracy as good as or better than nets trained using back-propagation, and the training process is much faster than back-propagation. Accuracy is comparable to that for the  earest neighbour"" algorithm, which is slower and requires more storage space. Comments Only the Abstract is given here. The full paper ap...","2000-09-23"
962,391828,"A Temporal Logic of Actions","null","In 1977, Pnueli introduced to computer scientists a temporal logic for reasoning about concurrent programs. His logic was simple and elegant, based on the single temporal modality ""forever"", but it was not expressive enough to completely describe programs. Since then, a plethora of more expressive logics have been proposed, all with additional temporal modalities such as ""next"", ""until"", and ""since"". Here, a temporal logic is introduced based only on Pnueli's original modality ""forever"", but with predicates (assertions about a single state) generalized to actions---assertions about pairs of states. This logic has all the expressive power needed to describe and reason about concurrent programs. Much of the temporal reasoning required with other logics is replaced by nontemporal reasoning about actions. vi Perspective by Kevin D. Jones It is generally accepted by the software engineering community that some means of formally specifying software is an important tool in increasing con...","2000-06-27"
963,392298,"HyTech: A Model Checker for Hybrid Systems","Howard Wong-toi; Pei-hsin Ho; Thomas A. Henzinger;","A hybrid system is a dynamical system whose behavior exhibits both discrete and continuous change. A hybrid automaton is a mathematical model for hybrid systems, which combines, in a single formalism, automaton transitions for capturing discrete change with differential equations for capturing continuous change. HyTech is a symbolic model checker for linear hybrid automata, a subclass of hybrid automata that can be analyzed automatically by computing with polyhedral state sets. A key feature of HyTech is its ability to perform parametric analysis, i.e. to determine the values of design parameters for which a linear hybrid automaton satisfies a temporal-logic requirement.","1998-03-18"
964,392541,"An Application Oriented Real-Time Algebra","Adrian Robson; David Kendall; Steven Bradley; William Henderson;","Many attempts have been made to define timed process algebras as a route to formal reasoning about real-time systems. In this paper we argue that existing timed process algebras unsuccessfully try to address all of the aspects which their untimed counterparts do --- specification, design and modelling --- where they would be more useful if they were restricted to one of these roles. Drawing on this, an Application Oriented Real-Time Algebra (AORTA) is introduced, which has special features making it suitable for the design of real-time systems which may need to be formally verified. Keywords: real-time, formal methods, process algebra 1 Introduction Process algebras such as CCS [1], CSP [2] and LOTOS [3] have proved themselves to be useful tools in the formal specification and verification of concurrent communicating systems. One of the reasons for their success is their broad spectrum of uses, from specification of system behaviour to high-level system modelling to parallel...","1998-07-20"
965,393734,"Deductive Modelchecking","null","interpretation of reactive systems. ACM Transactions on Programming Languages and Systems, 19(2):253--291, 1997. [DGH95] Werner Damm, Orna Grumberg, and Hardi Hungar. What if model checking must be truly symbolic. In First Intl. Workshop on Tools and Algorithms for the Construction and Analysis of Systems (TACAS 95), volume 1019 of LNCS, pages 230--244. Springer-Verlag, May 1995. [Eme90] E. Allen Emerson. Temporal and modal logic. In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, volume B, pages 995--1072. Elsevier Science Publishers (North-Holland), 1990. [FG96] Limor Fix and Orna Grumberg. Verification of temporal properties. J. Logic and Computation, 6(3):343--362, 1996. [GS97] Susanne Graf and Hassen Saidi. Construction of abstract state graphs with PVS. In Orna Grumberg, editor, Proc. 9 th Intl. Conference on Computer Aided Verification, volume 1254 of LNCS, pages 72--83. Springer-Verlag, June 1997. [Hun93] Hardi Hungar. Combining model checking and theor...","1999-04-22"
966,394586,"Supervisory Control of a Rapid Thermal Multiprocessor","Silvano Balemi;",": An application of supervisory control theory to a semiconductor manufacturing piece of equipment is presented. This approach allows the flexible design and update of processing ""recipes"" to accommodate frequently changing manufacturing requirements. An input/output interpretation of supervisory control theory is given. This interpretation leads to a generic implementation scheme for manufacturing systems. A synthesis fixpoint algorithm implementation using binary decision diagrams enables the design of supervisors of realistic size. A sample synthesis for an oxide growth recipe is performed on a state space of the order of 10 6 states. The actual implementation of the logic control software for the application under investigation is described. Keywords: Supervisory control theory, discrete event systems, binary decision diagrams, control of manufacturing systems, semiconductor manufacturing, rapid thermal multiprocessing. 1 Introduction The supervisory control theory introduced ...","1997-03-14"
967,394952,"The Concurrency Workbench: A Semantics Based Tool for the Verification of . . .","null","rling, Scotland, 1988. [43] Milner, R. Communication and Concurrency. Prentice Hall 1989. [44] Paige, R. and Tarjan, R.E. ""Three Partition Refinement Algorithms."" SIAM Journal of Computing, v. 16, n. 6, pp. 973--989, December 1987. [45] Parrow, J. ""Submodule Construction as Equation Solving in CCS."" Theoretical Computer Science, v. 68, pp. 175--202, 1989. [46] Parrow, J. ""Verifying a CSMA/CD-Protocol with CCS."" In Proceeding of the Eighth IFIP Symposium on Protocol Specification, Testing, and Verification, pp. 373--387. North-Holland, Amsterdam, 1988. [47] Richier, J., Rodriguez, C., Sifakis, J. and Voiron, J.. ""Verification in XESAR of the Sliding Window Protocol."" In Proceedings of the Seventh IFIP Symposium on Protocol Specification, Testing, and Verification. North-Holland, Amsterdam, 1987. [48] Roy, V. and de Simone, R. ""Auto/Autograph."" In Computer-Aided Verification '90, pp. 477--491.","1995-05-31"
968,395364,"Verification of Real-Time Systems by Successive Over and Under Approximation","And Howard Wong-toi; David L. Dill;",". Automata-theoretic techniques provide a powerful means of verifying finite-bounded systems. However they suffer from state explosion. One approach to this problem is to analyze an abstracted system description. In safety verification, different abstractions can yield true lower or upper approximations of the set of reachable states. Previously, we presented an algorithm [24] which uses simple heuristics to automatically generate finer abstractions until the approximations correctly answer the verification problem. In this paper, we extend the algorithm to include approximations over the system's transition structure as well as its state space. In terms of real-time system verification, this enables us to efficiently verify properties of the full class of timed safety automata augmented with urgency semantics. We describe a case study involving some bounded liveness properties of the MAC sublayer of the Ethernet protocol, based on a description by Weinberg and Zuck. This ...","1997-03-14"
969,396568,"The Existence of Refinement Mappings","Leslie Lamport; Martn Abadi;","Re#nement mappings are used to prove that a lower#level speci#cation cor# rectly implements a higher#level one. We consider speci#cations consisting of a state machine #which may be in#nite#state# that speci#es safety re# quirements# and an arbitrary supplementary property that speci#es liveness requirements. A re#nement mapping from a lower#level speci#cation S 1 to a higher#level one S 2 is a mapping from S 1 #s state space to S 2 #s state space. It maps steps of S 1 #s state machine to steps of S 2 #s state machine and maps behaviors allowed by S 1 to behaviors allowed by S 2 . We show that# un# der reasonable assumptions about the speci#cations# if S 1 implements S 2 # then by adding auxiliary variables to S 1 we can guarantee the existence of a re#nement mapping. This provides a completeness result for a practical# hierarchical speci#cation method. Capsule Review This report deals with the problem of proving that implementations satisfy their speci#cations. Suppose# for example# ...","2001-01-15"
970,397021,"Compositional Minimization of Finite State Systems","Aachener Informatikberichte; Bernhard Steffen; Susanne Graf;","In this paper we develop a compositional method for the construction of the minimal transition system that represents the semantics of a given reactive system. The point of this method is that it exploits structural properties of the reactive system in order to avoid the consideration of large intermediate representations. Central is the use of interface specifications here, which express constraints on the components' communication behaviour, and therefore to control the state explosion caused by the interleavings of actions of communicating parallel components. The effect of the method, which is developed for bisimulation semantics here, depends on the structure of the reactive system under consideration, in particular on the accuracy of the interface specifications. However, its correctness does not: every ""successful"" construction is guaranteed to yield the desired minimal transition system, independently of the correctness of the interface specifications provided by the designer.","1995-05-31"
971,397185,"Symbolic Synthesis of Supervisory Controllers","Howard Wong-toi;","Ramadge and Wonham [1] gave algorithms for finding controllers in their supervisory control framework. Their automatic synthesis techniques are implemented here using binary decision diagrams [2]. This technique of symbolic representation capitalizes on loose coupling between controlled plant components. We are able to synthesize a controller for a wafer-manufacturing plant, for which an explicit representation would have about 10 6 states. 1. Introduction The concepts of feedback, controllability and observability proved to be extremely successful in the analysis and control of continuous state dynamic models. These concepts rely heavily on the differentiability of certain continuous domain functions. In recent years, control researchers applied these ideas to discrete-state event-driven systems. Discrete event dynamic models however, do not have a simple compact system representation. Analysis and procedures for synthesizing control mechanisms typically involve brute force enumer...","1997-03-14"
972,399339,"Guided Synthesis of Control Programs Using UPPAAL","Kim G. Larsen; Paul Pettersson; Thomas Hune;",". In this paper we address the problem of scheduling and synthesizing distributed control programs for a batch production plant. We use a timed automata model of the batch plant and the verication tool Uppaal to solve the scheduling problem. In modeling the plant, we aim at a level of abstraction which is suciently accurate in order that synthesis of control programs from generated timed traces is possible. Consequently, the models quickly become too detailed and complicated for immediate automatic synthesis. In fact, only models of plants producing two batches can be analyzed directly! To overcome this problem, we present a general method allowing the user to guide the model-checker according to heuristically chosen strategies. The guidance is specied by augmenting the model with additional guidance variables and by decorating transitions with extra guards on these. Applying this method have made synthesis of control programs feasible for a plant producing as many as 60 batches. Th...","2000-12-07"
973,402497,"Planning as Model Checking","And Paolo Traverso; Fausto Giunchiglia;",". The goal of this paper is to provide an introduction, with various elements of novelty, to the Planning as Model Checking paradigm. 1 Introduction The key idea underlying the Planning as Model Checking paradigm is that planning problems should be solved model-theoretically. Planning domains are formalized as semantic models. Properties of planning domains are formalized as temporal formulas. Planning is done by verifying whether temporal formulas are true in a semantic model. The most important features of the proposed approach are: -- The approach is well-founded. Planning problems are given a clear and intuitive (semantic) formalization. -- The approach is general. The same framework can be used to tackle most research problems in planning, e.g., planning in deterministic and in nondeterministic domains, conditional and iterative planning, reactive planning. -- The approach is practical. It is possible to devise efficient algorithms that generate plans automatically and that ...","1999-10-18"
974,403583,"Propositional Planning","Michael Paul Fourman;","We describe a general setting for finite-state planning, where planning operators, or actions, act as functions mapping sets of states to sets of states. In particular, we introduce propositional actions, which generalise STRIPS actions. Sets of states may be represented by Binary Decision Diagrams (BDDs), and propositional actions may be directly encoded as efficient operations on BDDs. We describe PropPlan a planner based on this representation, report its performance on a variety of benchmark problems, and discuss its relation to other BDD-based approaches to planning, and to GraphPlan, by which it was inspired.","2000-04-03"
975,404934,"Planning via Model Checking: A Decision Procedure for AR","Alessandro Cimatti; Enrico Giunchiglia; Fausto Giunchiglia; Paolo Traverso;",". In this paper we propose a new approach to planning based on a ""high level action language"", called AR, and ""model checking"". AR is an expressive formalism which is able to handle, among other things, ramifications and non-deterministic effects. We define a decision procedure for planning in AR which is based on ""symbolic model checking "", a technique which has been successfully applied in hardware and software verification. The decision procedure always terminates with an optimal solution or with failure if no solution exists. We have constructed a planner, called mbp, which implements the decision procedure. 1 Introduction A lot of research has focused on the problem of the expressiveness of formalisms for reasoning about action and change. This has lead to the development of various formalisms, e.g. action languages able to deal with the ""frame problem"" and the ""ramification problem"" (see for instance [ 7 ] and [ 11 ] ). So far, however, little success has been obtained ...","1999-10-18"
976,405188,"A Heuristic for Domain Independent Planning and its Use in an Enforced Hill-climbing Algorithm","null",". We present a new heuristic method to evaluate planning states, which is based on solving a relaxation of the planning problem. The solutions to the relaxed problem give a good estimate for the length of a real solution, and they can also be used to guide action selection during planning. Using these informations, we employ a search strategy that combines Hill-climbing with systematic search. The algorithm is complete on what we call deadlock-free domains. Though it does not guarantee the solution plans to be optimal, it does find close to optimal plans in most cases. Often, it solves the problems almost without any search at all. In particular, it outperforms all state-of-the-art planners on a large range of domains. 1 INTRODUCTION The standard approach to obtain a heuristic is to relax the problem P at hand into some easier problem P 0 . The optimal solution length to a situation in P 0 can then be used as an admissible estimate for the optimal solution length of the same situ...","2000-06-08"
977,408033,"Representation of Function Variants for Embedded System Optimization and Synthesis","D. Ziegenbein; Ida Tu Braunschweig; J. Teich; K. Richter; L. Thiele; R. Ernst;","Many embedded systems are implemented with a set of alternative function variants to adapt the system to different applications or environments. This paper proposes a novel approach for the coherent representation and selection of function variants in the different phases of the design process. In this context, the modeling of reconfiguration of system parts is supported in a natural way. Using a real example from the video processing domain, the approach is explained and validated. 1 Introduction Many embedded systems are implemented with a fixed core function and a set of alternative function variants to adapt the system to different applications or environments. Examples are TV sets which can be adapted to different standards or automotive control systems to be used in countries with different emission laws. Function variants are mutually exclusive, i. e. only one variant of a set of alternative functions is selected a time. There may be several of those variant sets in one embedd...","1999-05-17"
978,410249,"Efficiently Computing Static Single Assignment Form and the Control Dependence Graph","null","This paper thus presents strong evidence that these structures can be of practical use in optimization.","1999-05-19"
979,411542,"RSVP: A New Resource ReSerVation Protocol","Deborah Estrin; Lixia Zhang; Scott Shenker; Steve Deering;","this article we describe another.","2000-02-29"
980,413665,"SpecCharts: A VHDL Front-End for Embedded Systems","Daniel D. Gajski; Frank Vahid; Sanjiv Narayan;","VHDL and other hardware description languages have become popular as system specification languages in top-down design. However, their constructs do not support the behavioral specification of embedded systems. We introduce a new conceptual design model, called Program-State Machines, that caters to embedded systems. We then describe the SpecCharts language, an extended version of VHDL, which supports capture of this design model. In conjunction with a translator to VHDL, SpecCharts can be easily incorporated into a VHDL design environment, with the advantages of significantly reduced specification time and fewer errors. The extensions introduced for VHDL are applicable to many other hardware description languages as well. We demonstrate the advantages of using SpecCharts for system specification capture through several experiments. Contents 1 Introduction 3 2 Existing HDL Limitations for Embedded Systems 5 2.1 Sequentially and Concurrently Decomposable activities : : : : : : : : : :...","1998-03-19"
981,416093,"Should Your Specification Language Be Typed?","Lawrence C. Paulson;","Incorporated's LAMBDA system moved from a definedness logic [Scott 1979] to conventional higher-order logic for similar reasons. 2.7 Examples The data structures and related operations found in programming and specification languages are easily represented in set theory. We show how to represent three of these structures: finite lists, records, and objects. 2.7.1 Finite Lists. We represent a finite list of length n as a function with domain 1 . . n, the set {i # N : 1 # i # n} of natural numbers from 1 through n. The set List(L) of all finite lists with elements in the set L is just equal to # {(1 . . n) # L : n # N}. The length Len(s) of a finite list s is defined by Len(s) # = choose n . (n # N) # (dom s = 1 . . n) . (5) List and Len are operators; they cannot be functions. For them to be functions, their domains would have to consist of all sets and all finite lists, respectively, neither of which forms a set. ACM Transactions on Programming Languages and Syst...","2000-06-28"
982,419407,"Integer Programs and Valid Inequalities for Planning Problems","Alexander Bockmayr; And Yannis Dimopoulos;",". Part of the recent work in AI planning is concerned with the development of algorithms that regard planning as a combinatorial search problem. The underlying representation language is basically propositional logic. While this is adequate for many domains, it is not clear if it remains so for problems that involve numerical constraints, or optimization of complex objective functions. Moreover, the propositional representation imposes restrictions on the domain knowledge that can be utilized by these approaches. In order to address these issues, we propose moving to the more expressive language of Integer Programming (IP). We show how capacity constraints can be easily encoded into linear 0-1 inequalities and how rich forms of domain knowledge can be compactly represented and computationally exploited by IP solvers. Then we introduce a novel heuristic search method based on the linear programming relaxation. Finally, we present the results of our experiments with a class...","1999-11-04"
983,422908,"Symbolic Model Checking for Real-time Systems","Joseph Sifakis; Miniparc-zirst Rue Lavoisier; Sergio Yovine; Thomas A. Henzinger; Xavier Nicollin;",". We describe finite-state programs over real-numbered time in a guardedcommand language with real-valued clocks or, equivalently, as finite automata with real-valued clocks. Model checking answers the question which states of a real-time program satisfy a branching-time specification (given in an extension of CTL with clock variables). We develop an algorithm that computes this set of states symbolically as a fixpoint of a functional on state predicates, without constructing the state space. For this purpose, we introduce a -calculus on computation trees over real-numbered time. Unfortunately, many standard program properties, such as response for all nonzeno execution sequences (during which time diverges), cannot be characterized by fixpoints: we show that the expressiveness of the timed -calculus is incomparable to the expressiveness of timed CTL. Fortunately, this result does not impair the symbolic verification of ""implementable"" real-time programs---those whose safety...","1996-07-12"
984,423702,"Dynamic Partitioning in Analyses of Numerical Properties","Nicolas Halbwachs; Pascal Raymond;",". We apply linear relation analysis [CH78,HPR97] to the verification of declarative synchronous programs [Hal98]. In this approach, state partitioning plays an important role: on one hand the precision of the results highly depends on the fineness of the partitioning; on the other hand, a too much detailed partitioning may result in an exponential explosion of the analysis. In this paper, we propose to dynamically select a suitable partitioning according to the property to be proved. 1 Introduction Partitioning in abstract interpretation: We address the standard case of abstract interpretation [CC77], where a set S of concrete states is considered, and where a complete lattice (L; v; u; t) of abstract values is associated with the powerset 2 S by means of a Galois connection (ff; fl). In this context, partitioning a fixpoint equation X = F (X) --- where F is a monotone function from L to L --- consists in choosing a finite partition (or, more generally, a finite covering) fS ...","2000-12-14"
985,424616,"Protocol Verification as a Hardware Design Aid","Alan J. Hu; Andreas J. Drexler; C. Han Yang; David L. Dill;","The role of automatic formal protocol verification in hardware design is considered. Principles are identified that maximize the benefits of protocol verification while minimizing the labor and computation required. A new protocol description language and verifier (both called Mur') are described, along with experiences in applying them to two industrial protocols that were developed as part of hardware designs. 1 Introduction Most complex digital designs must be regarded as concurrent systems: individual modules run in parallel and must coordinate by explicit synchronization and communication. Complexity will continue to increase, portending a shift in total design effort from, for instance, faster arithmetic circuits, to mechanisms for coordination. Those mechanisms usually involve protocols: rules that, if followed by each party in a coordinated action, assure a desired outcome. Unfortunately, protocol design is a subtle art. Even when a designer exercises the utmost care...","1997-03-20"
986,424782,"Object-Oriented Distributed Programming in BETA","Ole Lehrmann Madsen; Sren Br;","This paper describes abstractions that have been designed to support distributed programming in the object oriented programming language BETA. The approach is minimalistic in the sense that a goal is to provide the essential building blocks on top of which other distribution related abstractions may be built. This goal is made easier by demanding for type orthogonal persistence and distribution as the full power of the underlying language may then be used when building higher level abstractions on top of the basic ones. 1 Introduction This paper describes abstractions that have been designed to support distributed programming in the object oriented programming language BETA [19]. The abstractions are relatively simple, as they are designed to cope only with distribution specifics, whereas e.g. concurrency issues are dealt with by the basic BETA language. In general, when designing programming languages and systems, a goal is to keep them as simple as possible, consisting of a f...","1994-11-22"
987,425122,"The Verus Language: Representing Time Efficiently with BDDs","Aguiar Campos; Belo Horizonte; Edmund Clarke; Minas Gerais; Srgio Vale;",". There have been significant advances on formal methods to verify complex systems recently. Nevertheless, these methods have not yet been accepted as a realistic alternative to the verification of industrial systems. One reason for this is that formal methods are still difficult to apply efficiently. Another reason is that current verification algorithms are still not efficient enough to handle many complex systems. This work addresses the problem by presenting a language designed especially to simplify writing timecritical programs. It is an imperative language with a syntax similar to C. Special constructs are provided to allow the straightforward expression of timing properties. The familiar syntax makes it easier for non-experts to use the tool. The special constructs make it possible to model the timing characteristics of the system naturally and accurately. A symbolic representation using BDDs, model checking and quantitative algorithms are used to check system tim...","2000-06-07"
988,425638,"Delay Analysis in Synchronous Programs","Cc P. Cousot; R. Cousot;","interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. In 4th ACM Symposium on Principles of Programming Languages, January 1977. [CC92a] P. Cousot and R. Cousot. Abstract interpretation and application to logic programs. Research Report LIX/RR/92/08, Ecole Polytechnique, March 1992. (to appear in the Journal of Logic Programming, special issue on Abstract Interpretation). [CC92b] P. Cousot and R. Cousot. Comparing the Galois connection and widenning /narrowing approaches to abstract interpretation. Research Report LIX/RR/92/09, Ecole Polytechnique, June 1992. [CH78] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among variables of a program. In 5th ACM Symposium on Principles of Programming Languages, Tucson (Arizona), January 1978. [Che68] N. V. Chernikova. Algorithm for discovering the set of all solutions of a linear programming problem. U.S.S.R. Computational Mathematics and Mathematical Phys...","1994-06-10"
989,425810,"Salsa: Combining Constraint Solvers with BDDs for Automatic Invariant Checking","And Steve Sims; Ramesh Bharadwaj;",". Salsa is an invariant checker for specifications in SAL (the SCR Abstract Language). To establish a formula as an invariant without any user guidance Salsa carries out an induction proof that utilizes tightly integrated decision procedures, currently a combination of BDD algorithms and a constraint solver for integer linear arithmetic, for discharging the verification conditions. The user interface of Salsa is designed to mimic the interfaces of model checkers; i.e., given a formula and a system description, Salsa either establishes the formula as an invariant of the system (but returns no proof) or provides a counterexample. In either case, the algorithm will terminate. Unlike model checkers, Salsa returns a state pair as a counterexample and not an execution sequence. Also, due to the incompleteness of induction, users must validate the counterexamples. The use of induction enables Salsa to combat the state explosion problem that plagues model checkers -- it can handle...","2000-02-16"
990,425983,"Algorithms and Complexity","Herbert S. Wilf;","CONTENTS Chapter 0: What This Book Is About 0.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 0.2 Hard vs. easy problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 0.3 A preview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Chapter 1: Mathematical Preliminaries 1.1 Orders of magnitude . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Positional number systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.3 Manipulations with series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.4 Recurrence relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.5 Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.6 Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Chapter 2: Recursive Algorithms<F12.3","1997-02-03"
991,426325,"What Good Are Digital Clocks?","Amir Pnueli; Thomas A. Henzinger; Zohar Manna;",". Real-time systems operate in ""real,"" continuous time and state changes may occur at any real-numbered time point. Yet many verification methods are based on the assumption that states are observed at integer time points only. What can we conclude if a real-time system has been shown ""correct"" for integral observations? Integer time verification techniques suffice if the problem of whether all real-numbered behaviors of a system satisfy a property can be reduced to the question of whether the integral observations satisfy a (possibly modified) property. We show that this reduction is possible for a large and important class of systems and properties: the class of systems includes all systems that can be modeled as timed transition systems; the class of properties includes time-bounded invariance and time-bounded response. 1 Introduction Over the past few years, we have seen a proliferation of formal methodologies for software and hardware design that emphasize the treatm...","1994-09-19"
992,427509,"Partial-Order Reduction in Symbolic State Space Exploration","R. Alur; R. K. Brayton; S. K. Rajamani; S. Qadeer; T. A. Henzinger;",". State space explosion is a fundamental obstacle in formal verification of designs and protocols. Several techniques for combating this problem have emerged in the past few years, among which two are significant: partial-order reductions and symbolic state space search. In asynchronous systems, interleavings of independent concurrent events are equivalent, and only a representative interleaving needs to be explored to verify local properties. Partial-order methods exploit this redundancy and visit only a subset of the reachable states. Symbolic techniques, on the other hand, capture the transition relation of a system and the set of reachable states as boolean functions. In many cases, these functions can be represented compactly using binary decision diagrams (BDDs). Traditionally, the two techniques have been practiced by two different schools---partial-order methods with enumerative depth-first search for the analysis of asynchronous network protocols, and symbolic bread...","2001-01-20"
993,429457,"Executable Object Modeling with Statecharts","null","Statecharts, popular for modelling system behavior in the structural analysis paradigm, are part of a fully executable language set for modelling object-oriented systems. The languages form the core of the emerging Unified Modelling Language.","2001-03-13"
994,429561,"Selective Quantitative Analysis and Interval Model Checking: Verifying Different Facets of a System","Edmund M. Clarke; Orna Grumberg; Sergio Campos;","In this work we propose a verification methodology consisting of selective quantitative timing analysis and interval model checking. Our methods can aid not only in determining if a system works correctly, but also in understanding how well the system works. The selective quantitative algorithms compute minimum and maximum delays over a selected subset of system executions. A linear-time temporal logic (LTL) formula is used to select either infinite paths or finite intervals over which the computation is performed. We show how tableau for LTL formulas can be used for selecting either paths or intervals and also for model checking formulas interpreted over paths or intervals. To demonstrate the usefulness of our methods we have verified a complex and realistic distributed real-time system. Our tool has been able to analyze the system and to compute the response time of the various components. Moreover, we have been able to identify inefficiencies that caused the response time...","2000-06-07"
995,430308,"Automated Consistency Checking of Requirements Specifications","Bruce G. Labaw; Constance L. Heitmeyer; Ralph D. Jeffords;","This article describes a formal analysis technique, called consistency checking, for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique is designed to analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the SCR notation and a foundation for consistency checking, a formal requirements model is introduced; the model represents a software system as a finite-state automaton, which produces externally visible outputs in response to changes in monitored environmental quantities. Results of two experiments are presented which evaluated the utility and scalability of our technique for consistency checking in a real-world avionics application. The role of consistency checking during the requirements phase of software development is discussed.","2000-03-02"
996,431581,"Integrated Services in the Internet Architecture: An Overview","null","This memo discusses a proposed extension to the Internet architecture and protocols to provide integrated services, i.e., to support realtime as well as the current non-real-time service of IP. This extension is necessary to meet the growing need for real-time service for a variety of new applications, including teleconferencing, remote seminars, telescience, and distributed simulation. This memo represents the direct product of recent work by Dave Clark, Scott Shenker, Lixia Zhang, Deborah Estrin, Sugih Jamin, John Wroclawski, Shai Herzog, and Bob Braden, and indirectly draws upon the work of many others. 1. Introduction ...................................................2 2. Elements of the Architecture ...................................3 2.1 Integrated Services Model ..................................3 2.2 Reference Implementation Framework .........................6 3. Integrated Services Model ......................................11 3.1 Quality of Service Requirements ............","1970-01-01"
997,431696,"CLASSIC: A Structural Data Model for Objects","Alexander Borgida; Deborah L. Mcguinness; Lori Alperin Resnick; Ronald J. Brachman;","CLASSIC is a data model that encourages the description ofobjects not only in terms of their relations to other known objects, but in terms of a level of intensional structure as well. The CLASSIC language of structured descriptions permits i) partial descriptions of individuals, under an `open world' assumption, ii) answers to queries either as extensional lists of valuesorasdescriptions that necessarily hold of all possible answers, and iii) an easily extensible schema, which can be accessed uniformly with the data. One of the strengths of the approach is that the same language plays multiple roles in the processes of defining and populating the DB, as well as querying and answering. classic (for which we have a prototype main-memory implementation) can actively discover new information about objects from several sources: it can recognize new classes under which an object falls based on a description of the object, it can propagate some deductive consequences of DB upda...","2000-02-15"
998,434053,"A Tool to Support Formal Reasoning about Computer Languages","Richard J. Boulton;",". A tool to support formal reasoning about computer languages and specific language texts is described. The intention is to provide a tool that can build a formal reasoning system in a mechanical theorem prover from two specifications, one for the syntax of the language and one for the semantics. A parser, pretty-printer and internal representations are generated from the former. Logical representations of syntax and semantics, and associated theorem proving tools, are generated from the combination of the two specifications. The main aim is to eliminate tedious work from the task of prototyping a reasoning tool for a computer language, but the abstract specifications of the language also assist the automation of proof. 1 Introduction For several decades theorem proving systems have been used to reason about computer languages. A common approach has been to define the semantics of a language in the logic of the theorem prover. This may be done by defining new constants in t...","1997-12-03"
999,437334,"A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic","Alex Borgida; Peter F. Patel-schneider;","This paper analyzes the correctness of the subsumption algorithm used in classic, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in classic descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in classic, and are of independent interest. 1. Introduction to Description Logics Data and knowledge bases are models of some part of the natural world. Such models are often built from individual objects that are inter-related by relationships and g...","1996-01-11"
1000,437429,"Quasi-Static Scheduling of Embedded Software Using Equal Conflict Nets","Alberto Sangiovanni-vincentelli; Luciano Lavagno; Marco Sgroi;","Embedded system design requires the use of efficient scheduling policies to execute on shared resources, e.g. the processor, algorithms that consist of a set of concurrent tasks with complex mutual dependencies. Scheduling techniques are called static when the schedule is computed at compile time, dynamic when some or all decisions are made at run-time. The choice of the scheduling policy mainly depends on the specification of the system to be designed. For specifications containing only data computation, it is possible to use a fully static scheduling technique, while for specifications containing data-dependent control structures, like the if-then-else or while-do constructs, the dynamic behaviour of the system cannot be completely predicted at compile time and some scheduling decisions are to be made at run-time. For such applications we propose a Quasi-static scheduling (QSS) algorithm that generates a schedule in which run-time decisions are made only for data-dependent control st...","1970-01-01"
1001,437434,"Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods","Charles Romine; Henk Van Der Vorst; Jack Dongarra; James Demmel; June M. Donato; Michael Berry; Richard Barrett; Roldan Pozo; Tony F. Chan; Victor Eijkhout;","ps quit 2. from any machine on the Internet type: rcp anon@www.netlib.org:templates/templates.ps templates.ps 3. send email to netlib@ornl.gov and in the message type: send templates.ps from templates The url for this book is http://www.netlib.org/templates/Templates.html . A bibtex reference for this book follows: @BOOKftemplates, AUTHOR = fR. Barrett and M. Berry and T. F. Chan and J. Demmel and J. Donato and J. Dongarra and V. Eijkhout and R. Pozo and C. Romine, and H. Van der Vorst g, TITLE = fTemplates for the Solution of Linear Systems: Building Blocks for Iterative Methodsg, PUBLISHER = fSIAMg, YEAR = f1994g, ADDRESS = fPhiladelphia, PAg g ii How to Use This Book We have divided this book into five main chapters. Chapter 1 gives the motivation for this book and the u","1996-08-15"
1002,437951,"Probabilistic Simulations for Probabilistic Processes","Nancy Lynch; Roberto Segala;",". Several probabilistic simulation relations for probabilistic systems are defined and evaluated according to two criteria: compositionality and preservation of ""interesting"" properties. Here, the interesting properties of a system are identified with those that are expressible in an untimed version of the Timed Probabilistic concurrent Computation Tree Logic (TPCTL) of Hansson. The definitions are made, and the evaluations carried out, in terms of a general labeled transition system model for concurrent probabilistic computation. The results cover weak simulations, which abstract from internal computation, as well as strong simulations, which do not. 1 Introduction Randomization has been shown to be a useful tool for the solution of problems in distributed systems [1, 2, 12]. In order to support reasoning about probabilistic distributed systems, many researchers have recently focused on the study of models and methods for the analysis of such systems [3, 5, 7, 19--21]. The g...","1994-09-29"
1003,441895,"Learning Decision Lists","Ronald L. Rivest;","This paper introduces a new representation for Boolean functions, called decision lists,","2001-04-24"
1004,442323,"Bounded Scheduling of Process Networks","Thomas M. Parks;","Bounded Scheduling of Process Networks","1999-03-23"
1005,445265,"Deriving Petri Nets from Finite Transition Systems","Jordi Cortadella; Luciano Lavagno; Michael Kishinevsky; Re Yakovlev; Senior Member;","This paper presents a novel method to derive a Petri Net from any specification model that can be mapped into a statebased representation with arcs labeled with symbols from an alphabet of events (a Transition System, TS). The method is based on the theory of regions for Elementary Transition Systems (ETS). Previous work has shown that, for any ETS, there exists a Petri Net with minimum transition count (one transition for each label) with a reachability graph isomorphic to the original Transition System. Our method extends and implements that theory by using the following three mechanisms that provide a framework for synthesis of safe Petri Nets from arbitrary TSs. First, the requirement of isomorphism is relaxed to bisimulation of TSs, thus extending the class of synthesizable TSs to a new class called Excitation-Closed Transition Systems (ECTS). Second, for the first time, we propose a method of PN synthesis for an arbitrary TS based on mapping a TS event into a set of transition labels in a PN. Third, the notion of irredundant region set is exploited, to minimize the number of places in the net without affecting its behavior. The synthesis method can derive different classes of place-irredundant Petri Nets (e.g., pure, free choice, unique choice) from the same TS, depending on the constraints imposed on the synthesis algorithm. This method has been implemented and applied in different frameworks. The results obtained from the experiments have demonstrated the wide applicability of the method. Index Terms---Petri Nets, transition systems, concurrent systems, asynchronous systems, synthesis. ------------------------------ ###p### ------------------------------ 1","2001-06-14"
1006,446046,"Simulation of Hybrid Circuits in Constraint Logic Programming","Claudine Pradelles-lasserre; F- Saint-cloud; Laurent Zimmer; Pascal Van Hentenryck; Quai Marcel Dassault; Thomas Graf;","This paper presents LOGISIM, a CAD tool to simulate the temporal behaviour of hybrid circuits containing electro-mechanical, electrohydraulic, hydro-mechanic, and digital control devices. LOGISIM combines the advantages of both qualitative and quantitative reasoning by producing a high-level description (discrete states) of the circuit behaviour while reasoning at the quantitative level (physical values). In addition, device models in LOGISIM follow a particular description methodology proposed to avoid introducing an artificial computational complexity in the simulation. LOGISIM is fully implemented in the constraint logic programming language CHIP. The constraint-solving techniques of CHIP used in LOGISIM, i.e., an incremental decision procedure for linear constraints over rational numbers, consistency techniques on domain-variables and conditional inference, are all necessary to solve the problem efficiently. LOGISIM has been applied successfully to real-life industrial circuits from aerospace industry in the ELSA project and clearly demonstrates the potential of this kind of tool to support the design process for these circuits. This paper is a revised and extended version of [GVHPZ89]","2001-06-11"
1007,448345,"Average-Case Analysis of Algorithms and Data Structures L'analyse en moyenne des algorithmes et des structures de donn'ees","Jeffrey Scott;",". This report is a contributed chapter to the Handbook of Theoretical Computer Science (North-Holland, 1990). Its aim is to describe the main mathematical methods and applications in the average-case analysis of algorithms and data structures. It comprises two parts: First, we present basic combinatorial enumerations based on symbolic methods and asymptotic methods with emphasis on complex analysis techniques (such as singularity analysis, saddle point, Mellin transforms). Next, we show how to apply these general methods to the analysis of sorting, searching, tree data structures, hashing, and dynamic algorithms. The emphasis is on algorithms for which exact ""analytic models"" can be derived. R'esum'e. Ce rapport est un chapitre qui parait dans le Handbook of Theoretical Computer Science (North-Holland, 1990). Son but est de d'ecrire les principales m'ethodes et applications de l'analyse de complexit'e en moyenne des algorithmes. Il comprend deux parties. Tout d'abord, nous donnons une pr'esentation des m'ethodes de d'enombrements combinatoires qui repose sur l'utilisation de m'ethodes symboliques, ainsi que des techniques asymptotiques fond'ees sur l'analyse complexe (analyse de singularit'es, m'ethode du col, transformation de Mellin). Ensuite, nous d'ecrivons l'application de ces m'ethodes g'enerales `a l'analyse du tri, de la recherche, de la manipulation d'arbres, du hachage et des algorithmes dynamiques. L'accent est mis dans cette pr'esentation sur les algorithmes pour lesquels existent des hh mod`eles analytiques ii exacts. 1 Dept. of Computer Science, Brown University, Providence, R. I. 02912, USA. Research was also done while the author was on sabbatical at INRIA in Rocquencourt, France, and at Ecole Normale Sup'erieure in Paris. Support was provided in p...","2001-05-05"
1008,448406,"On the Use of Integer Programming Models in AI Planning","Amnon Lotem; Dana Nau; Michael Ball; Thomas Vossen;","Recent research has shown the promise of using propositional reasoning and search to solve AI planning problems. In this paper, we further explore this area by applying Integer Programming to solve AI planning problems. The application of Integer Programming to AI planning has a potentially significant advantage, as it allows quite naturally for the incorporation of numerical constraints and objectives into the planning domain. Moreover, the application of Integer Programming to AI planning addresses one of the challenges in propositional reasoning posed by Kautz and Selman, who conjectured that the principal technique used to solve Integer Programs---the linear programming (LP) relaxation---is not useful when applied to propositional search. We discuss various IP formulations for the class of planning problems based on STRIPS-style planning operators. Our main objective is to show that a carefully chosen IP formulation significantly improves the ""strength"" of the LP relaxation, and that the resultant LPs are useful in solving the IP and the associated planning problems. Our results clearly show the importance of choosing the ""right"" representation, and more generally the promise of using Integer Programming techniques in the AI planning domain. 1 ","1999-05-20"
1009,449353,"As Cheap as Possible: Efficient Cost-Optimal Reachability for Priced Timed Automata","Ansgar Fehnker; Gerd Behrmann; Judi Romijn; Kim Larsen; Paul Pettersson;","In this paper we present an algorithm for efficiently computing optimal cost of reaching a goal state in the model of Linearly Priced Timed Automata (LPTA). The central contribution of this paper is a priced extension of so-called zones. This, together with a notion of facets of a zone, allows the entire machinery for symbolic reachability for timed automata in terms of zones to be lifted to cost-optimal reachability using priced zones. We report on experiments with a cost-optimizing extension of Uppaal on a number of examples.","2001-04-26"
1010,449470,"Automated Deduction by Theory Resolution","Mark E. Stickel;","Theory resolution constitutes a set of complete procedures for incorporating theories into a resolution theorem-proving program, thereby making it unnecessary to resolve directly upon axioms of the theory. This can greatly reduce the length of proofs and the size of the search space. Theory resolution effects a beneficial division of labor, improving the performance of the theorem prover and increasing the applicability of the specialized reasoning procedures. Total theory resolution utilizes a decision procedure that is capable of determining unsatisfiability of any set of clauses using predicates in the theory. Partial theory resolution employs a weaker decision procedure that can determine potential unsatisfiability of sets of literals. Applications include the building in of both mathematical and special decision procedures, e.g., for the taxonomic information furnished by a knowledge representation system. Theory resolution is a generalization of numerous previously known resolution refinements. Its power is demonstrated by comparing solutions of ""Schubert's Steamroller"" challenge problem with and without building in axioms through theory resolution. 1 1 ","2001-08-16"
1011,450449,"Weak-commitment Search for Solving Constraint Satisfaction Problems","Makoto Yokoo;","The min-conflict heuristic has been introduced into backtracking algorithms and iterative improvement algorithms as a powerful heuristic for solving constraint satisfaction problems. Backtracking algorithms become inefficient when a bad partial solution is constructed, since an exhaustive search is required for revising the bad decision. On the other hand, iterative improvement algorithms do not construct a consistent partial solution and can revise a bad decision without exhaustive search. However, most of the powerful heuristics obtained through the long history of constraint satisfaction studies (e.g., forward checking) presuppose the existence of a consistent partial solution. Therefore, these heuristics can not be applied to iterative improvement algorithms. Furthermore, these algorithms are not theoretically complete. In this paper, a new algorithm called weak-commitment search which utilizes the min-conflict heuristic is developed. This algorithm removes the drawbacks of backtracking algorithms and iterative improvement algorithms, i.e., the algorithm can revise bad decisions without exhaustive search, the completeness of the algorithm is guaranteed, and various heuristics can be introduced since a consistent partial solution is constructed. The experimental results on various example problems show that this algorithm is 3 to 10 times more efficient than other algorithms.","2001-07-18"
1012,453690,"Programming Paradigms of the Andorra Kernel Language","Seif Haridi; Sverker Janson;","The Andorra Kernel Language #AKL# is introduced. It is shown how AKL provides the programming paradigms of both Prolog and GHC. This is the original goal of the design. However, it has also been possible to provide capabilities beyond that of Prolog and GHC. There are means to structure search, more powerful than plain backtracking. It is possible to encapsulate search in concurrent reactive processes. It is also possible to write a multi-way merger with constant delay. In these respects AKL is quite original. Although AKL is an instance of our previously introduced Kernel Andorra Prolog framework, this exposition contains important extensions, and a considerable amount of unnecessary formal overhead has been stripped away. 1 ","1998-01-22"
1013,453813,"SPUDD: Stochastic Planning using Decision Diagrams","Alan Hu; Craig Boutilier; Jesse Hoey; Robert St-aubin;","Recently, structured methods for solving factored Markov decisions processes (MDPs) with large state spaces have been proposed recently to allow dynamic programming to be applied without the need for complete state enumeration. We propose and examine a new value iteration algorithm for MDPs that uses algebraic decision diagrams (ADDs) to represent value functions and policies, assuming an ADD input representation of the MDP. Dynamic programming is implemented via ADD manipulation. We demonstrate our method on a class of large MDPs (up to 63 million states) and show that significant gains can be had when compared to tree-structured representations (with up to a thirty-fold reduction in the number of nodes required to represent optimal value functions). 1 ","2001-01-12"
1014,453924,"Model Checking for a Probabilistic Branching Time Logic With Fairness","Christel Baier; Marta Kwiatkowska;","this paper have applications in automatic verification of randomized distributed systems. Key words: Probabilistic processes -- Temporal logic -- Verification -- Fairness 1 Introduction Probabilistic techniques, and in particular probabilistic logics, have proved successful in the specification and verification of systems that exhibit uncertainty, for example, fault-tolerant systems, randomized algorithms, distributed systems, and communication protocols. However, as already observed in [45,52,56], concurrent probabilistic systems, for example randomized distributed algorithms, are notoriously difficult to verify: the proofs of their correctness are complex, and therefore argued informally, and thus appropriate # Supported in part by EPSRC grant GR/K42028. formal methods for their specification and verification are called for. This paper presents an automatic model checking method applicable, amongst others, to the verification of randomized distributed systems. The particular difficulty in establishing correctness of randomized distributed algorithms is due to the fact that they exhibit both probabilistic choice (which comes from random assignment and is considered internal to the system) as well as non-determinism. This means that it is possible in a given state to non-deterministically choose between two or more probability distributions on the successor states; these distributions determine the probability with which a successor state is taken. Non-determinism may arise e.g. from the asynchronicity of certain subprocesses, or external intervention such as an action taken by environment. As an example of the former, consider the randomized dining philosophers: when two philosophers are simultaneously ready to flip a fair coin in order to decide which fork to pic...","2000-06-21"
1015,455070,"Error Detection with Directed Symbolic Model Checking","And Stefan Edelkamp;",". In practice due to entailed memory limitations the most important problem in model checking is state space explosion. Therefore, to prove the correctness of a given design binary decision diagrams #BDDs# are widely used as a concise and symbolic state space representation. Nevertheless, BDDs are not able to avoid an exponential blow-up in general. If we restrict ourselves to #nd an error of a design which violates a safety property,inmany cases a complete state space exploration is not necessary and the introduction of a heuristic to guide the search can help to keep both the explored part and the associated BDD representation smaller than with the classical approach. In this paper we will show that this idea can be extended with an automatically generated heuristic and that it is applicable to a large class of designs. Since the proposed algorithm can be expressed in terms of BDDs it is even possible to use an existent model checker without any internal changes. 1 ","2001-02-22"
1016,455837,"Solving the Entailment Problem in the Fluent Calculus using Binary Decision Diagrams","Hans--peter Storr; Steffen Holldobler;",". It is rigorously shown how planning problems encoded as a class of entailment problems in the fluent calculus can be mapped onto satisfiability problems for propositional formulas, which in turn can be mapped to the problem of finding models using binary decision diagrams (BDDs). The mapping is shown to be sound and complete. First experimental results of an implementation are presented and discussed. 1 ","2000-05-05"
1017,455847,"Dataflow Process Networks","Edward A. Lee; Thomas M. Parks;","We review a model of computation used in industrial practice in signal processing software environments and experimentally in other contexts. We give this model the name ""dataflow process networks,"" and study its formal properties as well as its utility as a basis for programming language design. Variants of this model are used in commercial visual programming systems such as SPW from the Alta Group of Cadence (formerly Comdisco Systems), COSSAP from Synopsys (formerly Cadis), the DSP Station from Mentor Graphics, and Hypersignal from Hyperception. They are also used in research software such as Khoros from the University of New Mexico and Ptolemy from the University of California at Berkeley, among many others. Dataflow process networks are shown to be a special case of Kahn process networks, a model of computation where a number of concurrent processes communicate through unidirectional FIFO channels, where writes to the channel are non-blocking, and reads are blocking. In dataflow process networks, each process consists of repeated ""firings"" of a dataflow ""actor"". An actor defines a (often functional) quantum of computation. By dividing processes into actor firings, the considerable overhead of context switching incurred in most implementations of Kahn process networks is avoided. We relate dataflow process networks to other dataflow models, including those used in dataflow machines, such as static dataflow and the tagged-token model. We also relate dataflow process networks to functional languages such as Haskell, and show that modern language concepts such as higher-order functions and polymorphism can be used effectively in dataflow process networks. A number of programming examples using a visual syntax are given. This research is part of the Ptolemy project, whi...","1999-04-21"
1018,456820,"Disk Striping Strategies for Large Video-on-Demand Servers","Chin Ooi; Chua Jiandong; Kian-lee Tan; Li Beng; Tat Seng;","The storage structure of videos on disks affects the number of concurrent users a video- on-demand system can support and hence the average waiting time. In this paper, we propose a phase-based striping method which has the desirable characteristics that it guarantees the maximum waiting time. We also develop a data replication scheme to further reduce the average waiting time of the phase-based method. These two schemes, together with the conventional sequential striping scheme, are then employed to optimize the waiting time of videos based on their access pattern. Simulations were conducted using a video server containing 36 videos under different loading and hardware configurations. The results show that such optimization can reduce the waiting time significantly. The use of replication method could further improve the server performance by over 25%. The overall results indicate that with limited resources, the use of phase-based striping method with replication is preferable under heavy loads. Keywords: Video-on-demand, disk striping, replication. ","1996-09-23"
1019,458474,"Who Are the Variables in Your Neighborhood","Fabio Somenzi; Shipra Panda;","Dynamic reordering techniques have had considerable success in reducing the impact of the initial variable order on the size of Binary Decision Diagrams (BDDs) and related data structures (e.g., Algebraic Decision Diagrams). Sifting, in particular, has emerged as a very good compromise between low CPU time requirements and high quality of results. Sifting, however, has the absolute position of a variable as the primary objective, and only considers the relative positions of groups of variables indirectly. In this paper we propose an extension to sifting that may sift groups of variables simultaneously to produce better results. Variables are aggregated by looking if they have a strong affinity to their neighbors. (Hence the title.) We discuss the detail of the groups sifting algorithm, and we present experimental results. We show that considerable improvement in size (33% on average) can be achieved with negligible additional CPU time. We also show that our method can in most cases identify automatically those variable groups that can be derived from structural analysis of the circuit. Finally, we show that our method produces results that are on average within 1% from those of a much more expensive simulated annealing algorithm. 1 ","1998-03-24"
1020,459193,"Models for Concurrency","Glynnwinskel Mogensnielsen;","Thisis,we believe,theflnalversionofa chaperfortheHandbookofLogic andtheFoundationsofComputerScience,vol.IV,OxfordUniversity Press. Itsurveysa rangeofmodelsforparallelcomputertationtoincludeinterleavingmodelslike transitionsystems,synchronisationtreesand languages (oftencalledHoaretracesinthiscontext),andmodelslike Petrinets,asynchronoustransitionsystems, event structures,promsetsand Mazurkiewicz traceswhereconcurrencyisrepresentedmoreexplicitlyby aformofcausalindependence. Thepresentationisuniflesby castingthemodelsinancategorytheoreticframework. One aimistousecategorytheorytoprovideabstract characterisationsofconstructionslike parallelcompositionvalidthroughout a rangeofdifieren tmodelsanduseadjunctionstoprovideformalmeansfor translatingbetweendifierentmodels.A knowledgeofbasiccategorytheory isassumed,up toanacquaintancewiththenotionofadjunction. Contents 1 ","2001-11-07"
1021,459683,"Symmetry Detection and Dynamic Variable Ordering of Decision Diagrams","Bernard F. Plessier; Fabio Somenzi; Shipra Panda;","Knowing that some variables are symmetric in a function has numerous applications; in particular, it can help produce better variable orders for Binary Decision Diagrams (BDDs) and related data structures (e.g., Algebraic Decision Diagrams). It has been observed that there often exists an optimum order for a BDD wherein symmetric variables are contiguous. We propose a new algorithm for the detection of symmetries, based on dynamic reordering, and we study its interaction with the reordering algorithm itself. We show that combining sifting with an efficient symmetry check for contiguous variables results in the fastest symmetry detection algorithm reported to date and produces better variable orders for many BDDs. The overhead on the sifting algorithm is negligible. 1 ","1998-03-24"
1022,460258,"An Empirical Analysis of Search in GSAT","Ian P. Gent; Toby Walsh;","We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hillclimbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3-SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms. 1. ","1993-09-16"
1023,460273,"Compiling Proof Search in Semantic Tableaux","Joachim Posegga;","An approach to implementing deduction systems based on semantic tableaux is described; it works by compiling a graphical representation of a fully expanded tableaux into a program that performs the search for a proof at runtime. This results in more efficient proof search, since the tableau needs not to be expanded any more, but the proof consists of determining whether it can be closed, only. It is shown how the method can be applied for compiling to the target language Prolog, although any other general purpose language can be used. 1 ","2001-11-05"
1024,463585,"Proof Verification and the Hardness of Approximation Problems","Carsten Lund; Mario Szegedy; Rajeev Motwani; Sanjeev Arora;","We show that every language in NP has a probablistic verifier that checks membership proofs for it using logarithmic number of random bits and by examining a constant number of bits in the proof. If a string is in the language, then there exists a proof such that the verifier accepts with probability 1 (i.e., for every choice of its random string). For strings not in the language, the verifier rejects every provided ""proof"" with probability at least 1/2. Our result builds upon and improves a recent result of Arora and Safra [6] whose verifiers examine a nonconstant number of bits in the proof (though this number is a very slowly growing function of the input length). As a consequence we prove that no MAX SNP-hard problem has a polynomial time approximation scheme, unless NP=P. The class MAX SNP was defined by Papadimitriou and Yannakakis [82] and hard problems for this class include vertex cover, maximum satisfiability, maximum cut, metric TSP, Steiner trees and shortest superstring. We also improve upon the clique hardness results of Feige, Goldwasser, Lov&#x00E1;sz, Safra and Szegedy [42], and Arora and Safra [6] and show that there exists a positive such that approximating the maximum clique size in an N-vertex graph to within a factor of N is NP-hard.","2001-08-16"
1025,463810,"Automatic Test Generation from Statecharts Using Model Checking","Hyoung Seok Hong; Insup Lee; Oleg Sokolsky; Sung Deok Cha;","This paper discusses the application of model checking to test generation from statecharts. We consider a family of coverage criteria based on the control flow and data flow of statecharts and formulate the problem of test generation as finding counterexamples during the model checking of statecharts. The ability of model checkers to construct counterexamples allows test generation to be automatic. To illustrate our approach, we use the temporal logic CTL and its symbolic model checker SMV. We describe how to translate statecharts to inputs to SMV after defining the semantics of statecharts in terms of Kripke structures. We, then, describe how to express various coverage criteria in CTL and show how SMV can be used to generate only executable tests. 1 ","2001-09-14"
1026,464834,"Kronos: A Model-Checking Tool for Real-Time Systems","Alfredo Olivero; And Sergio Yovine; Conrado Daws; Marius Bozga; Oded Maler; Stavros Tripakis; Verimag Centre;","s (LTS) which has been used to exploit untimed verification techniques [20]. Theoretical background The system-description language of Kronos is the model of timed automata [2], which are communicating finite-state machines extended with continuous realvalued variables (clocks) used to measure time delays. Usually a system is modeled as a network of automata. Communication is achieved by label synchronization `a la CCS or CSP (binary or n-ary rendez-vous), or shared variables (of bounded integer or enumeration type). System requirements can be specified in Kronos using a variety of formalisms, such as the real-time logic Tctl [1, 14], timed Buchi automata, or ? Kronos is developed at Verimag, a joint laboratory of UJF, Ensimag and CNRS. http://www.imag.fr/VERIMAG/PEOPLE/Sergio.Yov","2001-10-29"
1027,466838,"Sooner is Safer than Later","Thomas A. Henzinger;","It has been observed repeatedly that the standard safety-liveness classification for properties of reactive systems does not fit for real-time properties. This is because the implicit ""liveness"" of time shifts the spectrum towards the safety side. While, for example, response --- that ""something good"" will happen eventually --- is a classical liveness property, bounded response --- that ""something good"" will happen soon, within a certain amount of time --- has many characteristics of safety. We account for this phenomenon formally by defining safety and liveness relative to a given condition, such as the progress of time. Keywords. Safety, liveness, real time, topology, concurrency, semantics. 1 Safety, Liveness, and Operationality The behavior of a discrete reactive system can be described as an infinite string oe : oe 0 oe 1 oe 2 oe 3 oe 4 : : : over an alphabet Sigma, which represents the states of the system. A property Pi is a subset of Sigma ! , the set of all infinite...","2001-05-31"
1028,467755,"Bit-Level Analysis of an SRT Divider Circuit","Randal E. Bryant;","It is impractical to verify multiplier or divider circuits entirely at the bit-level using ordered Binary Decision Diagrams (BDDs), because the BDD representations for these functions grow exponentially with the word size. It is possible, however, to analyze individual stages of these circuits using BDDs. Such analysis can be helpful when implementing complex arithmetic algorithms. As a demonstration, we show that Intel could haveused BDDs to detect erroneous lookup table entries in the Pentium(TM) floating point divider. Going beyond verification, we show that bit-level analysis can be used to generate a correct version of the table. 1. ","1998-07-24"
1029,468667,"Efficient Verification of Parallel Real-Time Systems","Bernd--holger Schlingloff; Tomohiro Yoneda;","This paper presents an efficient model checking algorithm for one--safe time Petri nets and a timed temporal logic. The approach is based on the idea of (1) using only differences of timing variables to be able to construct a finite representation of the set of all reachable states and (2) further reducing the size of this representation by exploiting the concurrency in the net. This reduction of the state space is possible, because the considered linear--time temporal logic is stuttering invariant. The firings of transitions are only partially ordered by causality and a given formula; therefore the order of firings of independent transitions is irrelevant, and only one of several equivalent interleavings has to be generated for the evaluation of the given formula. In this paper the theory of timing verification with time Petri nets and temporal logic is presented, a concrete model checking algorithm is developed and proved to be correct, and some experimental results demonstrating the efficiency of the method are given. 1 ","1997-05-06"
1030,468911,"Programmable Syntax Macros","Daniel Weise; Roger Crew;","Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP #the C preprocessor#, syntax macros operate on Abstract Syntax Trees #ASTs#. Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, di#cult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems byhaving the macro language be a minimal extension of the programming language, byintroducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro de#nition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro definitions, or to parse user code that invokes macros. 1 ","1999-04-23"
1031,470971,"Towards First-order Deduction Based on Shannon Graphs","Bertram Ludascher; Joachim Posegga;","We present a new approach to Automated Deduction based on the concept of Shannon graphs, which are also known as Binary Decision Diagrams (BDDs). A Skolemized formula is first transformed into a Shannon graph, then the latter is compiled into a set of Horn clauses. These can finally be run as a Prolog program trying to refute the initial formula. It is also possible to precompile axiomatizations into Prolog and load these theories as required. Keywords: Automated Deduction, Shannon Graphs, Binary Decision Diagrams 1 ","1995-10-31"
1032,471166,"OBDD-based Universal Planning: Specifying and Solving Planning Problems for Synchronized Agents in Non-Deterministic Domains","Manuela M. Veloso; Rune M. Jensen;","Recently model checking representation and search techniques were shown to be efficiently applicable to planning, in particular to non-deterministic planning. Such planning approaches use Ordered Binary Decision Diagrams (obdds) to encode a planning domain as a non-deterministic finite automaton (NFA) and then apply fast algorithms from model checking to search for a solution. obdds can effectively scale and can provide universal plans for complex planning domains. We are particularly interested in addressing the complexities arising in nondeterministic, multi-agent domains. In this chapter, we present umop, 1 a new universal obdd-based planning framework for non-deterministic, multi-agent domains, which is also applicable to deterministic singleagent domains as a special case. We introduce a new planning domain description language, NADL, 2 to specify non-deterministic multi-agent domains. The language contributes the explicit definition of controllable agents and uncontrollable environment agents. We describe the syntax and semantics of NADL and show how to build an efficient obdd-based representation of an NADL description. The umop planning system uses NADL and different obdd-based universal planning algorithms. It includes the previously developed strong and strong cyclic planning algorithms [9, 10]. In addition, we introduce our new optimistic planning algorithm, which relaxes optimality guarantees and generates plausible universal plans in some domains where no strong or strong cyclic solution exist. We present empirical results from domains ranging from deterministic and single-agent with no environment actions to non-deterministic and multi-agent with complex environment actions. Umop is shown to be a rich and efficient planning sy...","2000-05-16"
1033,475473,"Symbolic Reachability Analysis based on SAT-Solvers","Niklas E En; Niklas Een;","In this report we present an implementation of a symbolic reachability analyzer, FIXIT, based on SAT-methods. The problem of reachability is to determine for a given transition system, whether a set of bad states is reachable from a set of initial states. We call our approach symbolic, as we use formulas to represent set of states. The representation includes the standard connectives AND, EQUIV, and NOT, plus sharing of common subformulas. We show how the operations of a standard reachability algorithm can be implemented on this representation, and especially how boolean quantification can be performed. Finally, we present some experimental results, and conclude that a straightforward quantification algorithm together with local simplification procedures suffice for making SAT-based methods an interesting alternative to BDD-based methods. Contents 1 ","2000-01-10"
1034,475716,"Scheduling a Steel Plant with Timed Automata","Ansgar Fehnker;","Scheduling in an environment with a large number of constraints is known to be a hard problem. We tackle this problem for a integrated steel plant in Ghent, Belgium, using Uppaal, a model checker for networks of timed automata. We show how to translate schedulability to reachability, enabling us to use Uppaal 's model checking algorithms. Keywords and Phrases: Timed automata, Static Scheduling, Test automata, Reachability, Model checking, Uppaal AMS Subject Classification:90B06, 90B35, 90B70, 90B90, 93B50 CR Subject Classification:D.2.2., D.2.4, D.2.10, F.1.1, F.3.1, G.2.1, G.2.4, I.2.2 1 ","1999-09-15"
1035,476376,"Model Checking Probabilistic Real Time Systems","Henrik Ejersbo Jensen;","In this paper we present a formal model of probabilistic real time systems as an extension of traditional finite labelled transition graphs with dense time and probabilities. Furthermore, we present a specification formalism in terms of a real timed probabilistic logic and also a model checking method for verification with respect to the logic is presented. 1 ","2001-12-11"
1036,479277,"Compositional Reasoning in Model Checking","Edmund M. Clarke; Sergey Berezin;","This is our abstract. And this is the Brayton's paper reference: [18].","2000-10-10"
1037,479857,"Formal Verification for Fault-Tolerant Architectures: Prolegomena to the Design of PVS","Friedrich Von Henke; John Rushby; Natarajan Shankar; Sam Owre;","PVS is the most recent in a series of veri- cation systems developed at SRI. Its design was strongly inuenced, and later rened, by our experiences in developing formal specications and mechanically checked veri- cations for the fault-tolerant architecture, algorithms, and implementations of a model  eliable computing platform"" (RCP) for life-critical digital ight-control applications, and by a collaborative project to formally verify the design of a commercial avionics processor called AAMP5.","2002-01-24"
1038,483672,"On-The-Fly Model Checking of RCTL Formulas","Avner L; Ilan Beer; Shoham Ben-david;","The specification language RCTL, an extension of CTL, is defined by adding the power of regular expressions to CTL. In addition to being a more expressive and natural hardware specification language than CTL, a large family of RCTL formulas can be verified on-the-fly (during symbolic reachability analysis). On-the-fly model checking, as a powerful verification paradigm, is especially efficient when the specification is false and extremely efficient when the computation needed to get to a failing state is short. It is suitable for the inherently gradual design process since it detects a multitude of bugs at the early verification stages, and paves the way towards finding the more complex errors as the design matures. It is shown that for every erroneous finite computation, there is an RCTL formula that detects it and can be verified on-the-fly. On-thefly verification of RCTL formulas has moved model checking in IBM into a different class of designs inaccessible by prior techniques.","1999-11-23"
1039,484296,"RTP: A Transport Protocol for Real-Time Applications","null","This memorandum is a revision of RFC 1889 in preparation for advancement from Proposed Standard to Draft Standard status.","2001-11-29"
1040,484335,"Congestion Avoidance and Control","Michael J. Karels; Van Jacobson;","This paper is a brief description of (i) --(v) and the rationale behind them. (vi) is an algorithm recently developed by Phil Karn of Bell Communications Research, described in [16]. (vii) is described in a soon-to-be-published RFC (ARPANET ""Request for Comments"")","2001-02-01"
1041,484551,"Computing Optimal Policies for Partially Observable Decision Processes using Compact Representations","Craig Boutilier; David Poole;","Partially-observable Markov decision processes provide a very general model for decision-theoretic planning problems, allowing the trade-offs between various courses of actions to be determined under conditions of uncertainty, and incorporating partial observations made by an agent. Dynamic programming algorithms based on the information or belief state of an agent can be used to construct optimal policies without explicit consideration of past history, but at high computational cost. In this paper, we discuss how structured representations of the system dynamics can be incorporated in classic POMDP solution algorithms. We use Bayesian networks with structured conditional probability matrices to represent POMDPs, and use this representation to structure the belief space for POMDP algorithms. This allows irrelevant distinctions to be ignored. Apart from speeding up optimal policy construction, we suggest that such representations can be exploited to great extent in the development of useful approximation methods. We also briefly discuss the difference in perspective adopted by influence diagram solution methods vis a vis POMDP techniques.","2001-01-12"
1042,484597,"Reasoning about Rings","E. Allen; Emerson Kedar; S. Namjoshi;","The ring is a useful means of structuring concurrent processes. Processes communicate by passing a token in a fixed direction; the process that possesses the token is allowed to perfrom certain actions. Usually, correctness properties are expected to hold irrespective of the size of the ring. We show that the problem of checking many useful correctness properties for rings of all sizes can be reduced to checking them on ring of sizes up to a small cutoff size. We apply our results to the verification of a mutual exclusion protocol and Milner's scheduler protocol. 1 ","1999-08-16"
1043,484773,"Efficient Model Checking by Automated Ordering of Transition Relation Partitions","Daniel Geist; Ilan Beer;","In symbolic model checking, the behavior of a model to be verified is captured by the transition relation of the state space implied by the model. Unfortunately, the size of the transition relation grows rapidly with the number of states even for small models, rendering them impossible to verify. A recent work [5] described a method for partitioning the transition relation, thus reducing the overall space requirement. Using this method, actions that require the transition relation can be executed by using one partition at a time. This process, however, strongly depends on the order in which the partitions are processed during the action.","1999-11-23"
1044,485823,"The Progressive Party Problem: Integer Linear Programming and Constraint Programming Compared","Barbara M. Smith; H. Paul Williams; Peter M. Hubbard; Sally C. Brailsford;","Many discrete optimization problems can be formulated as either integer linear programming problems or constraint satisfaction problems. Although ILP methods appear to be more powerful, sometimes constraint programming can solve these problems more quickly. This paper describes a problem in which the di#erence in performance between the two approaches was particularly marked, since a solution could not be found using ILP.","2000-12-16"
1045,486115,"Automatic Verification of Parameterized Synchronous Systems","E. Allen Emerson; Kedar S. Namjoshi;","Systems with an arbitrary number of homogeneous processes occur in many applications.","2001-08-01"
1046,486490,"The Independent Choice Logic for modelling multiple agents under Uncertainty","David Poole;","Inspired by game theory representations, Bayesian networks, influence diagrams, structured Markov decision process models, logic programming, and work in dynamical systems, the independent choice logic (ICL) is a semantic framework that allows for independent choices (made by various agents, including nature) and a logic program that gives the consequence of choices. This representation can be used as a specification for agents that act in a world, make observations of that world and have memory, as well as a modelling tool for dynamic environments with uncertainty. The rules specify the consequences of an action, what can be sensed and the utility of outcomes. This paper presents a possible-worlds semantics for ICL, and shows how to embed influence diagrams, structured Markov decision processes, and both the strategic (normal) form and extensive (game-tree) form of games within the Thanks to Craig Boutilier and Holger Hoos for detailed comments on this paper. This work was supported by Institute for Robotics and Intelligent Systems, Project IC-7 and Natural Sciences and Engineering Research Council of Canada Operating Grant OGPOO44121. Scholar, Canadian Institute for Advanced Research 1 ICL. It's argued that the ICL provides a natural and concise representation for multi-agent decision-making under uncertainty that allows for the representation of structured probability tables, the dynamic construction of networks (through the use of logical variables) and a way to handle uncertainty and decisions in a logical representation. 1 ","2001-01-12"
1047,486680,"An Overview of Quality-of-Service Routing for the Next Generation HighSpeed Networks: Problems and Solutions","Klara Nahrstedt; Shigang Chen;","The up-coming Gbps high-speed networks are expected to support a wide range of communication-intensive, real-time multimedia applications. The requirement for timely delivery of digitized audio-visual information raises new challenges for the next generation integrated-service broadband networks. One of the key issues is the Quality-of-Service (QoS) routing. It selects network routes with sufficient resources for the requested QoS parameters. The goal of routing solutions is two-fold: (1) satisfying the QoS requirements for every admitted connection and (2) achieving the global efficiency in resource utilization. Many unicast/multicast QoS routing algorithms were published recently, and they work with a variety of QoS requirements and resource constraints. Overall, they can be partitioned into three broad classes: (1) source routing, (2) distributed routing and (3) hierarchical routing algorithms. In this paper we give an overview of the QoS routing problem as well as the existing solutions. We present the strengths and the weaknesses of different routing strategies and outline the challenges. We also discuss the basic algorithms in each class, classify and compare them, and point out possible future directions in the QoS routing area.","1999-07-21"
1048,486876,"A Generalized Theory of Bit Vector Data Flow Analysis","Dhananjay M. Dhamdhere; Uday P. Khedker;","this paper we present a theory which handles unidirectional as well as bidirectional data ow problems uniformly. Apart from explaining the known results in unidirectional and bidirectional ows, it provides deeper insights into the process of data ow analysis. Though the exposition of the theory is based on the bit vector problems, the theory is applicable to all bounded monotone data ow problems which possess the property of separability of solution. Several proofs have been omitted from the paper for brevity; they can be found in [Khedker and Dhamdhere 1992]","2001-08-09"
1049,487117,"Using Abstraction","null","References ","2000-05-03"
1050,488607,"Minimum-Cost Reachability for Priced Timed Automata","A. Fehnker; Ansgar Fehnker; F. W. Va; Frits Va; G. Behrmann; Gerd Behrmann; J. M. T. Romijn; Judi Romijn; K. G. Larsen; Kim Larsen; P. Pettersson; Paul Pettersson; T. S. Hune; Thomas Hune;","This paper introduces the model of linearly priced timed automata as an extension of timed automata, with prices on both transitions and locations. For this model we consider the minimum-cost reachability problem: i.e. given a linearly priced timed automaton and a target state, determine the minimum cost of executions from the initial state to the target state. This problem generalizes the minimum-time reachability problem for ordinary timed automata. We prove decidability of this problem by oering an algorithmic solution, which is based on a combination of branch-and-bound techniques and a new notion of priced regions. The latter allows symbolic representation and manipulation of reachable states together with the cost of reaching them.","2001-05-03"
1051,492136,"RuleBase: Model Checking at IBM","C. Eisner; D. Geist; G. Ronin; I. Beer; L. Gluhovsky; P. Paanah; S. Ben-david; T. Heyman; Y. Rodeh; Y. Wolfsthal;","RuleBase is a symbolic model checking tool, developed by the IBM Haifa Research Laboratory. It is the result of four years of experience in practical formal verification of hardware which, we believe, has been a key factor in bringing the tool to its current level of maturity. Our experience shows that after a short training period, designers can operate the tool independently and achieve impressive results. We present the tool and summarize our development and usage experience, focusing on some work done during 1996.","1999-11-23"
1052,493125,"Processor Verification with Precise Exceptions and Speculative Execution","And Warren A. Hunt; Jun Sawada;","We describe a framework for verifying a pipelined microprocessor whose implementation contains precise exceptions, external interrupts, and speculative execution. We present our correctness criterion which compares the state transitions of pipelined and non-pipelined machines in presence of external interrupts. To perform the verification, we created a table-based model of pipeline execution. This model records committed and in-flight instructions as performed by the microarchitecture.","2000-11-16"
1053,493240,"Methodology and System for Practical Formal Verification of Reactive Hardware","Daniel Geist; Ilan Beer; Michael Yoeli; Raanan Gewirtzman; Shoham Ben-david;","Making formal verification a practicality in industrial environments is still difficult. The capacity of most verification tools is too small, their integration in a design process is difficult and the methodology that should guide their usage is unclear.","1999-11-23"
1054,495190,"OBDD-based Deterministic Planning using the UMOP Planning Framework","Manuela M. Veloso; Rune M. Jensen;","Model checking representation and search techniques were recently shown to be efficiently applicable to planning. Ordered Binary Decision Diagrams (obdds) encode a planning domain as a finite transition system and fast algorithms from model checking search for a solution plan. With proper encodings, obdds can effectively scale and can provide plans for complex planning domains. In this paper, we present results obtained in classical deterministic domains using umop, 1 a new universal obdd-based planning framework applicable to non-deterministic and multi-agent domains (Jensen & Veloso, 1999). A key difference between umop and previous obdd-based planning systems is that the obdd encoding of planning problems is partitioned.","2000-05-16"
1055,496938,"Verifying ET-LOTOS programs with KRONOS","A. Olivero; C. Daws; S. Yovine;","This paper shows that real-time systems described in a reasonable subset of ET-LOTOS can be verified with Kronos by compiling them into timed automata. We illustrate the practical interest of our approach with a case study: the Tick-Tock protocol","2000-12-14"
1056,497542,"Compiling Real-Time Specifications into Extended Automata","Joseph Sifakis; Sergio Yovine; Xavier Nicollin;","We propose a method for the implementation and analysis of real-time systems, based on the compilation of specifications into extended automata. Such a method has been already adopted for the so called ""synchronous"" real-time programming languages.","2000-12-14"
1057,499073,"A Theoretical and Experimental Comparison of Constraint Propagation Techniques for Disjunctive Scheduling","Claude Le Pape;","Disjunctive constraints are widely used to ensure that the time intervals over whichtwo activities require the same resource cannot overlap: if a resource is required bytwo activities A and B, the disjunctive constraint states that either A precedes B or B precedes A. The #propagation "" of disjunctive constraints consists in determining cases where only one of the two orderings is feasible. It results in updating the time-bounds of the two activities. The standard algorithm for propagating disjunctive constraints achieves arc-B-consistency.Twotypes of methods that provide more precise timebounds are studied and compared. The #rst type of method consists in determining whether an activity A must, can, or cannot be the #rst or the last to execute among a set of activities that require the same resource. The second consists in comparing the amount of #resource energy"" required over a time interval #t 1 t 2 #to the amount of energy that is available over the same interval. The main result of the study is an implementation of the #rst method in Ilog Schedule, a generic tool for constraint-based scheduling which exhibits performance in the same range of e#ciency as speci#c operations research algorithms.","2000-12-16"
1058,499366,"Alternating-time Temporal Logic","Orna Kupferman; Rajeev Alur; Thomas A. Henzinger;","Temporal logic comes in two varieties: linear-time temporal logic assumes implicit universal quantification over all paths that are generated by system moves; branching-time temporal logic allows explicit existential and universal quantification over all paths. We introduce a third, more general variety of temporal logic: alternating-time temporal logic offers selective quantification over those paths that are possible outcomes of games, such as the game in which the system and the environment alternate moves. While linear-time and branching-time logics are natural specification languages for closed systems, alternating-time logics are natural specification languages for open systems. For example, by preceding the temporal operator ""eventually"" with a selective path quantifier, we can specify that in the game between the system and the environment, the system has a strategy to reach a certain state. Also the problems of receptiveness, realizability, and controllability can be formulated as model-checking problems for alternating-time formulas.","1998-02-12"
1059,499426,"A Dataflow Approach to Event-Based Debugging","Richard H. Crawford; Ronald A. Olsson; W. Wilson Ho;","This paper describes a novel approach to event-based debugging. The approach is based on a (coarse-grained) dataflow view of events: a high-level event is recognized when an appropriate combination of lower-level events on which it depends has occurred. Event recognition is controlled using familiar programming language constructs. This approach is more flexible and powerful than current ones. It allows arbitrary debugger language commands to be executed when attempting to form higher-level events. It also allows users to specify event recognition in much the same way that they write programs. This paper also describes a prototype, Dalek, that employs the dataflow approach for debugging sequential programs. Dalek demonstrates the feasibility and attractiveness of the dataflow approach. One important motivation for this work is that current sequential debugging tools are inadequate. Dalek contributes toward remedying such inadequacies by providing events and a powerful debugging language. Generalizing the dataflow approach so it can aid in the debugging of concurrent programs is under investigation","1998-07-10"
1060,500980,"FunState - An Internal Design Representation for Codesign","Dirk Ziegenbein; Jurgen Teich; Karsten Strehl; Lothar Thiele; Matthias Gries; Rolf Ernst; Student Member;","In this paper, an internal design model called FunState (functions driven by state machines) is presented that enables the representation of different types of system components and scheduling mechanisms using a mixture of functional programming and state machines. It is shown how properties relevant for scheduling and verification of specification models such as Boolean dataflow, cyclostatic dataflow, synchronous dataflow, marked graphs, and communicating state machines as well as Petri nets can be represented in the FunState model of computation. Examples of methods suited for FunState are described, such as scheduling and verification. They are based on the representation of the model's state transitions in form of a periodic graph. The feasibility of the novel approach is shown with an ATM switch example.","2001-11-15"
1061,502960,"Context-Specific Independence in Bayesian Networks","Craig Boutilier; Daphne Koller; Moises Goldszmidt; Nir Friedman;","Bayesiannetworks provide a languagefor qualitatively representing the conditional independence properties of a distribution. This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms.","2002-02-25"
1062,503616,"MONA: Monadic Second-Order Logic in Practice","Anders B. Sandholm; Jesper G. Henriksen; Michael E. Jrgensen; Nils Klarlund; Ole J. L. Jensen; Robert Paige; Theis Rauhe;","The purpose of this article is to introduce Monadic Secondorder Logic as a practical means of specifying regularity. The logic is a highly succinct alternative to the use of regular expressions. We have built a tool MONA, which acts as a decision procedure and as a translator to finitestate automata. The tool is based on new algorithms for minimizing finitestate automata that use binary decision diagrams (BDDs) to represent transition functions in compressed form. A byproduct of this work is a new bottom-up algorithm to reduce BDDs in linear time without hashing.","1995-04-12"
1063,504401,"Timed Modal Specification - Theory and Tools","Jens Chr Godskesen; Kim G. Larsen;","Lar90, LT88] and the Tav system [GLZ89, BLS92].","1997-06-05"
1064,506043,"Null","null","null",""
1065,509950,"Null","null","null",""
1066,511026,"Null","null","null",""
1067,512685,"Null","null","null",""
1068,513018,"Null","null","null",""
1069,514768,"Null","null","null",""
1070,517603,"Null","null","null",""
1071,520688,"Null","null","null",""
1072,520768,"Null","null","null",""
1073,522428,"Null","null","null",""
1074,526502,"Null","null","null",""
1075,527057,"Null","null","null",""
1076,529251,"Null","null","null",""
1077,534200,"Null","null","null",""
1078,535205,"Null","null","null",""
1079,535995,"Null","null","null",""
1080,538553,"Null","null","null",""
1081,543817,"Null","null","null",""
1082,545708,"Null","null","null",""
1083,547939,"Null","null","null",""
1084,548310,"Null","null","null",""
1085,548620,"Null","null","null",""
1086,548670,"Null","null","null",""
1087,549091,"Null","null","null",""
1088,561317,"Null","null","null",""
1089,570764,"Null","null","null",""
1090,573595,"Null","null","null",""
